{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "for p in (\"Knet\",\"Images\",\"ImageMagick\", \"MAT\", \"LinearAlgebra\")\n",
    "    haskey(Pkg.installed(),p) || Pkg.add(p)\n",
    "end\n",
    "using Knet, MAT, Images, Random\n",
    "using Base.Iterators: flatten, cycle, take\n",
    "using IterTools\n",
    "using LinearAlgebra\n",
    "using Statistics: mean\n",
    "using Plots; default(fmt=:png,ls=:auto)\n",
    "include(Knet.dir(\"data\",\"mnist.jl\"))  # Load data\n",
    "import Base: length, size, iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Knet: Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading MNIST...\n",
      "└ @ Main /home/cankucuksozen/.julia/packages/Knet/vxHRi/data/mnist.jl:33\n"
     ]
    }
   ],
   "source": [
    "dtrn,dtst = mnistdata(;batchsize = 25);  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28×28×1×25 KnetArray{Float32,4}\n",
      "25-element Array{UInt8,1}\n",
      "2400-element Data{Tuple{KnetArray{Float32,4},Array{UInt8,1}}}\n"
     ]
    }
   ],
   "source": [
    "(x,y) = first(dtst)\n",
    "println.(summary.((x,y)));\n",
    "println(summary(dtrn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Conv\n",
    "    w\n",
    "    stride\n",
    "    padding\n",
    "end\n",
    "\n",
    "function Conv(w1::Int, w2::Int, cx::Int, cy::Int; stride = 1, padding = 0)\n",
    "    w = param(w1, w2, cx, cy)\n",
    "    return Conv(w, stride, padding)\n",
    "end\n",
    "\n",
    "function (c::Conv)(x)\n",
    "    return conv4(c.w, x ; padding = c.padding, stride = c.stride)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Deconv\n",
    "    w\n",
    "    stride\n",
    "    padding\n",
    "end\n",
    "\n",
    "function Deconv(w1::Int, w2::Int, cy::Int, cx::Int; stride = 1, padding = 0)\n",
    "    w = param(w1, w2, cy, cx)\n",
    "    return Deconv(w, stride, padding)\n",
    "end\n",
    "\n",
    "function (c::Deconv)(x)\n",
    "    return deconv4(c.w, x ; padding = c.padding, stride = c.stride)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Dense\n",
    "    w\n",
    "    b\n",
    "end\n",
    "function Dense(i::Int,o::Int)     \n",
    "    w = param(o,i)\n",
    "    b = param0(o)\n",
    "    return Dense(w,b)\n",
    "end\n",
    "\n",
    "function (d::Dense)(x)\n",
    "    return d.w * mat(x) .+ d.b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Chain\n",
    "    layers\n",
    "    Chain(layers...) = new(layers)\n",
    "end\n",
    "(c::Chain)(x) = (for l in c.layers; x = l(x); end; x)\n",
    "(c::Chain)(x,y) = nll(c(x),y, average = true)\n",
    "(c::Chain)(d::Data) = mean(c(x,y) for (x,y) in d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rel_logits_2d (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function rel_logits_2d(flat_q, rel, kernel_size)\n",
    "    h2, d, Nh, b = size(flat_q)\n",
    "    flat_q = reshape(flat_q, (1, h2, d, Nh, b))\n",
    "    rel = flatten_rel(rel, kernel_size)\n",
    "    rel_logits = rel .* flat_q\n",
    "    rel_logits = reshape(sum(rel_logits, dims = 3),(h2, h2, Nh, b))\n",
    "    return rel_logits\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flatten_hw (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function flatten_hw(input)\n",
    "    h, w, d, Nh, b = size(input)\n",
    "    new_size = (h*w, d, Nh, b)\n",
    "    return reshape(input, new_size)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flatten_rel (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function flatten_rel(rel, kernel_size)\n",
    "    h2, w2, c, Nh, b = size(rel)\n",
    "    temp_size = (kernel_size, kernel_size, kernel_size, kernel_size, c, Nh, b)\n",
    "    rel = permutedims(reshape(rel, temp_size),[1,3,2,4,5,6,7])\n",
    "    new_size = (h2, w2, c, Nh, b)\n",
    "    rel = reshape(rel, new_size)\n",
    "    return rel\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split_heads_2d (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function split_heads_2d(inputs, Nh)\n",
    "    h, w, d, b = size(inputs)\n",
    "    ret_shape = (h, w, floor(Int,d/Nh), Nh, b)\n",
    "    out = reshape(inputs, ret_shape)\n",
    "    return out\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "combine_heads_2d (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function combine_heads_2d(inputs)\n",
    "    h, w, dh, Nh, b  = size(inputs)\n",
    "    ret_shape = (h,w, dh*Nh, b)\n",
    "    return reshape(inputs, ret_shape)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct self_attention_2d\n",
    "    conv_q\n",
    "    conv_k\n",
    "    conv_v\n",
    "    conv_rel\n",
    "    deconv_rel\n",
    "    conv_attn\n",
    "    kernel_size\n",
    "    stride\n",
    "    padding\n",
    "    dk\n",
    "    dv\n",
    "    Nh\n",
    "    dkh\n",
    "    dvh\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "self_attention_2d"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function self_attention_2d(input_dims, kernel_size, stride, padding, Nh, dk, dv)\n",
    "    conv_q = Conv(1,1, input_dims, dk)\n",
    "    conv_k = Conv(1,1, input_dims, dk)\n",
    "    conv_v = Conv(1,1, input_dims, dv)\n",
    "    conv_rel = Conv(5, 5, input_dims, dk; padding = 2)\n",
    "    deconv_rel = Deconv(kernel_size, kernel_size, dk, dk; stride = kernel_size)\n",
    "    conv_attn = Conv(1,1, dv, dv)\n",
    "    \n",
    "    dkh = floor(Int, dk/Nh)\n",
    "    dvh = floor(Int, dv/Nh)\n",
    "    \n",
    "    stride = stride\n",
    "    padding = padding\n",
    "    \n",
    "    return self_attention_2d(conv_q, conv_k, conv_v, conv_rel, deconv_rel,\n",
    "                                conv_attn, kernel_size, stride, padding, dk, dv, Nh, dkh, dvh)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odims (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function odims(input, kernel_size, stride, padding, dv)\n",
    "    inh,inw,inc,b = size(input)\n",
    "    out_dims_h = Int(((inh-kernel_size) + 2*pad)/stride + 1)\n",
    "    out_dims = (out_dims_h^2, dv, b)\n",
    "    return out_dims, out_dims_h\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s::self_attention_2d)(x)\n",
    "    \n",
    "    out_dims, out_h = odims(x, s.kernel_size, s.stride, s.padding, s.dv)\n",
    "    out = nothing\n",
    "    imh, imw, imc, b = size(x)\n",
    "    \n",
    "    for i = 1:s.stride:imh-s.kernel_size+1\n",
    "        for j = 1:s.stride:imw-s.kernel_size+1\n",
    "\n",
    "            x_patch = x[i:i+s.kernel_size-1, j:j+s.kernel_size-1, :, :]\n",
    "            \n",
    "            _, _, _, b = size(x_patch)\n",
    "            q = s.conv_q(x_patch)\n",
    "            k = s.conv_k(x_patch)\n",
    "            v = s.conv_v(x_patch)\n",
    "\n",
    "            rel = s.conv_rel(x_patch)\n",
    "            rel = s.deconv_rel(rel)\n",
    "\n",
    "            q = q .* (s.dkh ^ -0.5)\n",
    "\n",
    "            q = split_heads_2d(q,s.Nh)\n",
    "            k = split_heads_2d(k,s.Nh)\n",
    "            v = split_heads_2d(v,s.Nh)\n",
    "            rel = split_heads_2d(rel,s.Nh)\n",
    "\n",
    "            flat_q = flatten_hw(q)\n",
    "            flat_k = flatten_hw(k)\n",
    "            flat_v = flatten_hw(v)\n",
    "\n",
    "            logits = bmm(flat_q, flat_k, transB = true)\n",
    "            logits = permutedims(logits, [2,1,3,4])\n",
    "\n",
    "            rel_logits = rel_logits_2d(flat_q, rel, s.kernel_size)\n",
    "\n",
    "            logits += rel_logits\n",
    "\n",
    "            weights = softmax(logits; dims = 1)\n",
    "\n",
    "            attn_out = bmm(weights, flat_v, transA = true)\n",
    "\n",
    "            attn_out = reshape(attn_out, (s.kernel_size, s.kernel_size, s.dvh, s.Nh, :))\n",
    "            attn_out = combine_heads_2d(attn_out) \n",
    "\n",
    "            attn_out = s.conv_attn(attn_out)\n",
    "            \n",
    "            attn_out = pool(attn_out; window = s.kernel_size, mode = 2)\n",
    "            \n",
    "            attn_out = reshape(attn_out, (1, s.dv, :))\n",
    "            \n",
    "            if out == nothing\n",
    "                out = attn_out\n",
    "            else\n",
    "                out = cat(out, attn_out; dims = 1)\n",
    "            end\n",
    "            \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    out = permutedims(reshape(out, (out_h, out_h, s.dv, :)), [2, 1, 3, 4])\n",
    "    \n",
    "    return out\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct self_Attn_Net\n",
    "    activation\n",
    "    conv_i\n",
    "    self_attn\n",
    "    avg_pool\n",
    "    fc\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "self_Attn_Net"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function self_Attn_Net(num_classes = 10)\n",
    "    \n",
    "    activation = relu\n",
    "    conv_i = Conv(6,6,1,64; stride = 2, padding = 2)\n",
    "    self_attn = self_attention_2d(64, 4, 8, 128, 128)\n",
    "    avg_pool = pool\n",
    "    fc = Dense(128, num_classes)\n",
    "    \n",
    "    return self_Attn_Net(activation, conv_i, self_attn, avg_pool, fc)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (r::self_Attn_Net)(x)\n",
    "    x = r.conv_i(x)\n",
    "    x = r.activation.(x)\n",
    "    x = r.self_attn(x)\n",
    "    x = r.activation.(x)\n",
    "    x = r.avg_pool(x; window = 6, mode = 2)\n",
    "    x = mat(x)\n",
    "    x = r.fc(x)\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (r::self_Attn_Net)(x,y)\n",
    "    scores = r(x)\n",
    "    loss = nll(scores, y)\n",
    "    return loss\n",
    "end\n",
    "\n",
    "function (r::self_Attn_Net)(d::Data)\n",
    "    mean_loss = mean(r(x,y) for (x,y) in d)\n",
    "    return mean_loss\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(file, dtrn, dtst, epochs; lr = 0.001)\n",
    "    net = self_Attn_Net()\n",
    "    avgloss = []\n",
    "    sumloss = 0\n",
    "    currloss = []\n",
    "    len = length(dtrn)\n",
    "    iteration = 0\n",
    "    ind = []\n",
    "    for e = 1:epochs\n",
    "        for (i,v) in enumerate(adam(net, dtrn; lr = lr))\n",
    "            iteration += 1\n",
    "            push!(ind, iteration)\n",
    "            push!(currloss,v)\n",
    "            sumloss += v\n",
    "            avg_temp = sumloss / iteration\n",
    "            push!(avgloss, avg_temp)\n",
    "            println(\"iteration: $i / $len   loss: $v\")\n",
    "        end\n",
    "        acc = accuracy(net, dtst)\n",
    "        println(\"epoch: $e    test_acc: $acc\")\n",
    "        Knet.gc()\n",
    "    end\n",
    "    Knet.save(file,\"net\", net)\n",
    "    return ind, currloss, avgloss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 / 2400   loss: 2.2992837\n",
      "iteration: 2 / 2400   loss: 2.304644\n",
      "iteration: 3 / 2400   loss: 2.2968507\n",
      "iteration: 4 / 2400   loss: 2.296804\n",
      "iteration: 5 / 2400   loss: 2.2750115\n",
      "iteration: 6 / 2400   loss: 2.3167093\n",
      "iteration: 7 / 2400   loss: 2.2964826\n",
      "iteration: 8 / 2400   loss: 2.30612\n",
      "iteration: 9 / 2400   loss: 2.2734694\n",
      "iteration: 10 / 2400   loss: 2.2977426\n",
      "iteration: 11 / 2400   loss: 2.3079855\n",
      "iteration: 12 / 2400   loss: 2.2847757\n",
      "iteration: 13 / 2400   loss: 2.29953\n",
      "iteration: 14 / 2400   loss: 2.3231761\n",
      "iteration: 15 / 2400   loss: 2.3014867\n",
      "iteration: 16 / 2400   loss: 2.290228\n",
      "iteration: 17 / 2400   loss: 2.335736\n",
      "iteration: 18 / 2400   loss: 2.2679193\n",
      "iteration: 19 / 2400   loss: 2.2566788\n",
      "iteration: 20 / 2400   loss: 2.2976248\n",
      "iteration: 21 / 2400   loss: 2.3386312\n",
      "iteration: 22 / 2400   loss: 2.3025866\n",
      "iteration: 23 / 2400   loss: 2.2879744\n",
      "iteration: 24 / 2400   loss: 2.279912\n",
      "iteration: 25 / 2400   loss: 2.3329585\n",
      "iteration: 26 / 2400   loss: 2.2911625\n",
      "iteration: 27 / 2400   loss: 2.2372591\n",
      "iteration: 28 / 2400   loss: 2.3072686\n",
      "iteration: 29 / 2400   loss: 2.2912521\n",
      "iteration: 30 / 2400   loss: 2.3034\n",
      "iteration: 31 / 2400   loss: 2.3553743\n",
      "iteration: 32 / 2400   loss: 2.3008506\n",
      "iteration: 33 / 2400   loss: 2.29653\n",
      "iteration: 34 / 2400   loss: 2.3029273\n",
      "iteration: 35 / 2400   loss: 2.2425647\n",
      "iteration: 36 / 2400   loss: 2.3126562\n",
      "iteration: 37 / 2400   loss: 2.3111467\n",
      "iteration: 38 / 2400   loss: 2.2771957\n",
      "iteration: 39 / 2400   loss: 2.3047779\n",
      "iteration: 40 / 2400   loss: 2.2845643\n",
      "iteration: 41 / 2400   loss: 2.2777767\n",
      "iteration: 42 / 2400   loss: 2.2683039\n",
      "iteration: 43 / 2400   loss: 2.3055022\n",
      "iteration: 44 / 2400   loss: 2.269044\n",
      "iteration: 45 / 2400   loss: 2.2984552\n",
      "iteration: 46 / 2400   loss: 2.2785027\n",
      "iteration: 47 / 2400   loss: 2.2750962\n",
      "iteration: 48 / 2400   loss: 2.2695608\n",
      "iteration: 49 / 2400   loss: 2.2902198\n",
      "iteration: 50 / 2400   loss: 2.2976573\n",
      "iteration: 51 / 2400   loss: 2.2926152\n",
      "iteration: 52 / 2400   loss: 2.3219125\n",
      "iteration: 53 / 2400   loss: 2.2820323\n",
      "iteration: 54 / 2400   loss: 2.270764\n",
      "iteration: 55 / 2400   loss: 2.255902\n",
      "iteration: 56 / 2400   loss: 2.2673063\n",
      "iteration: 57 / 2400   loss: 2.2725427\n",
      "iteration: 58 / 2400   loss: 2.2684443\n",
      "iteration: 59 / 2400   loss: 2.2734594\n",
      "iteration: 60 / 2400   loss: 2.266804\n",
      "iteration: 61 / 2400   loss: 2.2702935\n",
      "iteration: 62 / 2400   loss: 2.2707777\n",
      "iteration: 63 / 2400   loss: 2.2546947\n",
      "iteration: 64 / 2400   loss: 2.2424097\n",
      "iteration: 65 / 2400   loss: 2.2406008\n",
      "iteration: 66 / 2400   loss: 2.2286973\n",
      "iteration: 67 / 2400   loss: 2.2326102\n",
      "iteration: 68 / 2400   loss: 2.22694\n",
      "iteration: 69 / 2400   loss: 2.1952088\n",
      "iteration: 70 / 2400   loss: 2.2110991\n",
      "iteration: 71 / 2400   loss: 2.2196565\n",
      "iteration: 72 / 2400   loss: 2.2330015\n",
      "iteration: 73 / 2400   loss: 2.2335744\n",
      "iteration: 74 / 2400   loss: 2.193891\n",
      "iteration: 75 / 2400   loss: 2.2333434\n",
      "iteration: 76 / 2400   loss: 2.167821\n",
      "iteration: 77 / 2400   loss: 2.1757123\n",
      "iteration: 78 / 2400   loss: 2.142157\n",
      "iteration: 79 / 2400   loss: 2.170885\n",
      "iteration: 80 / 2400   loss: 2.1659582\n",
      "iteration: 81 / 2400   loss: 2.1399505\n",
      "iteration: 82 / 2400   loss: 2.273016\n",
      "iteration: 83 / 2400   loss: 2.1863\n",
      "iteration: 84 / 2400   loss: 2.0816383\n",
      "iteration: 85 / 2400   loss: 2.0597389\n",
      "iteration: 86 / 2400   loss: 2.1157274\n",
      "iteration: 87 / 2400   loss: 2.0556717\n",
      "iteration: 88 / 2400   loss: 2.0223987\n",
      "iteration: 89 / 2400   loss: 2.0571408\n",
      "iteration: 90 / 2400   loss: 2.036767\n",
      "iteration: 91 / 2400   loss: 2.046092\n",
      "iteration: 92 / 2400   loss: 1.9861972\n",
      "iteration: 93 / 2400   loss: 1.9235241\n",
      "iteration: 94 / 2400   loss: 1.9111573\n",
      "iteration: 95 / 2400   loss: 1.968695\n",
      "iteration: 96 / 2400   loss: 2.0843647\n",
      "iteration: 97 / 2400   loss: 1.9663749\n",
      "iteration: 98 / 2400   loss: 1.8769989\n",
      "iteration: 99 / 2400   loss: 2.1038783\n",
      "iteration: 100 / 2400   loss: 1.9945276\n",
      "iteration: 101 / 2400   loss: 2.0526447\n",
      "iteration: 102 / 2400   loss: 1.7998397\n",
      "iteration: 103 / 2400   loss: 1.8540242\n",
      "iteration: 104 / 2400   loss: 1.9691329\n",
      "iteration: 105 / 2400   loss: 1.7580878\n",
      "iteration: 106 / 2400   loss: 1.86715\n",
      "iteration: 107 / 2400   loss: 1.6474967\n",
      "iteration: 108 / 2400   loss: 1.9134618\n",
      "iteration: 109 / 2400   loss: 1.7226648\n",
      "iteration: 110 / 2400   loss: 1.6400946\n",
      "iteration: 111 / 2400   loss: 1.799255\n",
      "iteration: 112 / 2400   loss: 1.830827\n",
      "iteration: 113 / 2400   loss: 1.8581122\n",
      "iteration: 114 / 2400   loss: 1.7921093\n",
      "iteration: 115 / 2400   loss: 1.5918775\n",
      "iteration: 116 / 2400   loss: 1.4294766\n",
      "iteration: 117 / 2400   loss: 1.5855564\n",
      "iteration: 118 / 2400   loss: 1.6835566\n",
      "iteration: 119 / 2400   loss: 1.7941891\n",
      "iteration: 120 / 2400   loss: 1.561496\n",
      "iteration: 121 / 2400   loss: 1.6850374\n",
      "iteration: 122 / 2400   loss: 1.7465781\n",
      "iteration: 123 / 2400   loss: 1.9946299\n",
      "iteration: 124 / 2400   loss: 1.7428683\n",
      "iteration: 125 / 2400   loss: 1.5406528\n",
      "iteration: 126 / 2400   loss: 1.5281167\n",
      "iteration: 127 / 2400   loss: 1.9241922\n",
      "iteration: 128 / 2400   loss: 1.7196786\n",
      "iteration: 129 / 2400   loss: 1.6672308\n",
      "iteration: 130 / 2400   loss: 1.3864031\n",
      "iteration: 131 / 2400   loss: 1.7390152\n",
      "iteration: 132 / 2400   loss: 2.0107543\n",
      "iteration: 133 / 2400   loss: 1.6834881\n",
      "iteration: 134 / 2400   loss: 2.037603\n",
      "iteration: 135 / 2400   loss: 1.7658597\n",
      "iteration: 136 / 2400   loss: 1.4482874\n",
      "iteration: 137 / 2400   loss: 1.5223739\n",
      "iteration: 138 / 2400   loss: 1.566642\n",
      "iteration: 139 / 2400   loss: 1.532032\n",
      "iteration: 140 / 2400   loss: 1.8298303\n",
      "iteration: 141 / 2400   loss: 1.8463392\n",
      "iteration: 142 / 2400   loss: 1.4660715\n",
      "iteration: 143 / 2400   loss: 1.3479608\n",
      "iteration: 144 / 2400   loss: 1.4633054\n",
      "iteration: 145 / 2400   loss: 1.5326189\n",
      "iteration: 146 / 2400   loss: 1.7331839\n",
      "iteration: 147 / 2400   loss: 1.5618206\n",
      "iteration: 148 / 2400   loss: 1.5856757\n",
      "iteration: 149 / 2400   loss: 1.3507288\n",
      "iteration: 150 / 2400   loss: 1.369966\n",
      "iteration: 151 / 2400   loss: 1.5486597\n",
      "iteration: 152 / 2400   loss: 1.5410413\n",
      "iteration: 153 / 2400   loss: 1.4251044\n",
      "iteration: 154 / 2400   loss: 1.696747\n",
      "iteration: 155 / 2400   loss: 1.5910692\n",
      "iteration: 156 / 2400   loss: 1.2344644\n",
      "iteration: 157 / 2400   loss: 1.4611676\n",
      "iteration: 158 / 2400   loss: 1.6745065\n",
      "iteration: 159 / 2400   loss: 1.2456484\n",
      "iteration: 160 / 2400   loss: 1.54193\n",
      "iteration: 161 / 2400   loss: 1.3842326\n",
      "iteration: 162 / 2400   loss: 1.5654225\n",
      "iteration: 163 / 2400   loss: 2.0291984\n",
      "iteration: 164 / 2400   loss: 1.5359122\n",
      "iteration: 165 / 2400   loss: 1.3884025\n",
      "iteration: 166 / 2400   loss: 1.219067\n",
      "iteration: 167 / 2400   loss: 1.2131077\n",
      "iteration: 168 / 2400   loss: 1.3373092\n",
      "iteration: 169 / 2400   loss: 1.5895249\n",
      "iteration: 170 / 2400   loss: 1.3113409\n",
      "iteration: 171 / 2400   loss: 1.4257507\n",
      "iteration: 172 / 2400   loss: 1.3526394\n",
      "iteration: 173 / 2400   loss: 1.6060164\n",
      "iteration: 174 / 2400   loss: 1.4034446\n",
      "iteration: 175 / 2400   loss: 1.400287\n",
      "iteration: 176 / 2400   loss: 1.4232047\n",
      "iteration: 177 / 2400   loss: 1.0727278\n",
      "iteration: 178 / 2400   loss: 1.5023602\n",
      "iteration: 179 / 2400   loss: 1.4897821\n",
      "iteration: 180 / 2400   loss: 1.3010198\n",
      "iteration: 181 / 2400   loss: 1.9019047\n",
      "iteration: 182 / 2400   loss: 1.2775661\n",
      "iteration: 183 / 2400   loss: 1.1032654\n",
      "iteration: 184 / 2400   loss: 1.3422241\n",
      "iteration: 185 / 2400   loss: 1.3541123\n",
      "iteration: 186 / 2400   loss: 1.4025439\n",
      "iteration: 187 / 2400   loss: 1.5116947\n",
      "iteration: 188 / 2400   loss: 1.5435638\n",
      "iteration: 189 / 2400   loss: 1.6937107\n",
      "iteration: 190 / 2400   loss: 1.5008391\n",
      "iteration: 191 / 2400   loss: 1.0103216\n",
      "iteration: 192 / 2400   loss: 1.0930505\n",
      "iteration: 193 / 2400   loss: 1.3084582\n",
      "iteration: 194 / 2400   loss: 1.4199568\n",
      "iteration: 195 / 2400   loss: 1.3237436\n",
      "iteration: 196 / 2400   loss: 1.1182636\n",
      "iteration: 197 / 2400   loss: 1.2624028\n",
      "iteration: 198 / 2400   loss: 1.1605805\n",
      "iteration: 199 / 2400   loss: 1.2457304\n",
      "iteration: 200 / 2400   loss: 1.1622119\n",
      "iteration: 201 / 2400   loss: 1.1116223\n",
      "iteration: 202 / 2400   loss: 1.3713266\n",
      "iteration: 203 / 2400   loss: 1.1472735\n",
      "iteration: 204 / 2400   loss: 1.22644\n",
      "iteration: 205 / 2400   loss: 1.1770546\n",
      "iteration: 206 / 2400   loss: 1.2404122\n",
      "iteration: 207 / 2400   loss: 1.3369186\n",
      "iteration: 208 / 2400   loss: 1.3723931\n",
      "iteration: 209 / 2400   loss: 1.5849396\n",
      "iteration: 210 / 2400   loss: 0.9811108\n",
      "iteration: 211 / 2400   loss: 1.2615794\n",
      "iteration: 212 / 2400   loss: 1.6552736\n",
      "iteration: 213 / 2400   loss: 1.3732246\n",
      "iteration: 214 / 2400   loss: 1.3339665\n",
      "iteration: 215 / 2400   loss: 1.3743591\n",
      "iteration: 216 / 2400   loss: 1.2441742\n",
      "iteration: 217 / 2400   loss: 1.493866\n",
      "iteration: 218 / 2400   loss: 0.91352683\n",
      "iteration: 219 / 2400   loss: 1.2139469\n",
      "iteration: 220 / 2400   loss: 1.0852123\n",
      "iteration: 221 / 2400   loss: 1.2019519\n",
      "iteration: 222 / 2400   loss: 1.0271933\n",
      "iteration: 223 / 2400   loss: 1.3392743\n",
      "iteration: 224 / 2400   loss: 1.0316194\n",
      "iteration: 225 / 2400   loss: 1.4389347\n",
      "iteration: 226 / 2400   loss: 1.0691627\n",
      "iteration: 227 / 2400   loss: 1.2475896\n",
      "iteration: 228 / 2400   loss: 1.0534813\n",
      "iteration: 229 / 2400   loss: 1.3855366\n",
      "iteration: 230 / 2400   loss: 1.5047857\n",
      "iteration: 231 / 2400   loss: 1.055608\n",
      "iteration: 232 / 2400   loss: 1.1139197\n",
      "iteration: 233 / 2400   loss: 1.4286773\n",
      "iteration: 234 / 2400   loss: 2.23213\n",
      "iteration: 235 / 2400   loss: 0.9538274\n",
      "iteration: 236 / 2400   loss: 1.6967838\n",
      "iteration: 237 / 2400   loss: 1.2437611\n",
      "iteration: 238 / 2400   loss: 1.0739642\n",
      "iteration: 239 / 2400   loss: 1.2293502\n",
      "iteration: 240 / 2400   loss: 1.1332551\n",
      "iteration: 241 / 2400   loss: 1.2432877\n",
      "iteration: 242 / 2400   loss: 1.1316624\n",
      "iteration: 243 / 2400   loss: 0.97471434\n",
      "iteration: 244 / 2400   loss: 1.15185\n",
      "iteration: 245 / 2400   loss: 0.98404616\n",
      "iteration: 246 / 2400   loss: 1.0750412\n",
      "iteration: 247 / 2400   loss: 1.2806425\n",
      "iteration: 248 / 2400   loss: 0.99126905\n",
      "iteration: 249 / 2400   loss: 1.3060542\n",
      "iteration: 250 / 2400   loss: 1.181236\n",
      "iteration: 251 / 2400   loss: 1.542622\n",
      "iteration: 252 / 2400   loss: 1.4819064\n",
      "iteration: 253 / 2400   loss: 1.4033009\n",
      "iteration: 254 / 2400   loss: 1.3686836\n",
      "iteration: 255 / 2400   loss: 1.2078588\n",
      "iteration: 256 / 2400   loss: 1.091427\n",
      "iteration: 257 / 2400   loss: 1.3408432\n",
      "iteration: 258 / 2400   loss: 1.2446991\n",
      "iteration: 259 / 2400   loss: 1.293077\n",
      "iteration: 260 / 2400   loss: 1.2011551\n",
      "iteration: 261 / 2400   loss: 1.1921923\n",
      "iteration: 262 / 2400   loss: 1.2995092\n",
      "iteration: 263 / 2400   loss: 0.79967165\n",
      "iteration: 264 / 2400   loss: 1.0012138\n",
      "iteration: 265 / 2400   loss: 1.2294791\n",
      "iteration: 266 / 2400   loss: 1.5406549\n",
      "iteration: 267 / 2400   loss: 1.4714546\n",
      "iteration: 268 / 2400   loss: 1.1322964\n",
      "iteration: 269 / 2400   loss: 1.1887099\n",
      "iteration: 270 / 2400   loss: 1.3334947\n",
      "iteration: 271 / 2400   loss: 1.04774\n",
      "iteration: 272 / 2400   loss: 1.128492\n",
      "iteration: 273 / 2400   loss: 1.6077685\n",
      "iteration: 274 / 2400   loss: 1.1494409\n",
      "iteration: 275 / 2400   loss: 1.3624858\n",
      "iteration: 276 / 2400   loss: 1.6252371\n",
      "iteration: 277 / 2400   loss: 1.0635053\n",
      "iteration: 278 / 2400   loss: 0.99475664\n",
      "iteration: 279 / 2400   loss: 0.9499533\n",
      "iteration: 280 / 2400   loss: 1.1158166\n",
      "iteration: 281 / 2400   loss: 1.107494\n",
      "iteration: 282 / 2400   loss: 1.2615763\n",
      "iteration: 283 / 2400   loss: 1.1758479\n",
      "iteration: 284 / 2400   loss: 1.051172\n",
      "iteration: 285 / 2400   loss: 1.2570688\n",
      "iteration: 286 / 2400   loss: 1.1140275\n",
      "iteration: 287 / 2400   loss: 1.5082016\n",
      "iteration: 288 / 2400   loss: 0.896931\n",
      "iteration: 289 / 2400   loss: 1.1684041\n",
      "iteration: 290 / 2400   loss: 1.4001056\n",
      "iteration: 291 / 2400   loss: 1.8729434\n",
      "iteration: 292 / 2400   loss: 1.2941738\n",
      "iteration: 293 / 2400   loss: 1.0982633\n",
      "iteration: 294 / 2400   loss: 1.0661507\n",
      "iteration: 295 / 2400   loss: 1.054966\n",
      "iteration: 296 / 2400   loss: 1.4178543\n",
      "iteration: 297 / 2400   loss: 1.4181029\n",
      "iteration: 298 / 2400   loss: 1.1049433\n",
      "iteration: 299 / 2400   loss: 1.088654\n",
      "iteration: 300 / 2400   loss: 1.2027385\n",
      "iteration: 301 / 2400   loss: 1.0628704\n",
      "iteration: 302 / 2400   loss: 1.0872194\n",
      "iteration: 303 / 2400   loss: 0.8730059\n",
      "iteration: 304 / 2400   loss: 1.1856586\n",
      "iteration: 305 / 2400   loss: 1.109986\n",
      "iteration: 306 / 2400   loss: 1.5152555\n",
      "iteration: 307 / 2400   loss: 0.95707124\n",
      "iteration: 308 / 2400   loss: 1.3139764\n",
      "iteration: 309 / 2400   loss: 1.1239254\n",
      "iteration: 310 / 2400   loss: 1.3358328\n",
      "iteration: 311 / 2400   loss: 1.087697\n",
      "iteration: 312 / 2400   loss: 1.0947512\n",
      "iteration: 313 / 2400   loss: 1.0485431\n",
      "iteration: 314 / 2400   loss: 1.0184494\n",
      "iteration: 315 / 2400   loss: 1.106626\n",
      "iteration: 316 / 2400   loss: 1.3197843\n",
      "iteration: 317 / 2400   loss: 0.89366317\n",
      "iteration: 318 / 2400   loss: 0.9228718\n",
      "iteration: 319 / 2400   loss: 1.0888879\n",
      "iteration: 320 / 2400   loss: 1.3155277\n",
      "iteration: 321 / 2400   loss: 1.2326702\n",
      "iteration: 322 / 2400   loss: 0.9645503\n",
      "iteration: 323 / 2400   loss: 1.0054018\n",
      "iteration: 324 / 2400   loss: 0.8649551\n",
      "iteration: 325 / 2400   loss: 0.83394957\n",
      "iteration: 326 / 2400   loss: 1.1023337\n",
      "iteration: 327 / 2400   loss: 0.7083313\n",
      "iteration: 328 / 2400   loss: 0.9091977\n",
      "iteration: 329 / 2400   loss: 1.8777188\n",
      "iteration: 330 / 2400   loss: 1.0663023\n",
      "iteration: 331 / 2400   loss: 1.0686302\n",
      "iteration: 332 / 2400   loss: 1.2463422\n",
      "iteration: 333 / 2400   loss: 1.3645014\n",
      "iteration: 334 / 2400   loss: 1.0470216\n",
      "iteration: 335 / 2400   loss: 1.0408313\n",
      "iteration: 336 / 2400   loss: 1.1045643\n",
      "iteration: 337 / 2400   loss: 1.0185575\n",
      "iteration: 338 / 2400   loss: 1.253633\n",
      "iteration: 339 / 2400   loss: 1.5943727\n",
      "iteration: 340 / 2400   loss: 1.644699\n",
      "iteration: 341 / 2400   loss: 1.0929701\n",
      "iteration: 342 / 2400   loss: 1.0044447\n",
      "iteration: 343 / 2400   loss: 0.562767\n",
      "iteration: 344 / 2400   loss: 0.57825124\n",
      "iteration: 345 / 2400   loss: 0.92697364\n",
      "iteration: 346 / 2400   loss: 1.1159941\n",
      "iteration: 347 / 2400   loss: 0.941852\n",
      "iteration: 348 / 2400   loss: 1.4537978\n",
      "iteration: 349 / 2400   loss: 1.5412068\n",
      "iteration: 350 / 2400   loss: 1.0609596\n",
      "iteration: 351 / 2400   loss: 1.2456871\n",
      "iteration: 352 / 2400   loss: 1.3025235\n",
      "iteration: 353 / 2400   loss: 0.8989524\n",
      "iteration: 354 / 2400   loss: 1.0748404\n",
      "iteration: 355 / 2400   loss: 1.2827542\n",
      "iteration: 356 / 2400   loss: 1.3511804\n",
      "iteration: 357 / 2400   loss: 1.6912726\n",
      "iteration: 358 / 2400   loss: 0.8752428\n",
      "iteration: 359 / 2400   loss: 0.83885664\n",
      "iteration: 360 / 2400   loss: 0.7355199\n",
      "iteration: 361 / 2400   loss: 0.65045244\n",
      "iteration: 362 / 2400   loss: 0.7337448\n",
      "iteration: 363 / 2400   loss: 0.9593542\n",
      "iteration: 364 / 2400   loss: 0.9722974\n",
      "iteration: 365 / 2400   loss: 1.1049674\n",
      "iteration: 366 / 2400   loss: 0.7388469\n",
      "iteration: 367 / 2400   loss: 1.0213672\n",
      "iteration: 368 / 2400   loss: 0.8134664\n",
      "iteration: 369 / 2400   loss: 0.96342605\n",
      "iteration: 370 / 2400   loss: 0.99396896\n",
      "iteration: 371 / 2400   loss: 1.1933489\n",
      "iteration: 372 / 2400   loss: 0.84541535\n",
      "iteration: 373 / 2400   loss: 1.0247836\n",
      "iteration: 374 / 2400   loss: 1.0280598\n",
      "iteration: 375 / 2400   loss: 0.7672308\n",
      "iteration: 376 / 2400   loss: 1.0670346\n",
      "iteration: 377 / 2400   loss: 0.82969576\n",
      "iteration: 378 / 2400   loss: 1.1384622\n",
      "iteration: 379 / 2400   loss: 1.259844\n",
      "iteration: 380 / 2400   loss: 0.8523361\n",
      "iteration: 381 / 2400   loss: 0.9651734\n",
      "iteration: 382 / 2400   loss: 1.0469517\n",
      "iteration: 383 / 2400   loss: 1.2192711\n",
      "iteration: 384 / 2400   loss: 0.8195378\n",
      "iteration: 385 / 2400   loss: 1.147282\n",
      "iteration: 386 / 2400   loss: 0.7865489\n",
      "iteration: 387 / 2400   loss: 0.7866269\n",
      "iteration: 388 / 2400   loss: 1.1043377\n",
      "iteration: 389 / 2400   loss: 0.66087264\n",
      "iteration: 390 / 2400   loss: 0.9263147\n",
      "iteration: 391 / 2400   loss: 1.1095588\n",
      "iteration: 392 / 2400   loss: 0.91423476\n",
      "iteration: 393 / 2400   loss: 1.0248966\n",
      "iteration: 394 / 2400   loss: 0.7969948\n",
      "iteration: 395 / 2400   loss: 0.7485849\n",
      "iteration: 396 / 2400   loss: 0.686248\n",
      "iteration: 397 / 2400   loss: 0.6735157\n",
      "iteration: 398 / 2400   loss: 0.67710775\n",
      "iteration: 399 / 2400   loss: 0.84995985\n",
      "iteration: 400 / 2400   loss: 0.7971715\n",
      "iteration: 401 / 2400   loss: 1.1170968\n",
      "iteration: 402 / 2400   loss: 1.168591\n",
      "iteration: 403 / 2400   loss: 0.803149\n",
      "iteration: 404 / 2400   loss: 0.85317534\n",
      "iteration: 405 / 2400   loss: 1.2929987\n",
      "iteration: 406 / 2400   loss: 0.6129252\n",
      "iteration: 407 / 2400   loss: 1.002644\n",
      "iteration: 408 / 2400   loss: 0.9663909\n",
      "iteration: 409 / 2400   loss: 1.9839277\n",
      "iteration: 410 / 2400   loss: 1.4877881\n",
      "iteration: 411 / 2400   loss: 1.5256186\n",
      "iteration: 412 / 2400   loss: 1.1054726\n",
      "iteration: 413 / 2400   loss: 1.0440465\n",
      "iteration: 414 / 2400   loss: 1.085749\n",
      "iteration: 415 / 2400   loss: 1.364252\n",
      "iteration: 416 / 2400   loss: 0.917845\n",
      "iteration: 417 / 2400   loss: 0.82591784\n",
      "iteration: 418 / 2400   loss: 1.0488901\n",
      "iteration: 419 / 2400   loss: 0.84193456\n",
      "iteration: 420 / 2400   loss: 0.9257742\n",
      "iteration: 421 / 2400   loss: 0.97415316\n",
      "iteration: 422 / 2400   loss: 0.8104406\n",
      "iteration: 423 / 2400   loss: 0.7789459\n",
      "iteration: 424 / 2400   loss: 1.1110048\n",
      "iteration: 425 / 2400   loss: 0.8166948\n",
      "iteration: 426 / 2400   loss: 0.8342114\n",
      "iteration: 427 / 2400   loss: 0.7132194\n",
      "iteration: 428 / 2400   loss: 0.6632127\n",
      "iteration: 429 / 2400   loss: 0.8990503\n",
      "iteration: 430 / 2400   loss: 0.93490684\n",
      "iteration: 431 / 2400   loss: 1.2131686\n",
      "iteration: 432 / 2400   loss: 0.7353507\n",
      "iteration: 433 / 2400   loss: 0.82175016\n",
      "iteration: 434 / 2400   loss: 0.69307953\n",
      "iteration: 435 / 2400   loss: 1.4289305\n",
      "iteration: 436 / 2400   loss: 0.7463461\n",
      "iteration: 437 / 2400   loss: 0.837059\n",
      "iteration: 438 / 2400   loss: 0.8256165\n",
      "iteration: 439 / 2400   loss: 0.75973433\n",
      "iteration: 440 / 2400   loss: 1.7498099\n",
      "iteration: 441 / 2400   loss: 1.0169098\n",
      "iteration: 442 / 2400   loss: 0.91449493\n",
      "iteration: 443 / 2400   loss: 0.6145316\n",
      "iteration: 444 / 2400   loss: 0.6612776\n",
      "iteration: 445 / 2400   loss: 0.71652913\n",
      "iteration: 446 / 2400   loss: 0.66354585\n",
      "iteration: 447 / 2400   loss: 0.7152805\n",
      "iteration: 448 / 2400   loss: 0.89320964\n",
      "iteration: 449 / 2400   loss: 0.90273803\n",
      "iteration: 450 / 2400   loss: 0.91422486\n",
      "iteration: 451 / 2400   loss: 0.695213\n",
      "iteration: 452 / 2400   loss: 0.75259966\n",
      "iteration: 453 / 2400   loss: 1.0951542\n",
      "iteration: 454 / 2400   loss: 0.760812\n",
      "iteration: 455 / 2400   loss: 0.6730017\n",
      "iteration: 456 / 2400   loss: 1.0237\n",
      "iteration: 457 / 2400   loss: 0.6489907\n",
      "iteration: 458 / 2400   loss: 0.70785403\n",
      "iteration: 459 / 2400   loss: 0.5373856\n",
      "iteration: 460 / 2400   loss: 0.78309625\n",
      "iteration: 461 / 2400   loss: 0.7816243\n",
      "iteration: 462 / 2400   loss: 0.6705749\n",
      "iteration: 463 / 2400   loss: 0.98608935\n",
      "iteration: 464 / 2400   loss: 1.0351515\n",
      "iteration: 465 / 2400   loss: 1.1608616\n",
      "iteration: 466 / 2400   loss: 1.1622217\n",
      "iteration: 467 / 2400   loss: 0.76509094\n",
      "iteration: 468 / 2400   loss: 0.69037795\n",
      "iteration: 469 / 2400   loss: 1.0011696\n",
      "iteration: 470 / 2400   loss: 0.95398843\n",
      "iteration: 471 / 2400   loss: 0.75426126\n",
      "iteration: 472 / 2400   loss: 1.5017669\n",
      "iteration: 473 / 2400   loss: 0.7875707\n",
      "iteration: 474 / 2400   loss: 1.023715\n",
      "iteration: 475 / 2400   loss: 1.014475\n",
      "iteration: 476 / 2400   loss: 1.039151\n",
      "iteration: 477 / 2400   loss: 0.8022537\n",
      "iteration: 478 / 2400   loss: 1.3166232\n",
      "iteration: 479 / 2400   loss: 0.68538374\n",
      "iteration: 480 / 2400   loss: 0.83463484\n",
      "iteration: 481 / 2400   loss: 0.8196801\n",
      "iteration: 482 / 2400   loss: 0.56892973\n",
      "iteration: 483 / 2400   loss: 0.7756826\n",
      "iteration: 484 / 2400   loss: 0.77997315\n",
      "iteration: 485 / 2400   loss: 0.6381756\n",
      "iteration: 486 / 2400   loss: 0.44493443\n",
      "iteration: 487 / 2400   loss: 0.9405084\n",
      "iteration: 488 / 2400   loss: 0.82815367\n",
      "iteration: 489 / 2400   loss: 0.8662478\n",
      "iteration: 490 / 2400   loss: 0.8566797\n",
      "iteration: 491 / 2400   loss: 0.99817854\n",
      "iteration: 492 / 2400   loss: 0.68044984\n",
      "iteration: 493 / 2400   loss: 0.9834671\n",
      "iteration: 494 / 2400   loss: 0.95731163\n",
      "iteration: 495 / 2400   loss: 0.6499168\n",
      "iteration: 496 / 2400   loss: 0.98409575\n",
      "iteration: 497 / 2400   loss: 0.60913134\n",
      "iteration: 498 / 2400   loss: 0.6621821\n",
      "iteration: 499 / 2400   loss: 0.6727353\n",
      "iteration: 500 / 2400   loss: 0.8911221\n",
      "iteration: 501 / 2400   loss: 0.8557927\n",
      "iteration: 502 / 2400   loss: 0.6778868\n",
      "iteration: 503 / 2400   loss: 0.92670244\n",
      "iteration: 504 / 2400   loss: 0.97674346\n",
      "iteration: 505 / 2400   loss: 0.67640024\n",
      "iteration: 506 / 2400   loss: 0.6080543\n",
      "iteration: 507 / 2400   loss: 0.8963354\n",
      "iteration: 508 / 2400   loss: 1.1414584\n",
      "iteration: 509 / 2400   loss: 0.66853815\n",
      "iteration: 510 / 2400   loss: 0.7002635\n",
      "iteration: 511 / 2400   loss: 0.6855725\n",
      "iteration: 512 / 2400   loss: 0.5806188\n",
      "iteration: 513 / 2400   loss: 0.6600145\n",
      "iteration: 514 / 2400   loss: 0.877612\n",
      "iteration: 515 / 2400   loss: 0.91363174\n",
      "iteration: 516 / 2400   loss: 0.98825425\n",
      "iteration: 517 / 2400   loss: 0.66628975\n",
      "iteration: 518 / 2400   loss: 0.9876222\n",
      "iteration: 519 / 2400   loss: 0.79985726\n",
      "iteration: 520 / 2400   loss: 0.6149325\n",
      "iteration: 521 / 2400   loss: 0.975853\n",
      "iteration: 522 / 2400   loss: 1.0028642\n",
      "iteration: 523 / 2400   loss: 1.1450137\n",
      "iteration: 524 / 2400   loss: 1.2809131\n",
      "iteration: 525 / 2400   loss: 0.74144554\n",
      "iteration: 526 / 2400   loss: 0.672435\n",
      "iteration: 527 / 2400   loss: 0.80249834\n",
      "iteration: 528 / 2400   loss: 1.113755\n",
      "iteration: 529 / 2400   loss: 0.43484315\n",
      "iteration: 530 / 2400   loss: 0.511545\n",
      "iteration: 531 / 2400   loss: 0.45316666\n",
      "iteration: 532 / 2400   loss: 0.5698908\n",
      "iteration: 533 / 2400   loss: 0.7914447\n",
      "iteration: 534 / 2400   loss: 0.80419296\n",
      "iteration: 535 / 2400   loss: 0.77737945\n",
      "iteration: 536 / 2400   loss: 0.6901171\n",
      "iteration: 537 / 2400   loss: 0.6212826\n",
      "iteration: 538 / 2400   loss: 0.62340087\n",
      "iteration: 539 / 2400   loss: 0.52549917\n",
      "iteration: 540 / 2400   loss: 0.6448808\n",
      "iteration: 541 / 2400   loss: 0.68517923\n",
      "iteration: 542 / 2400   loss: 0.6775551\n",
      "iteration: 543 / 2400   loss: 0.5721147\n",
      "iteration: 544 / 2400   loss: 0.56478363\n",
      "iteration: 545 / 2400   loss: 0.5970499\n",
      "iteration: 546 / 2400   loss: 0.58943385\n",
      "iteration: 547 / 2400   loss: 0.8588234\n",
      "iteration: 548 / 2400   loss: 0.9230723\n",
      "iteration: 549 / 2400   loss: 0.861326\n",
      "iteration: 550 / 2400   loss: 0.9076146\n",
      "iteration: 551 / 2400   loss: 0.58876115\n",
      "iteration: 552 / 2400   loss: 0.92078245\n",
      "iteration: 553 / 2400   loss: 0.5340202\n",
      "iteration: 554 / 2400   loss: 0.67401886\n",
      "iteration: 555 / 2400   loss: 0.5392258\n",
      "iteration: 556 / 2400   loss: 0.6148341\n",
      "iteration: 557 / 2400   loss: 1.0925963\n",
      "iteration: 558 / 2400   loss: 1.1575867\n",
      "iteration: 559 / 2400   loss: 0.637058\n",
      "iteration: 560 / 2400   loss: 0.97462714\n",
      "iteration: 561 / 2400   loss: 0.974184\n",
      "iteration: 562 / 2400   loss: 0.8176923\n",
      "iteration: 563 / 2400   loss: 0.80026156\n",
      "iteration: 564 / 2400   loss: 0.77036995\n",
      "iteration: 565 / 2400   loss: 0.60345966\n",
      "iteration: 566 / 2400   loss: 0.85275435\n",
      "iteration: 567 / 2400   loss: 0.6994606\n",
      "iteration: 568 / 2400   loss: 0.77983314\n",
      "iteration: 569 / 2400   loss: 0.7731007\n",
      "iteration: 570 / 2400   loss: 0.5243143\n",
      "iteration: 571 / 2400   loss: 0.7643676\n",
      "iteration: 572 / 2400   loss: 0.7932248\n",
      "iteration: 573 / 2400   loss: 1.0322783\n",
      "iteration: 574 / 2400   loss: 0.5968964\n",
      "iteration: 575 / 2400   loss: 0.79143846\n",
      "iteration: 576 / 2400   loss: 0.7749712\n",
      "iteration: 577 / 2400   loss: 0.6580182\n",
      "iteration: 578 / 2400   loss: 0.6214603\n",
      "iteration: 579 / 2400   loss: 0.6175591\n",
      "iteration: 580 / 2400   loss: 0.5988052\n",
      "iteration: 581 / 2400   loss: 0.9310829\n",
      "iteration: 582 / 2400   loss: 0.7669362\n",
      "iteration: 583 / 2400   loss: 0.5883534\n",
      "iteration: 584 / 2400   loss: 0.7870589\n",
      "iteration: 585 / 2400   loss: 0.60083723\n",
      "iteration: 586 / 2400   loss: 1.135997\n",
      "iteration: 587 / 2400   loss: 0.9072232\n",
      "iteration: 588 / 2400   loss: 0.9413716\n",
      "iteration: 589 / 2400   loss: 0.83524626\n",
      "iteration: 590 / 2400   loss: 0.69992155\n",
      "iteration: 591 / 2400   loss: 1.4101061\n",
      "iteration: 592 / 2400   loss: 0.8806962\n",
      "iteration: 593 / 2400   loss: 0.6052132\n",
      "iteration: 594 / 2400   loss: 0.62567514\n",
      "iteration: 595 / 2400   loss: 0.894499\n",
      "iteration: 596 / 2400   loss: 0.98171854\n",
      "iteration: 597 / 2400   loss: 0.6216363\n",
      "iteration: 598 / 2400   loss: 0.6107352\n",
      "iteration: 599 / 2400   loss: 0.6328485\n",
      "iteration: 600 / 2400   loss: 0.48284706\n",
      "iteration: 601 / 2400   loss: 0.75199693\n",
      "iteration: 602 / 2400   loss: 0.3475934\n",
      "iteration: 603 / 2400   loss: 0.9174928\n",
      "iteration: 604 / 2400   loss: 0.43528983\n",
      "iteration: 605 / 2400   loss: 0.83793634\n",
      "iteration: 606 / 2400   loss: 0.63595015\n",
      "iteration: 607 / 2400   loss: 1.0655216\n",
      "iteration: 608 / 2400   loss: 0.93558306\n",
      "iteration: 609 / 2400   loss: 0.49351364\n",
      "iteration: 610 / 2400   loss: 0.5565076\n",
      "iteration: 611 / 2400   loss: 0.9392213\n",
      "iteration: 612 / 2400   loss: 0.5621223\n",
      "iteration: 613 / 2400   loss: 0.5500639\n",
      "iteration: 614 / 2400   loss: 0.69895697\n",
      "iteration: 615 / 2400   loss: 1.1291391\n",
      "iteration: 616 / 2400   loss: 0.60471785\n",
      "iteration: 617 / 2400   loss: 0.97742265\n",
      "iteration: 618 / 2400   loss: 0.41449332\n",
      "iteration: 619 / 2400   loss: 0.52044386\n",
      "iteration: 620 / 2400   loss: 0.713774\n",
      "iteration: 621 / 2400   loss: 0.79493284\n",
      "iteration: 622 / 2400   loss: 0.6451506\n",
      "iteration: 623 / 2400   loss: 0.6101547\n",
      "iteration: 624 / 2400   loss: 0.4526851\n",
      "iteration: 625 / 2400   loss: 0.7827794\n",
      "iteration: 626 / 2400   loss: 0.4795985\n",
      "iteration: 627 / 2400   loss: 0.4462442\n",
      "iteration: 628 / 2400   loss: 0.5157116\n",
      "iteration: 629 / 2400   loss: 0.6041878\n",
      "iteration: 630 / 2400   loss: 0.9302013\n",
      "iteration: 631 / 2400   loss: 1.1600689\n",
      "iteration: 632 / 2400   loss: 0.6858862\n",
      "iteration: 633 / 2400   loss: 0.53224164\n",
      "iteration: 634 / 2400   loss: 0.92214143\n",
      "iteration: 635 / 2400   loss: 0.9403256\n",
      "iteration: 636 / 2400   loss: 1.2965858\n",
      "iteration: 637 / 2400   loss: 0.6525309\n",
      "iteration: 638 / 2400   loss: 1.2554618\n",
      "iteration: 639 / 2400   loss: 0.96135277\n",
      "iteration: 640 / 2400   loss: 0.72343844\n",
      "iteration: 641 / 2400   loss: 0.8472287\n",
      "iteration: 642 / 2400   loss: 1.1228601\n",
      "iteration: 643 / 2400   loss: 0.5736468\n",
      "iteration: 644 / 2400   loss: 0.516996\n",
      "iteration: 645 / 2400   loss: 0.6426866\n",
      "iteration: 646 / 2400   loss: 0.83289075\n",
      "iteration: 647 / 2400   loss: 0.6711767\n",
      "iteration: 648 / 2400   loss: 0.5780909\n",
      "iteration: 649 / 2400   loss: 0.6184216\n",
      "iteration: 650 / 2400   loss: 0.38273513\n",
      "iteration: 651 / 2400   loss: 0.40737593\n",
      "iteration: 652 / 2400   loss: 0.7529883\n",
      "iteration: 653 / 2400   loss: 0.28705466\n",
      "iteration: 654 / 2400   loss: 0.4702451\n",
      "iteration: 655 / 2400   loss: 0.48718584\n",
      "iteration: 656 / 2400   loss: 0.77858317\n",
      "iteration: 657 / 2400   loss: 0.40996605\n",
      "iteration: 658 / 2400   loss: 0.50790113\n",
      "iteration: 659 / 2400   loss: 0.45751372\n",
      "iteration: 660 / 2400   loss: 0.47562256\n",
      "iteration: 661 / 2400   loss: 0.2487302\n",
      "iteration: 662 / 2400   loss: 0.7981265\n",
      "iteration: 663 / 2400   loss: 0.51872027\n",
      "iteration: 664 / 2400   loss: 0.39783165\n",
      "iteration: 665 / 2400   loss: 0.33187675\n",
      "iteration: 666 / 2400   loss: 0.5873241\n",
      "iteration: 667 / 2400   loss: 0.46676272\n",
      "iteration: 668 / 2400   loss: 0.7247244\n",
      "iteration: 669 / 2400   loss: 0.4977976\n",
      "iteration: 670 / 2400   loss: 0.41970876\n",
      "iteration: 671 / 2400   loss: 0.72409666\n",
      "iteration: 672 / 2400   loss: 0.76433814\n",
      "iteration: 673 / 2400   loss: 0.69295657\n",
      "iteration: 674 / 2400   loss: 0.5733514\n",
      "iteration: 675 / 2400   loss: 0.5056929\n",
      "iteration: 676 / 2400   loss: 0.7539929\n",
      "iteration: 677 / 2400   loss: 0.74638015\n",
      "iteration: 678 / 2400   loss: 0.6066977\n",
      "iteration: 679 / 2400   loss: 0.89357966\n",
      "iteration: 680 / 2400   loss: 0.6385728\n",
      "iteration: 681 / 2400   loss: 0.60568464\n",
      "iteration: 682 / 2400   loss: 0.6372368\n",
      "iteration: 683 / 2400   loss: 0.4714157\n",
      "iteration: 684 / 2400   loss: 0.64512616\n",
      "iteration: 685 / 2400   loss: 0.48926336\n",
      "iteration: 686 / 2400   loss: 0.74102855\n",
      "iteration: 687 / 2400   loss: 0.33961204\n",
      "iteration: 688 / 2400   loss: 0.6304323\n",
      "iteration: 689 / 2400   loss: 1.1565775\n",
      "iteration: 690 / 2400   loss: 0.48306355\n",
      "iteration: 691 / 2400   loss: 0.19910315\n",
      "iteration: 692 / 2400   loss: 0.60478115\n",
      "iteration: 693 / 2400   loss: 0.2807868\n",
      "iteration: 694 / 2400   loss: 0.26733798\n",
      "iteration: 695 / 2400   loss: 0.47329533\n",
      "iteration: 696 / 2400   loss: 0.97933304\n",
      "iteration: 697 / 2400   loss: 0.3165674\n",
      "iteration: 698 / 2400   loss: 0.439587\n",
      "iteration: 699 / 2400   loss: 0.48861763\n",
      "iteration: 700 / 2400   loss: 0.93313235\n",
      "iteration: 701 / 2400   loss: 0.8774954\n",
      "iteration: 702 / 2400   loss: 1.1977774\n",
      "iteration: 703 / 2400   loss: 0.61789685\n",
      "iteration: 704 / 2400   loss: 1.0818287\n",
      "iteration: 705 / 2400   loss: 0.4869447\n",
      "iteration: 706 / 2400   loss: 0.6955697\n",
      "iteration: 707 / 2400   loss: 0.46141258\n",
      "iteration: 708 / 2400   loss: 0.7060838\n",
      "iteration: 709 / 2400   loss: 1.0359867\n",
      "iteration: 710 / 2400   loss: 0.9185146\n",
      "iteration: 711 / 2400   loss: 0.8661301\n",
      "iteration: 712 / 2400   loss: 0.56187963\n",
      "iteration: 713 / 2400   loss: 0.68053544\n",
      "iteration: 714 / 2400   loss: 0.40376368\n",
      "iteration: 715 / 2400   loss: 0.8472211\n",
      "iteration: 716 / 2400   loss: 0.6990895\n",
      "iteration: 717 / 2400   loss: 0.6700241\n",
      "iteration: 718 / 2400   loss: 0.755533\n",
      "iteration: 719 / 2400   loss: 0.606729\n",
      "iteration: 720 / 2400   loss: 0.29883966\n",
      "iteration: 721 / 2400   loss: 0.37234375\n",
      "iteration: 722 / 2400   loss: 0.4425737\n",
      "iteration: 723 / 2400   loss: 0.3871864\n",
      "iteration: 724 / 2400   loss: 0.2757898\n",
      "iteration: 725 / 2400   loss: 0.62385786\n",
      "iteration: 726 / 2400   loss: 0.34157345\n",
      "iteration: 727 / 2400   loss: 0.5485398\n",
      "iteration: 728 / 2400   loss: 0.26080516\n",
      "iteration: 729 / 2400   loss: 0.6574525\n",
      "iteration: 730 / 2400   loss: 0.7668145\n",
      "iteration: 731 / 2400   loss: 0.2054481\n",
      "iteration: 732 / 2400   loss: 0.22481757\n",
      "iteration: 733 / 2400   loss: 0.4513373\n",
      "iteration: 734 / 2400   loss: 0.53068\n",
      "iteration: 735 / 2400   loss: 0.3402018\n",
      "iteration: 736 / 2400   loss: 0.6845461\n",
      "iteration: 737 / 2400   loss: 0.6559099\n",
      "iteration: 738 / 2400   loss: 0.66448313\n",
      "iteration: 739 / 2400   loss: 0.61110866\n",
      "iteration: 740 / 2400   loss: 0.60718024\n",
      "iteration: 741 / 2400   loss: 0.5404403\n",
      "iteration: 742 / 2400   loss: 0.62359166\n",
      "iteration: 743 / 2400   loss: 0.5887506\n",
      "iteration: 744 / 2400   loss: 0.8079883\n",
      "iteration: 745 / 2400   loss: 0.3449623\n",
      "iteration: 746 / 2400   loss: 0.3976632\n",
      "iteration: 747 / 2400   loss: 0.611088\n",
      "iteration: 748 / 2400   loss: 0.8532671\n",
      "iteration: 749 / 2400   loss: 0.7546921\n",
      "iteration: 750 / 2400   loss: 0.31817496\n",
      "iteration: 751 / 2400   loss: 0.64279836\n",
      "iteration: 752 / 2400   loss: 0.45230418\n",
      "iteration: 753 / 2400   loss: 0.5449284\n",
      "iteration: 754 / 2400   loss: 0.39627624\n",
      "iteration: 755 / 2400   loss: 0.63287663\n",
      "iteration: 756 / 2400   loss: 0.23249607\n",
      "iteration: 757 / 2400   loss: 0.295926\n",
      "iteration: 758 / 2400   loss: 0.50545573\n",
      "iteration: 759 / 2400   loss: 0.3673082\n",
      "iteration: 760 / 2400   loss: 0.4973813\n",
      "iteration: 761 / 2400   loss: 0.41540796\n",
      "iteration: 762 / 2400   loss: 0.7058756\n",
      "iteration: 763 / 2400   loss: 0.37290204\n",
      "iteration: 764 / 2400   loss: 0.50766623\n",
      "iteration: 765 / 2400   loss: 0.3939992\n",
      "iteration: 766 / 2400   loss: 0.44354913\n",
      "iteration: 767 / 2400   loss: 0.46728194\n",
      "iteration: 768 / 2400   loss: 1.0164739\n",
      "iteration: 769 / 2400   loss: 0.5770097\n",
      "iteration: 770 / 2400   loss: 0.60738647\n",
      "iteration: 771 / 2400   loss: 0.37206292\n",
      "iteration: 772 / 2400   loss: 0.38521206\n",
      "iteration: 773 / 2400   loss: 0.30580175\n",
      "iteration: 774 / 2400   loss: 0.30726585\n",
      "iteration: 775 / 2400   loss: 0.4903607\n",
      "iteration: 776 / 2400   loss: 0.2077116\n",
      "iteration: 777 / 2400   loss: 0.3811505\n",
      "iteration: 778 / 2400   loss: 0.3004521\n",
      "iteration: 779 / 2400   loss: 0.35848677\n",
      "iteration: 780 / 2400   loss: 0.40975943\n",
      "iteration: 781 / 2400   loss: 0.2875969\n",
      "iteration: 782 / 2400   loss: 0.4404108\n",
      "iteration: 783 / 2400   loss: 0.2665142\n",
      "iteration: 784 / 2400   loss: 0.4917556\n",
      "iteration: 785 / 2400   loss: 0.27245927\n",
      "iteration: 786 / 2400   loss: 0.13433366\n",
      "iteration: 787 / 2400   loss: 0.2271176\n",
      "iteration: 788 / 2400   loss: 0.37486884\n",
      "iteration: 789 / 2400   loss: 0.25409228\n",
      "iteration: 790 / 2400   loss: 0.33451426\n",
      "iteration: 791 / 2400   loss: 0.19876935\n",
      "iteration: 792 / 2400   loss: 0.65666914\n",
      "iteration: 793 / 2400   loss: 0.7162623\n",
      "iteration: 794 / 2400   loss: 0.4817361\n",
      "iteration: 795 / 2400   loss: 0.5404017\n",
      "iteration: 796 / 2400   loss: 0.57998693\n",
      "iteration: 797 / 2400   loss: 0.74037707\n",
      "iteration: 798 / 2400   loss: 0.67138827\n",
      "iteration: 799 / 2400   loss: 0.25007522\n",
      "iteration: 800 / 2400   loss: 0.31539655\n",
      "iteration: 801 / 2400   loss: 0.8369277\n",
      "iteration: 802 / 2400   loss: 0.52016985\n",
      "iteration: 803 / 2400   loss: 0.8778206\n",
      "iteration: 804 / 2400   loss: 0.36379117\n",
      "iteration: 805 / 2400   loss: 0.2443983\n",
      "iteration: 806 / 2400   loss: 0.14937246\n",
      "iteration: 807 / 2400   loss: 1.1677763\n",
      "iteration: 808 / 2400   loss: 0.5809151\n",
      "iteration: 809 / 2400   loss: 0.5350236\n",
      "iteration: 810 / 2400   loss: 0.63209355\n",
      "iteration: 811 / 2400   loss: 0.52204996\n",
      "iteration: 812 / 2400   loss: 0.5727177\n",
      "iteration: 813 / 2400   loss: 0.7442548\n",
      "iteration: 814 / 2400   loss: 0.26903236\n",
      "iteration: 815 / 2400   loss: 0.6670183\n",
      "iteration: 816 / 2400   loss: 0.4542136\n",
      "iteration: 817 / 2400   loss: 0.39023635\n",
      "iteration: 818 / 2400   loss: 0.39694917\n",
      "iteration: 819 / 2400   loss: 0.336741\n",
      "iteration: 820 / 2400   loss: 0.21117549\n",
      "iteration: 821 / 2400   loss: 0.317181\n",
      "iteration: 822 / 2400   loss: 0.49431327\n",
      "iteration: 823 / 2400   loss: 0.6305876\n",
      "iteration: 824 / 2400   loss: 0.45913374\n",
      "iteration: 825 / 2400   loss: 0.23061003\n",
      "iteration: 826 / 2400   loss: 0.4365262\n",
      "iteration: 827 / 2400   loss: 1.6992232\n",
      "iteration: 828 / 2400   loss: 0.22712332\n",
      "iteration: 829 / 2400   loss: 0.42330045\n",
      "iteration: 830 / 2400   loss: 0.3617561\n",
      "iteration: 831 / 2400   loss: 0.45332476\n",
      "iteration: 832 / 2400   loss: 1.1853775\n",
      "iteration: 833 / 2400   loss: 0.5077965\n",
      "iteration: 834 / 2400   loss: 0.31552804\n",
      "iteration: 835 / 2400   loss: 0.91125196\n",
      "iteration: 836 / 2400   loss: 0.32096994\n",
      "iteration: 837 / 2400   loss: 0.51289004\n",
      "iteration: 838 / 2400   loss: 0.57067025\n",
      "iteration: 839 / 2400   loss: 0.35476577\n",
      "iteration: 840 / 2400   loss: 0.41842887\n",
      "iteration: 841 / 2400   loss: 0.5309341\n",
      "iteration: 842 / 2400   loss: 0.4985491\n",
      "iteration: 843 / 2400   loss: 0.33522296\n",
      "iteration: 844 / 2400   loss: 0.24182156\n",
      "iteration: 845 / 2400   loss: 0.27740175\n",
      "iteration: 846 / 2400   loss: 0.45396522\n",
      "iteration: 847 / 2400   loss: 0.37712708\n",
      "iteration: 848 / 2400   loss: 0.24425481\n",
      "iteration: 849 / 2400   loss: 0.37526986\n",
      "iteration: 850 / 2400   loss: 0.2488129\n",
      "iteration: 851 / 2400   loss: 0.3184772\n",
      "iteration: 852 / 2400   loss: 0.36843807\n",
      "iteration: 853 / 2400   loss: 0.4573341\n",
      "iteration: 854 / 2400   loss: 0.5206666\n",
      "iteration: 855 / 2400   loss: 0.41385674\n",
      "iteration: 856 / 2400   loss: 0.36379096\n",
      "iteration: 857 / 2400   loss: 0.26723522\n",
      "iteration: 858 / 2400   loss: 0.63908976\n",
      "iteration: 859 / 2400   loss: 0.53028184\n",
      "iteration: 860 / 2400   loss: 0.3367463\n",
      "iteration: 861 / 2400   loss: 0.3445549\n",
      "iteration: 862 / 2400   loss: 0.46227273\n",
      "iteration: 863 / 2400   loss: 0.28622994\n",
      "iteration: 864 / 2400   loss: 0.8189514\n",
      "iteration: 865 / 2400   loss: 0.722211\n",
      "iteration: 866 / 2400   loss: 0.25598574\n",
      "iteration: 867 / 2400   loss: 0.36652175\n",
      "iteration: 868 / 2400   loss: 0.52506655\n",
      "iteration: 869 / 2400   loss: 0.39967975\n",
      "iteration: 870 / 2400   loss: 0.36712807\n",
      "iteration: 871 / 2400   loss: 0.4783739\n",
      "iteration: 872 / 2400   loss: 0.23313957\n",
      "iteration: 873 / 2400   loss: 0.26349807\n",
      "iteration: 874 / 2400   loss: 0.17980346\n",
      "iteration: 875 / 2400   loss: 0.3052449\n",
      "iteration: 876 / 2400   loss: 0.15640496\n",
      "iteration: 877 / 2400   loss: 0.20788513\n",
      "iteration: 878 / 2400   loss: 0.41214192\n",
      "iteration: 879 / 2400   loss: 0.3656221\n",
      "iteration: 880 / 2400   loss: 0.5647388\n",
      "iteration: 881 / 2400   loss: 0.19226274\n",
      "iteration: 882 / 2400   loss: 0.2614899\n",
      "iteration: 883 / 2400   loss: 0.30263224\n",
      "iteration: 884 / 2400   loss: 0.40123314\n",
      "iteration: 885 / 2400   loss: 0.52823204\n",
      "iteration: 886 / 2400   loss: 0.43071336\n",
      "iteration: 887 / 2400   loss: 0.25656098\n",
      "iteration: 888 / 2400   loss: 0.7354391\n",
      "iteration: 889 / 2400   loss: 0.95723784\n",
      "iteration: 890 / 2400   loss: 0.4440737\n",
      "iteration: 891 / 2400   loss: 0.34259185\n",
      "iteration: 892 / 2400   loss: 0.3734578\n",
      "iteration: 893 / 2400   loss: 0.3830742\n",
      "iteration: 894 / 2400   loss: 0.298474\n",
      "iteration: 895 / 2400   loss: 0.19769302\n",
      "iteration: 896 / 2400   loss: 0.26118526\n",
      "iteration: 897 / 2400   loss: 0.31049165\n",
      "iteration: 898 / 2400   loss: 0.3655803\n",
      "iteration: 899 / 2400   loss: 0.391098\n",
      "iteration: 900 / 2400   loss: 0.79723215\n",
      "iteration: 901 / 2400   loss: 0.66756445\n",
      "iteration: 902 / 2400   loss: 1.7848712\n",
      "iteration: 903 / 2400   loss: 0.7368289\n",
      "iteration: 904 / 2400   loss: 0.51947105\n",
      "iteration: 905 / 2400   loss: 0.936744\n",
      "iteration: 906 / 2400   loss: 0.99286115\n",
      "iteration: 907 / 2400   loss: 0.6096637\n",
      "iteration: 908 / 2400   loss: 0.6192925\n",
      "iteration: 909 / 2400   loss: 0.77113456\n",
      "iteration: 910 / 2400   loss: 0.9122944\n",
      "iteration: 911 / 2400   loss: 0.6983443\n",
      "iteration: 912 / 2400   loss: 0.42817298\n",
      "iteration: 913 / 2400   loss: 1.0372821\n",
      "iteration: 914 / 2400   loss: 0.85328555\n",
      "iteration: 915 / 2400   loss: 0.4575525\n",
      "iteration: 916 / 2400   loss: 0.6229256\n",
      "iteration: 917 / 2400   loss: 0.33475158\n",
      "iteration: 918 / 2400   loss: 0.24195121\n",
      "iteration: 919 / 2400   loss: 0.24043599\n",
      "iteration: 920 / 2400   loss: 0.36730537\n",
      "iteration: 921 / 2400   loss: 0.35752165\n",
      "iteration: 922 / 2400   loss: 0.3422527\n",
      "iteration: 923 / 2400   loss: 0.40580654\n",
      "iteration: 924 / 2400   loss: 0.35564896\n",
      "iteration: 925 / 2400   loss: 0.6936186\n",
      "iteration: 926 / 2400   loss: 0.23597541\n",
      "iteration: 927 / 2400   loss: 0.23453023\n",
      "iteration: 928 / 2400   loss: 0.5720427\n",
      "iteration: 929 / 2400   loss: 0.38656083\n",
      "iteration: 930 / 2400   loss: 0.21902123\n",
      "iteration: 931 / 2400   loss: 0.5802576\n",
      "iteration: 932 / 2400   loss: 0.2812112\n",
      "iteration: 933 / 2400   loss: 0.3478405\n",
      "iteration: 934 / 2400   loss: 0.30239815\n",
      "iteration: 935 / 2400   loss: 0.26464328\n",
      "iteration: 936 / 2400   loss: 0.55416024\n",
      "iteration: 937 / 2400   loss: 0.4519742\n",
      "iteration: 938 / 2400   loss: 0.55917645\n",
      "iteration: 939 / 2400   loss: 0.7349244\n",
      "iteration: 940 / 2400   loss: 0.2949667\n",
      "iteration: 941 / 2400   loss: 0.4294598\n",
      "iteration: 942 / 2400   loss: 0.20074928\n",
      "iteration: 943 / 2400   loss: 0.28307083\n",
      "iteration: 944 / 2400   loss: 0.278485\n",
      "iteration: 945 / 2400   loss: 0.3999476\n",
      "iteration: 946 / 2400   loss: 0.46056274\n",
      "iteration: 947 / 2400   loss: 0.3998818\n",
      "iteration: 948 / 2400   loss: 0.3403551\n",
      "iteration: 949 / 2400   loss: 0.41921958\n",
      "iteration: 950 / 2400   loss: 0.59380007\n",
      "iteration: 951 / 2400   loss: 0.1924815\n",
      "iteration: 952 / 2400   loss: 0.2934141\n",
      "iteration: 953 / 2400   loss: 0.7360853\n",
      "iteration: 954 / 2400   loss: 0.20489866\n",
      "iteration: 955 / 2400   loss: 0.52716917\n",
      "iteration: 956 / 2400   loss: 0.4338029\n",
      "iteration: 957 / 2400   loss: 0.29561466\n",
      "iteration: 958 / 2400   loss: 0.22036625\n",
      "iteration: 959 / 2400   loss: 0.1983173\n",
      "iteration: 960 / 2400   loss: 0.46207634\n",
      "iteration: 961 / 2400   loss: 0.18923424\n",
      "iteration: 962 / 2400   loss: 1.2670281\n",
      "iteration: 963 / 2400   loss: 0.72569597\n",
      "iteration: 964 / 2400   loss: 0.32257506\n",
      "iteration: 965 / 2400   loss: 0.34727263\n",
      "iteration: 966 / 2400   loss: 0.3389529\n",
      "iteration: 967 / 2400   loss: 0.32381222\n",
      "iteration: 968 / 2400   loss: 0.2416573\n",
      "iteration: 969 / 2400   loss: 0.6348488\n",
      "iteration: 970 / 2400   loss: 0.31600213\n",
      "iteration: 971 / 2400   loss: 0.5678803\n",
      "iteration: 972 / 2400   loss: 0.35316926\n",
      "iteration: 973 / 2400   loss: 0.49014434\n",
      "iteration: 974 / 2400   loss: 0.18813261\n",
      "iteration: 975 / 2400   loss: 0.2395375\n",
      "iteration: 976 / 2400   loss: 0.30911335\n",
      "iteration: 977 / 2400   loss: 0.6662079\n",
      "iteration: 978 / 2400   loss: 0.17207648\n",
      "iteration: 979 / 2400   loss: 0.35377982\n",
      "iteration: 980 / 2400   loss: 0.5184648\n",
      "iteration: 981 / 2400   loss: 0.5641127\n",
      "iteration: 982 / 2400   loss: 0.46159133\n",
      "iteration: 983 / 2400   loss: 0.27494243\n",
      "iteration: 984 / 2400   loss: 0.6830703\n",
      "iteration: 985 / 2400   loss: 0.70921385\n",
      "iteration: 986 / 2400   loss: 1.4452683\n",
      "iteration: 987 / 2400   loss: 0.6293225\n",
      "iteration: 988 / 2400   loss: 0.74172765\n",
      "iteration: 989 / 2400   loss: 0.5285933\n",
      "iteration: 990 / 2400   loss: 0.50859904\n",
      "iteration: 991 / 2400   loss: 0.4825044\n",
      "iteration: 992 / 2400   loss: 0.61224556\n",
      "iteration: 993 / 2400   loss: 0.17752124\n",
      "iteration: 994 / 2400   loss: 0.18986389\n",
      "iteration: 995 / 2400   loss: 0.5698995\n",
      "iteration: 996 / 2400   loss: 0.37734136\n",
      "iteration: 997 / 2400   loss: 0.35429543\n",
      "iteration: 998 / 2400   loss: 0.86760545\n",
      "iteration: 999 / 2400   loss: 0.40515596\n",
      "iteration: 1000 / 2400   loss: 0.36010826\n",
      "iteration: 1001 / 2400   loss: 0.38950104\n",
      "iteration: 1002 / 2400   loss: 0.4770453\n",
      "iteration: 1003 / 2400   loss: 0.41941184\n",
      "iteration: 1004 / 2400   loss: 0.25546047\n",
      "iteration: 1005 / 2400   loss: 0.15527758\n",
      "iteration: 1006 / 2400   loss: 0.32635438\n",
      "iteration: 1007 / 2400   loss: 0.4620124\n",
      "iteration: 1008 / 2400   loss: 0.24290179\n",
      "iteration: 1009 / 2400   loss: 0.49390984\n",
      "iteration: 1010 / 2400   loss: 0.58316636\n",
      "iteration: 1011 / 2400   loss: 0.27813902\n",
      "iteration: 1012 / 2400   loss: 0.402227\n",
      "iteration: 1013 / 2400   loss: 0.43768677\n",
      "iteration: 1014 / 2400   loss: 0.39636457\n",
      "iteration: 1015 / 2400   loss: 0.1694622\n",
      "iteration: 1016 / 2400   loss: 0.06493389\n",
      "iteration: 1017 / 2400   loss: 0.31417057\n",
      "iteration: 1018 / 2400   loss: 0.114039525\n",
      "iteration: 1019 / 2400   loss: 0.24715753\n",
      "iteration: 1020 / 2400   loss: 0.38526797\n",
      "iteration: 1021 / 2400   loss: 0.23845722\n",
      "iteration: 1022 / 2400   loss: 0.63473225\n",
      "iteration: 1023 / 2400   loss: 0.24496742\n",
      "iteration: 1024 / 2400   loss: 0.4237242\n",
      "iteration: 1025 / 2400   loss: 0.28383905\n",
      "iteration: 1026 / 2400   loss: 0.18365307\n",
      "iteration: 1027 / 2400   loss: 0.26275358\n",
      "iteration: 1028 / 2400   loss: 0.40742755\n",
      "iteration: 1029 / 2400   loss: 0.37513354\n",
      "iteration: 1030 / 2400   loss: 0.33780497\n",
      "iteration: 1031 / 2400   loss: 0.36522934\n",
      "iteration: 1032 / 2400   loss: 0.5707555\n",
      "iteration: 1033 / 2400   loss: 1.0779216\n",
      "iteration: 1034 / 2400   loss: 0.35239032\n",
      "iteration: 1035 / 2400   loss: 0.4482998\n",
      "iteration: 1036 / 2400   loss: 0.38235912\n",
      "iteration: 1037 / 2400   loss: 1.0375073\n",
      "iteration: 1038 / 2400   loss: 0.6924899\n",
      "iteration: 1039 / 2400   loss: 0.29009083\n",
      "iteration: 1040 / 2400   loss: 0.3425388\n",
      "iteration: 1041 / 2400   loss: 0.15653698\n",
      "iteration: 1042 / 2400   loss: 0.21727085\n",
      "iteration: 1043 / 2400   loss: 0.69265854\n",
      "iteration: 1044 / 2400   loss: 0.10231491\n",
      "iteration: 1045 / 2400   loss: 0.21875675\n",
      "iteration: 1046 / 2400   loss: 0.25179708\n",
      "iteration: 1047 / 2400   loss: 0.18116651\n",
      "iteration: 1048 / 2400   loss: 0.27460524\n",
      "iteration: 1049 / 2400   loss: 0.4555611\n",
      "iteration: 1050 / 2400   loss: 0.24466535\n",
      "iteration: 1051 / 2400   loss: 0.4190336\n",
      "iteration: 1052 / 2400   loss: 0.19770922\n",
      "iteration: 1053 / 2400   loss: 0.24650967\n",
      "iteration: 1054 / 2400   loss: 0.20781869\n",
      "iteration: 1055 / 2400   loss: 0.44513434\n",
      "iteration: 1056 / 2400   loss: 1.6075352\n",
      "iteration: 1057 / 2400   loss: 0.4490017\n",
      "iteration: 1058 / 2400   loss: 0.52527857\n",
      "iteration: 1059 / 2400   loss: 0.3425046\n",
      "iteration: 1060 / 2400   loss: 0.36764058\n",
      "iteration: 1061 / 2400   loss: 0.43815452\n",
      "iteration: 1062 / 2400   loss: 0.46688807\n",
      "iteration: 1063 / 2400   loss: 0.40147725\n",
      "iteration: 1064 / 2400   loss: 0.2858948\n",
      "iteration: 1065 / 2400   loss: 0.53034645\n",
      "iteration: 1066 / 2400   loss: 0.51643324\n",
      "iteration: 1067 / 2400   loss: 0.24955438\n",
      "iteration: 1068 / 2400   loss: 0.22917518\n",
      "iteration: 1069 / 2400   loss: 0.34035537\n",
      "iteration: 1070 / 2400   loss: 0.49741477\n",
      "iteration: 1071 / 2400   loss: 0.6772447\n",
      "iteration: 1072 / 2400   loss: 0.30385467\n",
      "iteration: 1073 / 2400   loss: 0.21306713\n",
      "iteration: 1074 / 2400   loss: 0.34168965\n",
      "iteration: 1075 / 2400   loss: 0.41287488\n",
      "iteration: 1076 / 2400   loss: 0.20403138\n",
      "iteration: 1077 / 2400   loss: 0.22072548\n",
      "iteration: 1078 / 2400   loss: 0.3646904\n",
      "iteration: 1079 / 2400   loss: 0.18167868\n",
      "iteration: 1080 / 2400   loss: 0.23487432\n",
      "iteration: 1081 / 2400   loss: 0.17691061\n",
      "iteration: 1082 / 2400   loss: 0.19742222\n",
      "iteration: 1083 / 2400   loss: 0.30234906\n",
      "iteration: 1084 / 2400   loss: 0.5958209\n",
      "iteration: 1085 / 2400   loss: 0.6594562\n",
      "iteration: 1086 / 2400   loss: 0.24060059\n",
      "iteration: 1087 / 2400   loss: 1.2157027\n",
      "iteration: 1088 / 2400   loss: 1.0374211\n",
      "iteration: 1089 / 2400   loss: 0.7088672\n",
      "iteration: 1090 / 2400   loss: 0.54852086\n",
      "iteration: 1091 / 2400   loss: 0.424655\n",
      "iteration: 1092 / 2400   loss: 0.50327086\n",
      "iteration: 1093 / 2400   loss: 0.47431374\n",
      "iteration: 1094 / 2400   loss: 0.419814\n",
      "iteration: 1095 / 2400   loss: 0.6907806\n",
      "iteration: 1096 / 2400   loss: 0.29472667\n",
      "iteration: 1097 / 2400   loss: 0.44138253\n",
      "iteration: 1098 / 2400   loss: 0.28091455\n",
      "iteration: 1099 / 2400   loss: 0.25798666\n",
      "iteration: 1100 / 2400   loss: 0.36243197\n",
      "iteration: 1101 / 2400   loss: 0.9267033\n",
      "iteration: 1102 / 2400   loss: 0.43160307\n",
      "iteration: 1103 / 2400   loss: 0.2384589\n",
      "iteration: 1104 / 2400   loss: 0.517074\n",
      "iteration: 1105 / 2400   loss: 0.29595444\n",
      "iteration: 1106 / 2400   loss: 0.25921267\n",
      "iteration: 1107 / 2400   loss: 0.27139604\n",
      "iteration: 1108 / 2400   loss: 0.3175754\n",
      "iteration: 1109 / 2400   loss: 0.41676545\n",
      "iteration: 1110 / 2400   loss: 0.52382684\n",
      "iteration: 1111 / 2400   loss: 0.25969318\n",
      "iteration: 1112 / 2400   loss: 0.27073872\n",
      "iteration: 1113 / 2400   loss: 0.41129518\n",
      "iteration: 1114 / 2400   loss: 0.45663407\n",
      "iteration: 1115 / 2400   loss: 0.17695722\n",
      "iteration: 1116 / 2400   loss: 0.43722647\n",
      "iteration: 1117 / 2400   loss: 0.31385776\n",
      "iteration: 1118 / 2400   loss: 0.18526444\n",
      "iteration: 1119 / 2400   loss: 0.35771918\n",
      "iteration: 1120 / 2400   loss: 0.26917794\n",
      "iteration: 1121 / 2400   loss: 0.30600348\n",
      "iteration: 1122 / 2400   loss: 0.21658579\n",
      "iteration: 1123 / 2400   loss: 0.3250818\n",
      "iteration: 1124 / 2400   loss: 0.1992842\n",
      "iteration: 1125 / 2400   loss: 0.20123367\n",
      "iteration: 1126 / 2400   loss: 0.21392868\n",
      "iteration: 1127 / 2400   loss: 0.2902095\n",
      "iteration: 1128 / 2400   loss: 0.60259825\n",
      "iteration: 1129 / 2400   loss: 0.23883754\n",
      "iteration: 1130 / 2400   loss: 0.3464482\n",
      "iteration: 1131 / 2400   loss: 0.19833267\n",
      "iteration: 1132 / 2400   loss: 0.26863462\n",
      "iteration: 1133 / 2400   loss: 0.2657472\n",
      "iteration: 1134 / 2400   loss: 0.20855826\n",
      "iteration: 1135 / 2400   loss: 0.7626834\n",
      "iteration: 1136 / 2400   loss: 0.8242363\n",
      "iteration: 1137 / 2400   loss: 0.7366117\n",
      "iteration: 1138 / 2400   loss: 0.15314005\n",
      "iteration: 1139 / 2400   loss: 0.36141646\n",
      "iteration: 1140 / 2400   loss: 0.07583776\n",
      "iteration: 1141 / 2400   loss: 0.2806172\n",
      "iteration: 1142 / 2400   loss: 0.30085218\n",
      "iteration: 1143 / 2400   loss: 0.37687874\n",
      "iteration: 1144 / 2400   loss: 0.518728\n",
      "iteration: 1145 / 2400   loss: 0.35963318\n",
      "iteration: 1146 / 2400   loss: 0.50629944\n",
      "iteration: 1147 / 2400   loss: 0.7672849\n",
      "iteration: 1148 / 2400   loss: 0.21986492\n",
      "iteration: 1149 / 2400   loss: 0.3992891\n",
      "iteration: 1150 / 2400   loss: 0.25301576\n",
      "iteration: 1151 / 2400   loss: 0.27372146\n",
      "iteration: 1152 / 2400   loss: 0.2901246\n",
      "iteration: 1153 / 2400   loss: 0.1951525\n",
      "iteration: 1154 / 2400   loss: 0.3252209\n",
      "iteration: 1155 / 2400   loss: 0.28539687\n",
      "iteration: 1156 / 2400   loss: 0.22644502\n",
      "iteration: 1157 / 2400   loss: 0.21037549\n",
      "iteration: 1158 / 2400   loss: 0.4149575\n",
      "iteration: 1159 / 2400   loss: 0.34630036\n",
      "iteration: 1160 / 2400   loss: 0.3679401\n",
      "iteration: 1161 / 2400   loss: 0.28527218\n",
      "iteration: 1162 / 2400   loss: 0.23971993\n",
      "iteration: 1163 / 2400   loss: 0.75260985\n",
      "iteration: 1164 / 2400   loss: 0.33576366\n",
      "iteration: 1165 / 2400   loss: 0.548691\n",
      "iteration: 1166 / 2400   loss: 0.48315486\n",
      "iteration: 1167 / 2400   loss: 0.3175902\n",
      "iteration: 1168 / 2400   loss: 0.58699656\n",
      "iteration: 1169 / 2400   loss: 0.29211116\n",
      "iteration: 1170 / 2400   loss: 0.3389719\n",
      "iteration: 1171 / 2400   loss: 0.122871116\n",
      "iteration: 1172 / 2400   loss: 0.46372864\n",
      "iteration: 1173 / 2400   loss: 1.1981235\n",
      "iteration: 1174 / 2400   loss: 0.6801884\n",
      "iteration: 1175 / 2400   loss: 0.34680548\n",
      "iteration: 1176 / 2400   loss: 0.45871192\n",
      "iteration: 1177 / 2400   loss: 0.19470364\n",
      "iteration: 1178 / 2400   loss: 1.0197195\n",
      "iteration: 1179 / 2400   loss: 0.35094154\n",
      "iteration: 1180 / 2400   loss: 0.56203043\n",
      "iteration: 1181 / 2400   loss: 0.18258771\n",
      "iteration: 1182 / 2400   loss: 0.19621132\n",
      "iteration: 1183 / 2400   loss: 0.25551683\n",
      "iteration: 1184 / 2400   loss: 0.34320283\n",
      "iteration: 1185 / 2400   loss: 0.435391\n",
      "iteration: 1186 / 2400   loss: 0.4866291\n",
      "iteration: 1187 / 2400   loss: 0.5581199\n",
      "iteration: 1188 / 2400   loss: 0.1498322\n",
      "iteration: 1189 / 2400   loss: 0.82611054\n",
      "iteration: 1190 / 2400   loss: 0.4054564\n",
      "iteration: 1191 / 2400   loss: 0.47252265\n",
      "iteration: 1192 / 2400   loss: 0.22715253\n",
      "iteration: 1193 / 2400   loss: 0.25207534\n",
      "iteration: 1194 / 2400   loss: 0.52423185\n",
      "iteration: 1195 / 2400   loss: 0.6647462\n",
      "iteration: 1196 / 2400   loss: 0.6722581\n",
      "iteration: 1197 / 2400   loss: 0.93691194\n",
      "iteration: 1198 / 2400   loss: 0.60465956\n",
      "iteration: 1199 / 2400   loss: 0.5665178\n",
      "iteration: 1200 / 2400   loss: 0.38663918\n",
      "iteration: 1201 / 2400   loss: 0.3284338\n",
      "iteration: 1202 / 2400   loss: 0.31713048\n",
      "iteration: 1203 / 2400   loss: 0.5986689\n",
      "iteration: 1204 / 2400   loss: 0.42343116\n",
      "iteration: 1205 / 2400   loss: 0.5352118\n",
      "iteration: 1206 / 2400   loss: 0.39449674\n",
      "iteration: 1207 / 2400   loss: 0.40041518\n",
      "iteration: 1208 / 2400   loss: 0.49931297\n",
      "iteration: 1209 / 2400   loss: 0.54334444\n",
      "iteration: 1210 / 2400   loss: 0.2974119\n",
      "iteration: 1211 / 2400   loss: 0.16519766\n",
      "iteration: 1212 / 2400   loss: 0.13701291\n",
      "iteration: 1213 / 2400   loss: 0.2925839\n",
      "iteration: 1214 / 2400   loss: 0.2781275\n",
      "iteration: 1215 / 2400   loss: 0.22621517\n",
      "iteration: 1216 / 2400   loss: 0.14067958\n",
      "iteration: 1217 / 2400   loss: 0.15158084\n",
      "iteration: 1218 / 2400   loss: 0.41529202\n",
      "iteration: 1219 / 2400   loss: 0.55430865\n",
      "iteration: 1220 / 2400   loss: 0.2779854\n",
      "iteration: 1221 / 2400   loss: 0.99304885\n",
      "iteration: 1222 / 2400   loss: 0.3291236\n",
      "iteration: 1223 / 2400   loss: 0.29682687\n",
      "iteration: 1224 / 2400   loss: 0.36397964\n",
      "iteration: 1225 / 2400   loss: 0.5475821\n",
      "iteration: 1226 / 2400   loss: 0.43531558\n",
      "iteration: 1227 / 2400   loss: 0.35844094\n",
      "iteration: 1228 / 2400   loss: 0.33262402\n",
      "iteration: 1229 / 2400   loss: 0.3340878\n",
      "iteration: 1230 / 2400   loss: 0.19154015\n",
      "iteration: 1231 / 2400   loss: 0.38100913\n",
      "iteration: 1232 / 2400   loss: 0.48568836\n",
      "iteration: 1233 / 2400   loss: 0.17561002\n",
      "iteration: 1234 / 2400   loss: 0.22750698\n",
      "iteration: 1235 / 2400   loss: 0.5846746\n",
      "iteration: 1236 / 2400   loss: 0.6663378\n",
      "iteration: 1237 / 2400   loss: 0.43878105\n",
      "iteration: 1238 / 2400   loss: 0.23123291\n",
      "iteration: 1239 / 2400   loss: 0.47542202\n",
      "iteration: 1240 / 2400   loss: 0.39488578\n",
      "iteration: 1241 / 2400   loss: 0.435427\n",
      "iteration: 1242 / 2400   loss: 0.4619722\n",
      "iteration: 1243 / 2400   loss: 0.44064385\n",
      "iteration: 1244 / 2400   loss: 0.29770374\n",
      "iteration: 1245 / 2400   loss: 0.28201276\n",
      "iteration: 1246 / 2400   loss: 1.2178253\n",
      "iteration: 1247 / 2400   loss: 0.115314595\n",
      "iteration: 1248 / 2400   loss: 1.0070586\n",
      "iteration: 1249 / 2400   loss: 0.14413208\n",
      "iteration: 1250 / 2400   loss: 0.28201848\n",
      "iteration: 1251 / 2400   loss: 0.84817475\n",
      "iteration: 1252 / 2400   loss: 0.36183304\n",
      "iteration: 1253 / 2400   loss: 0.7470137\n",
      "iteration: 1254 / 2400   loss: 0.655383\n",
      "iteration: 1255 / 2400   loss: 0.23137449\n",
      "iteration: 1256 / 2400   loss: 0.5399746\n",
      "iteration: 1257 / 2400   loss: 0.3290912\n",
      "iteration: 1258 / 2400   loss: 0.32893312\n",
      "iteration: 1259 / 2400   loss: 0.35398906\n",
      "iteration: 1260 / 2400   loss: 0.2301601\n",
      "iteration: 1261 / 2400   loss: 0.54719085\n",
      "iteration: 1262 / 2400   loss: 0.1088846\n",
      "iteration: 1263 / 2400   loss: 0.3699207\n",
      "iteration: 1264 / 2400   loss: 0.7497104\n",
      "iteration: 1265 / 2400   loss: 0.50988275\n",
      "iteration: 1266 / 2400   loss: 0.2658194\n",
      "iteration: 1267 / 2400   loss: 0.7521483\n",
      "iteration: 1268 / 2400   loss: 0.23634118\n",
      "iteration: 1269 / 2400   loss: 0.76378274\n",
      "iteration: 1270 / 2400   loss: 0.33105937\n",
      "iteration: 1271 / 2400   loss: 0.17651363\n",
      "iteration: 1272 / 2400   loss: 0.24082798\n",
      "iteration: 1273 / 2400   loss: 0.12952766\n",
      "iteration: 1274 / 2400   loss: 0.12687826\n",
      "iteration: 1275 / 2400   loss: 0.36103252\n",
      "iteration: 1276 / 2400   loss: 0.23378368\n",
      "iteration: 1277 / 2400   loss: 0.16137306\n",
      "iteration: 1278 / 2400   loss: 0.55662936\n",
      "iteration: 1279 / 2400   loss: 0.7212345\n",
      "iteration: 1280 / 2400   loss: 0.10674732\n",
      "iteration: 1281 / 2400   loss: 0.52867645\n",
      "iteration: 1282 / 2400   loss: 0.09218821\n",
      "iteration: 1283 / 2400   loss: 0.9292982\n",
      "iteration: 1284 / 2400   loss: 0.14415795\n",
      "iteration: 1285 / 2400   loss: 0.30582535\n",
      "iteration: 1286 / 2400   loss: 0.22192948\n",
      "iteration: 1287 / 2400   loss: 0.36142474\n",
      "iteration: 1288 / 2400   loss: 0.19581322\n",
      "iteration: 1289 / 2400   loss: 0.56264395\n",
      "iteration: 1290 / 2400   loss: 0.13867317\n",
      "iteration: 1291 / 2400   loss: 0.47591928\n",
      "iteration: 1292 / 2400   loss: 0.42431533\n",
      "iteration: 1293 / 2400   loss: 0.2962283\n",
      "iteration: 1294 / 2400   loss: 0.48572624\n",
      "iteration: 1295 / 2400   loss: 0.28809455\n",
      "iteration: 1296 / 2400   loss: 0.37754142\n",
      "iteration: 1297 / 2400   loss: 0.5521815\n",
      "iteration: 1298 / 2400   loss: 0.44572535\n",
      "iteration: 1299 / 2400   loss: 0.3308757\n",
      "iteration: 1300 / 2400   loss: 0.4910656\n",
      "iteration: 1301 / 2400   loss: 0.58592695\n",
      "iteration: 1302 / 2400   loss: 0.16303164\n",
      "iteration: 1303 / 2400   loss: 0.4703798\n",
      "iteration: 1304 / 2400   loss: 0.11015203\n",
      "iteration: 1305 / 2400   loss: 0.35336268\n",
      "iteration: 1306 / 2400   loss: 0.36900994\n",
      "iteration: 1307 / 2400   loss: 0.5072629\n",
      "iteration: 1308 / 2400   loss: 0.18283086\n",
      "iteration: 1309 / 2400   loss: 0.24574125\n",
      "iteration: 1310 / 2400   loss: 0.63658774\n",
      "iteration: 1311 / 2400   loss: 0.32787108\n",
      "iteration: 1312 / 2400   loss: 0.3468108\n",
      "iteration: 1313 / 2400   loss: 0.5515294\n",
      "iteration: 1314 / 2400   loss: 0.18121794\n",
      "iteration: 1315 / 2400   loss: 0.26427802\n",
      "iteration: 1316 / 2400   loss: 0.32649568\n",
      "iteration: 1317 / 2400   loss: 0.31463668\n",
      "iteration: 1318 / 2400   loss: 0.1273449\n",
      "iteration: 1319 / 2400   loss: 0.05853363\n",
      "iteration: 1320 / 2400   loss: 0.08660445\n",
      "iteration: 1321 / 2400   loss: 0.09891182\n",
      "iteration: 1322 / 2400   loss: 0.29138017\n",
      "iteration: 1323 / 2400   loss: 0.23546204\n",
      "iteration: 1324 / 2400   loss: 0.14353552\n",
      "iteration: 1325 / 2400   loss: 0.19029179\n",
      "iteration: 1326 / 2400   loss: 0.320808\n",
      "iteration: 1327 / 2400   loss: 0.16104583\n",
      "iteration: 1328 / 2400   loss: 0.23435761\n",
      "iteration: 1329 / 2400   loss: 0.30003202\n",
      "iteration: 1330 / 2400   loss: 0.5604718\n",
      "iteration: 1331 / 2400   loss: 0.16052231\n",
      "iteration: 1332 / 2400   loss: 0.15944025\n",
      "iteration: 1333 / 2400   loss: 0.35957727\n",
      "iteration: 1334 / 2400   loss: 0.29026923\n",
      "iteration: 1335 / 2400   loss: 0.3202985\n",
      "iteration: 1336 / 2400   loss: 0.50432694\n",
      "iteration: 1337 / 2400   loss: 0.20223339\n",
      "iteration: 1338 / 2400   loss: 0.21228527\n",
      "iteration: 1339 / 2400   loss: 0.152255\n",
      "iteration: 1340 / 2400   loss: 0.21887268\n",
      "iteration: 1341 / 2400   loss: 0.23082012\n",
      "iteration: 1342 / 2400   loss: 0.14907497\n",
      "iteration: 1343 / 2400   loss: 0.42539176\n",
      "iteration: 1344 / 2400   loss: 0.30371708\n",
      "iteration: 1345 / 2400   loss: 0.321314\n",
      "iteration: 1346 / 2400   loss: 0.19823417\n",
      "iteration: 1347 / 2400   loss: 0.18459125\n",
      "iteration: 1348 / 2400   loss: 0.2646134\n",
      "iteration: 1349 / 2400   loss: 0.11520973\n",
      "iteration: 1350 / 2400   loss: 0.1640458\n",
      "iteration: 1351 / 2400   loss: 0.35150552\n",
      "iteration: 1352 / 2400   loss: 0.3486196\n",
      "iteration: 1353 / 2400   loss: 0.18822274\n",
      "iteration: 1354 / 2400   loss: 0.18253028\n",
      "iteration: 1355 / 2400   loss: 0.26701835\n",
      "iteration: 1356 / 2400   loss: 0.09115612\n",
      "iteration: 1357 / 2400   loss: 0.24911842\n",
      "iteration: 1358 / 2400   loss: 0.23177616\n",
      "iteration: 1359 / 2400   loss: 0.15466715\n",
      "iteration: 1360 / 2400   loss: 0.27991256\n",
      "iteration: 1361 / 2400   loss: 0.62600994\n",
      "iteration: 1362 / 2400   loss: 0.35428196\n",
      "iteration: 1363 / 2400   loss: 0.40800488\n",
      "iteration: 1364 / 2400   loss: 0.17280981\n",
      "iteration: 1365 / 2400   loss: 0.3949312\n",
      "iteration: 1366 / 2400   loss: 0.1903371\n",
      "iteration: 1367 / 2400   loss: 0.15985984\n",
      "iteration: 1368 / 2400   loss: 0.24450473\n",
      "iteration: 1369 / 2400   loss: 0.49721527\n",
      "iteration: 1370 / 2400   loss: 0.22104935\n",
      "iteration: 1371 / 2400   loss: 0.22684963\n",
      "iteration: 1372 / 2400   loss: 0.10806013\n",
      "iteration: 1373 / 2400   loss: 0.28654298\n",
      "iteration: 1374 / 2400   loss: 0.27097565\n",
      "iteration: 1375 / 2400   loss: 0.32671645\n",
      "iteration: 1376 / 2400   loss: 0.27166754\n",
      "iteration: 1377 / 2400   loss: 0.49737343\n",
      "iteration: 1378 / 2400   loss: 0.28324512\n",
      "iteration: 1379 / 2400   loss: 0.124976575\n",
      "iteration: 1380 / 2400   loss: 0.4718849\n",
      "iteration: 1381 / 2400   loss: 0.23069565\n",
      "iteration: 1382 / 2400   loss: 0.3740884\n",
      "iteration: 1383 / 2400   loss: 0.48086223\n",
      "iteration: 1384 / 2400   loss: 0.19524376\n",
      "iteration: 1385 / 2400   loss: 0.34225407\n",
      "iteration: 1386 / 2400   loss: 0.56323975\n",
      "iteration: 1387 / 2400   loss: 0.6697189\n",
      "iteration: 1388 / 2400   loss: 0.40933242\n",
      "iteration: 1389 / 2400   loss: 0.61986697\n",
      "iteration: 1390 / 2400   loss: 0.35138398\n",
      "iteration: 1391 / 2400   loss: 0.47509655\n",
      "iteration: 1392 / 2400   loss: 0.23385029\n",
      "iteration: 1393 / 2400   loss: 0.522311\n",
      "iteration: 1394 / 2400   loss: 0.80033857\n",
      "iteration: 1395 / 2400   loss: 0.60874486\n",
      "iteration: 1396 / 2400   loss: 0.15833181\n",
      "iteration: 1397 / 2400   loss: 0.34141654\n",
      "iteration: 1398 / 2400   loss: 0.14983469\n",
      "iteration: 1399 / 2400   loss: 0.19563022\n",
      "iteration: 1400 / 2400   loss: 0.2539094\n",
      "iteration: 1401 / 2400   loss: 0.29688165\n",
      "iteration: 1402 / 2400   loss: 0.13848741\n",
      "iteration: 1403 / 2400   loss: 0.4054226\n",
      "iteration: 1404 / 2400   loss: 0.23849922\n",
      "iteration: 1405 / 2400   loss: 0.20725437\n",
      "iteration: 1406 / 2400   loss: 0.4362317\n",
      "iteration: 1407 / 2400   loss: 0.20150782\n",
      "iteration: 1408 / 2400   loss: 0.26213765\n",
      "iteration: 1409 / 2400   loss: 0.30248365\n",
      "iteration: 1410 / 2400   loss: 1.0262678\n",
      "iteration: 1411 / 2400   loss: 0.13503148\n",
      "iteration: 1412 / 2400   loss: 0.20471786\n",
      "iteration: 1413 / 2400   loss: 0.542559\n",
      "iteration: 1414 / 2400   loss: 0.23698772\n",
      "iteration: 1415 / 2400   loss: 0.23405853\n",
      "iteration: 1416 / 2400   loss: 0.09196507\n",
      "iteration: 1417 / 2400   loss: 0.4399865\n",
      "iteration: 1418 / 2400   loss: 0.15168677\n",
      "iteration: 1419 / 2400   loss: 0.2634166\n",
      "iteration: 1420 / 2400   loss: 0.22725506\n",
      "iteration: 1421 / 2400   loss: 0.11597936\n",
      "iteration: 1422 / 2400   loss: 0.22604568\n",
      "iteration: 1423 / 2400   loss: 0.16777939\n",
      "iteration: 1424 / 2400   loss: 0.15177494\n",
      "iteration: 1425 / 2400   loss: 0.565607\n",
      "iteration: 1426 / 2400   loss: 0.4310043\n",
      "iteration: 1427 / 2400   loss: 0.24403122\n",
      "iteration: 1428 / 2400   loss: 0.48810875\n",
      "iteration: 1429 / 2400   loss: 0.10240364\n",
      "iteration: 1430 / 2400   loss: 0.29323444\n",
      "iteration: 1431 / 2400   loss: 0.31712174\n",
      "iteration: 1432 / 2400   loss: 0.21230747\n",
      "iteration: 1433 / 2400   loss: 0.059132133\n",
      "iteration: 1434 / 2400   loss: 0.13268054\n",
      "iteration: 1435 / 2400   loss: 0.3139862\n",
      "iteration: 1436 / 2400   loss: 0.3385115\n",
      "iteration: 1437 / 2400   loss: 0.49121857\n",
      "iteration: 1438 / 2400   loss: 0.7088373\n",
      "iteration: 1439 / 2400   loss: 0.5522018\n",
      "iteration: 1440 / 2400   loss: 0.34885162\n",
      "iteration: 1441 / 2400   loss: 0.47564057\n",
      "iteration: 1442 / 2400   loss: 0.37259087\n",
      "iteration: 1443 / 2400   loss: 0.30139312\n",
      "iteration: 1444 / 2400   loss: 0.37588775\n",
      "iteration: 1445 / 2400   loss: 0.4939981\n",
      "iteration: 1446 / 2400   loss: 0.15781368\n",
      "iteration: 1447 / 2400   loss: 0.25213146\n",
      "iteration: 1448 / 2400   loss: 0.018403817\n",
      "iteration: 1449 / 2400   loss: 0.561017\n",
      "iteration: 1450 / 2400   loss: 0.54067427\n",
      "iteration: 1451 / 2400   loss: 0.11423265\n",
      "iteration: 1452 / 2400   loss: 0.12913777\n",
      "iteration: 1453 / 2400   loss: 0.24408348\n",
      "iteration: 1454 / 2400   loss: 0.21385609\n",
      "iteration: 1455 / 2400   loss: 0.22729662\n",
      "iteration: 1456 / 2400   loss: 0.30581734\n",
      "iteration: 1457 / 2400   loss: 0.29205784\n",
      "iteration: 1458 / 2400   loss: 0.091067486\n",
      "iteration: 1459 / 2400   loss: 0.10869074\n",
      "iteration: 1460 / 2400   loss: 0.52890235\n",
      "iteration: 1461 / 2400   loss: 0.12506339\n",
      "iteration: 1462 / 2400   loss: 0.15306088\n",
      "iteration: 1463 / 2400   loss: 0.19861305\n",
      "iteration: 1464 / 2400   loss: 0.5017257\n",
      "iteration: 1465 / 2400   loss: 0.16806781\n",
      "iteration: 1466 / 2400   loss: 0.14534922\n",
      "iteration: 1467 / 2400   loss: 0.27886117\n",
      "iteration: 1468 / 2400   loss: 0.31021157\n",
      "iteration: 1469 / 2400   loss: 0.45116088\n",
      "iteration: 1470 / 2400   loss: 0.8002145\n",
      "iteration: 1471 / 2400   loss: 0.29214665\n",
      "iteration: 1472 / 2400   loss: 0.10242159\n",
      "iteration: 1473 / 2400   loss: 0.45603886\n",
      "iteration: 1474 / 2400   loss: 0.33785012\n",
      "iteration: 1475 / 2400   loss: 0.32591572\n",
      "iteration: 1476 / 2400   loss: 0.4118353\n",
      "iteration: 1477 / 2400   loss: 0.39313504\n",
      "iteration: 1478 / 2400   loss: 0.42216837\n",
      "iteration: 1479 / 2400   loss: 0.16413654\n",
      "iteration: 1480 / 2400   loss: 0.26683655\n",
      "iteration: 1481 / 2400   loss: 0.13798645\n",
      "iteration: 1482 / 2400   loss: 0.8927497\n",
      "iteration: 1483 / 2400   loss: 0.55588317\n",
      "iteration: 1484 / 2400   loss: 0.3019376\n",
      "iteration: 1485 / 2400   loss: 0.28234726\n",
      "iteration: 1486 / 2400   loss: 0.15449938\n",
      "iteration: 1487 / 2400   loss: 0.40021056\n",
      "iteration: 1488 / 2400   loss: 0.18748097\n",
      "iteration: 1489 / 2400   loss: 0.29459956\n",
      "iteration: 1490 / 2400   loss: 0.3070121\n",
      "iteration: 1491 / 2400   loss: 0.52414095\n",
      "iteration: 1492 / 2400   loss: 0.42758283\n",
      "iteration: 1493 / 2400   loss: 0.2982043\n",
      "iteration: 1494 / 2400   loss: 0.41257522\n",
      "iteration: 1495 / 2400   loss: 0.24260183\n",
      "iteration: 1496 / 2400   loss: 0.14598075\n",
      "iteration: 1497 / 2400   loss: 0.31185636\n",
      "iteration: 1498 / 2400   loss: 0.44224018\n",
      "iteration: 1499 / 2400   loss: 0.49947792\n",
      "iteration: 1500 / 2400   loss: 0.5719028\n",
      "iteration: 1501 / 2400   loss: 0.113181554\n",
      "iteration: 1502 / 2400   loss: 0.32532987\n",
      "iteration: 1503 / 2400   loss: 0.31311366\n",
      "iteration: 1504 / 2400   loss: 0.3885099\n",
      "iteration: 1505 / 2400   loss: 0.26522574\n",
      "iteration: 1506 / 2400   loss: 0.17161322\n",
      "iteration: 1507 / 2400   loss: 0.17469531\n",
      "iteration: 1508 / 2400   loss: 0.2620595\n",
      "iteration: 1509 / 2400   loss: 0.6852834\n",
      "iteration: 1510 / 2400   loss: 0.2995163\n",
      "iteration: 1511 / 2400   loss: 0.5524082\n",
      "iteration: 1512 / 2400   loss: 0.12809497\n",
      "iteration: 1513 / 2400   loss: 0.41526932\n",
      "iteration: 1514 / 2400   loss: 0.24277832\n",
      "iteration: 1515 / 2400   loss: 0.309322\n",
      "iteration: 1516 / 2400   loss: 0.25219253\n",
      "iteration: 1517 / 2400   loss: 0.4377997\n",
      "iteration: 1518 / 2400   loss: 0.28243124\n",
      "iteration: 1519 / 2400   loss: 0.18421894\n",
      "iteration: 1520 / 2400   loss: 0.13697018\n",
      "iteration: 1521 / 2400   loss: 0.379541\n",
      "iteration: 1522 / 2400   loss: 0.20912533\n",
      "iteration: 1523 / 2400   loss: 0.35484332\n",
      "iteration: 1524 / 2400   loss: 0.2582724\n",
      "iteration: 1525 / 2400   loss: 0.24122992\n",
      "iteration: 1526 / 2400   loss: 0.33919528\n",
      "iteration: 1527 / 2400   loss: 0.32331046\n",
      "iteration: 1528 / 2400   loss: 0.54445314\n",
      "iteration: 1529 / 2400   loss: 0.14397557\n",
      "iteration: 1530 / 2400   loss: 0.116303556\n",
      "iteration: 1531 / 2400   loss: 0.17571741\n",
      "iteration: 1532 / 2400   loss: 0.081983045\n",
      "iteration: 1533 / 2400   loss: 0.29338098\n",
      "iteration: 1534 / 2400   loss: 0.25413325\n",
      "iteration: 1535 / 2400   loss: 0.4458192\n",
      "iteration: 1536 / 2400   loss: 0.42989102\n",
      "iteration: 1537 / 2400   loss: 0.3760054\n",
      "iteration: 1538 / 2400   loss: 0.27769214\n",
      "iteration: 1539 / 2400   loss: 0.33670563\n",
      "iteration: 1540 / 2400   loss: 0.20751306\n",
      "iteration: 1541 / 2400   loss: 0.6581242\n",
      "iteration: 1542 / 2400   loss: 0.21525759\n",
      "iteration: 1543 / 2400   loss: 0.23067343\n",
      "iteration: 1544 / 2400   loss: 0.28898352\n",
      "iteration: 1545 / 2400   loss: 0.39498994\n",
      "iteration: 1546 / 2400   loss: 0.17972504\n",
      "iteration: 1547 / 2400   loss: 0.19907714\n",
      "iteration: 1548 / 2400   loss: 0.1567052\n",
      "iteration: 1549 / 2400   loss: 0.47447327\n",
      "iteration: 1550 / 2400   loss: 0.11210461\n",
      "iteration: 1551 / 2400   loss: 0.6121776\n",
      "iteration: 1552 / 2400   loss: 0.2191042\n",
      "iteration: 1553 / 2400   loss: 0.20738529\n",
      "iteration: 1554 / 2400   loss: 0.19209532\n",
      "iteration: 1555 / 2400   loss: 0.16615307\n",
      "iteration: 1556 / 2400   loss: 0.25424048\n",
      "iteration: 1557 / 2400   loss: 0.4515178\n",
      "iteration: 1558 / 2400   loss: 0.07365851\n",
      "iteration: 1559 / 2400   loss: 0.17578931\n",
      "iteration: 1560 / 2400   loss: 0.25903308\n",
      "iteration: 1561 / 2400   loss: 0.22025333\n",
      "iteration: 1562 / 2400   loss: 0.19505775\n",
      "iteration: 1563 / 2400   loss: 0.08400198\n",
      "iteration: 1564 / 2400   loss: 0.061173953\n",
      "iteration: 1565 / 2400   loss: 0.14303157\n",
      "iteration: 1566 / 2400   loss: 0.1523846\n",
      "iteration: 1567 / 2400   loss: 0.20193748\n",
      "iteration: 1568 / 2400   loss: 0.21069759\n",
      "iteration: 1569 / 2400   loss: 0.2804941\n",
      "iteration: 1570 / 2400   loss: 0.181222\n",
      "iteration: 1571 / 2400   loss: 0.367885\n",
      "iteration: 1572 / 2400   loss: 0.42761153\n",
      "iteration: 1573 / 2400   loss: 0.40297812\n",
      "iteration: 1574 / 2400   loss: 0.37375847\n",
      "iteration: 1575 / 2400   loss: 0.37918568\n",
      "iteration: 1576 / 2400   loss: 0.5310975\n",
      "iteration: 1577 / 2400   loss: 0.5072717\n",
      "iteration: 1578 / 2400   loss: 0.36599347\n",
      "iteration: 1579 / 2400   loss: 0.30275035\n",
      "iteration: 1580 / 2400   loss: 0.57887924\n",
      "iteration: 1581 / 2400   loss: 0.20014183\n",
      "iteration: 1582 / 2400   loss: 0.41204348\n",
      "iteration: 1583 / 2400   loss: 0.091846064\n",
      "iteration: 1584 / 2400   loss: 0.29300946\n",
      "iteration: 1585 / 2400   loss: 0.35261476\n",
      "iteration: 1586 / 2400   loss: 0.43407702\n",
      "iteration: 1587 / 2400   loss: 0.34048405\n",
      "iteration: 1588 / 2400   loss: 0.4979372\n",
      "iteration: 1589 / 2400   loss: 0.35133633\n",
      "iteration: 1590 / 2400   loss: 0.26222244\n",
      "iteration: 1591 / 2400   loss: 0.2796443\n",
      "iteration: 1592 / 2400   loss: 0.45277655\n",
      "iteration: 1593 / 2400   loss: 0.5601625\n",
      "iteration: 1594 / 2400   loss: 0.30832446\n",
      "iteration: 1595 / 2400   loss: 0.30252913\n",
      "iteration: 1596 / 2400   loss: 0.37734193\n",
      "iteration: 1597 / 2400   loss: 0.2973108\n",
      "iteration: 1598 / 2400   loss: 0.27757168\n",
      "iteration: 1599 / 2400   loss: 0.25493026\n",
      "iteration: 1600 / 2400   loss: 0.38947198\n",
      "iteration: 1601 / 2400   loss: 0.44582316\n",
      "iteration: 1602 / 2400   loss: 0.10603768\n",
      "iteration: 1603 / 2400   loss: 0.2740961\n",
      "iteration: 1604 / 2400   loss: 0.31947467\n",
      "iteration: 1605 / 2400   loss: 0.15710162\n",
      "iteration: 1606 / 2400   loss: 0.2947652\n",
      "iteration: 1607 / 2400   loss: 0.5656633\n",
      "iteration: 1608 / 2400   loss: 0.097954825\n",
      "iteration: 1609 / 2400   loss: 0.21846615\n",
      "iteration: 1610 / 2400   loss: 0.16650273\n",
      "iteration: 1611 / 2400   loss: 0.2916729\n",
      "iteration: 1612 / 2400   loss: 0.26553002\n",
      "iteration: 1613 / 2400   loss: 0.23312454\n",
      "iteration: 1614 / 2400   loss: 0.089116335\n",
      "iteration: 1615 / 2400   loss: 0.15703252\n",
      "iteration: 1616 / 2400   loss: 0.9451294\n",
      "iteration: 1617 / 2400   loss: 0.08249798\n",
      "iteration: 1618 / 2400   loss: 0.32453874\n",
      "iteration: 1619 / 2400   loss: 0.4335134\n",
      "iteration: 1620 / 2400   loss: 0.20377079\n",
      "iteration: 1621 / 2400   loss: 0.09602229\n",
      "iteration: 1622 / 2400   loss: 0.5097816\n",
      "iteration: 1623 / 2400   loss: 0.1811015\n",
      "iteration: 1624 / 2400   loss: 0.11686992\n",
      "iteration: 1625 / 2400   loss: 0.07967041\n",
      "iteration: 1626 / 2400   loss: 0.79742503\n",
      "iteration: 1627 / 2400   loss: 0.8109486\n",
      "iteration: 1628 / 2400   loss: 0.11190258\n",
      "iteration: 1629 / 2400   loss: 0.740285\n",
      "iteration: 1630 / 2400   loss: 0.04632863\n",
      "iteration: 1631 / 2400   loss: 0.2594566\n",
      "iteration: 1632 / 2400   loss: 0.12285057\n",
      "iteration: 1633 / 2400   loss: 0.2085518\n",
      "iteration: 1634 / 2400   loss: 0.37337223\n",
      "iteration: 1635 / 2400   loss: 0.19727191\n",
      "iteration: 1636 / 2400   loss: 0.20427746\n",
      "iteration: 1637 / 2400   loss: 0.13060701\n",
      "iteration: 1638 / 2400   loss: 0.1455258\n",
      "iteration: 1639 / 2400   loss: 0.2527533\n",
      "iteration: 1640 / 2400   loss: 0.4031559\n",
      "iteration: 1641 / 2400   loss: 0.20919555\n",
      "iteration: 1642 / 2400   loss: 0.16024666\n",
      "iteration: 1643 / 2400   loss: 0.79994035\n",
      "iteration: 1644 / 2400   loss: 0.3355425\n",
      "iteration: 1645 / 2400   loss: 0.2926437\n",
      "iteration: 1646 / 2400   loss: 0.21426925\n",
      "iteration: 1647 / 2400   loss: 0.17428249\n",
      "iteration: 1648 / 2400   loss: 0.4577021\n",
      "iteration: 1649 / 2400   loss: 0.21871838\n",
      "iteration: 1650 / 2400   loss: 0.34970492\n",
      "iteration: 1651 / 2400   loss: 0.71555024\n",
      "iteration: 1652 / 2400   loss: 0.53670275\n",
      "iteration: 1653 / 2400   loss: 0.20307031\n",
      "iteration: 1654 / 2400   loss: 0.43986046\n",
      "iteration: 1655 / 2400   loss: 0.2536276\n",
      "iteration: 1656 / 2400   loss: 0.71586126\n",
      "iteration: 1657 / 2400   loss: 0.26307783\n",
      "iteration: 1658 / 2400   loss: 0.51367325\n",
      "iteration: 1659 / 2400   loss: 0.78406614\n",
      "iteration: 1660 / 2400   loss: 0.53622633\n",
      "iteration: 1661 / 2400   loss: 0.64939606\n",
      "iteration: 1662 / 2400   loss: 0.7126593\n",
      "iteration: 1663 / 2400   loss: 0.27687877\n",
      "iteration: 1664 / 2400   loss: 0.39376014\n",
      "iteration: 1665 / 2400   loss: 0.5587163\n",
      "iteration: 1666 / 2400   loss: 0.22514348\n",
      "iteration: 1667 / 2400   loss: 0.16895099\n",
      "iteration: 1668 / 2400   loss: 0.23858573\n",
      "iteration: 1669 / 2400   loss: 0.33324662\n",
      "iteration: 1670 / 2400   loss: 0.18556446\n",
      "iteration: 1671 / 2400   loss: 0.3316304\n",
      "iteration: 1672 / 2400   loss: 0.13330597\n",
      "iteration: 1673 / 2400   loss: 0.53285825\n",
      "iteration: 1674 / 2400   loss: 0.30502826\n",
      "iteration: 1675 / 2400   loss: 0.14094119\n",
      "iteration: 1676 / 2400   loss: 0.13810006\n",
      "iteration: 1677 / 2400   loss: 0.7451287\n",
      "iteration: 1678 / 2400   loss: 0.13461967\n",
      "iteration: 1679 / 2400   loss: 0.16575547\n",
      "iteration: 1680 / 2400   loss: 0.33308864\n",
      "iteration: 1681 / 2400   loss: 0.14658347\n",
      "iteration: 1682 / 2400   loss: 0.25623\n",
      "iteration: 1683 / 2400   loss: 0.22475414\n",
      "iteration: 1684 / 2400   loss: 0.07912084\n",
      "iteration: 1685 / 2400   loss: 0.34371793\n",
      "iteration: 1686 / 2400   loss: 0.4160704\n",
      "iteration: 1687 / 2400   loss: 0.264627\n",
      "iteration: 1688 / 2400   loss: 0.111937806\n",
      "iteration: 1689 / 2400   loss: 0.25460178\n",
      "iteration: 1690 / 2400   loss: 0.09722083\n",
      "iteration: 1691 / 2400   loss: 0.23129417\n",
      "iteration: 1692 / 2400   loss: 0.22680928\n",
      "iteration: 1693 / 2400   loss: 0.5571409\n",
      "iteration: 1694 / 2400   loss: 0.19110566\n",
      "iteration: 1695 / 2400   loss: 0.20224777\n",
      "iteration: 1696 / 2400   loss: 0.097746775\n",
      "iteration: 1697 / 2400   loss: 0.21976711\n",
      "iteration: 1698 / 2400   loss: 0.5418305\n",
      "iteration: 1699 / 2400   loss: 0.4565355\n",
      "iteration: 1700 / 2400   loss: 0.21658234\n",
      "iteration: 1701 / 2400   loss: 0.22248247\n",
      "iteration: 1702 / 2400   loss: 0.1772337\n",
      "iteration: 1703 / 2400   loss: 0.25730693\n",
      "iteration: 1704 / 2400   loss: 0.08851284\n",
      "iteration: 1705 / 2400   loss: 0.13223957\n",
      "iteration: 1706 / 2400   loss: 0.27955735\n",
      "iteration: 1707 / 2400   loss: 0.1879258\n",
      "iteration: 1708 / 2400   loss: 0.32602012\n",
      "iteration: 1709 / 2400   loss: 0.3025299\n",
      "iteration: 1710 / 2400   loss: 0.20063977\n",
      "iteration: 1711 / 2400   loss: 0.19080651\n",
      "iteration: 1712 / 2400   loss: 0.26717508\n",
      "iteration: 1713 / 2400   loss: 0.43639895\n",
      "iteration: 1714 / 2400   loss: 0.3406037\n",
      "iteration: 1715 / 2400   loss: 0.38738862\n",
      "iteration: 1716 / 2400   loss: 0.2891185\n",
      "iteration: 1717 / 2400   loss: 0.15564914\n",
      "iteration: 1718 / 2400   loss: 0.6391482\n",
      "iteration: 1719 / 2400   loss: 0.25071502\n",
      "iteration: 1720 / 2400   loss: 0.7033585\n",
      "iteration: 1721 / 2400   loss: 0.4917522\n",
      "iteration: 1722 / 2400   loss: 0.4325106\n",
      "iteration: 1723 / 2400   loss: 0.6791531\n",
      "iteration: 1724 / 2400   loss: 0.2296062\n",
      "iteration: 1725 / 2400   loss: 0.4161938\n",
      "iteration: 1726 / 2400   loss: 0.12459959\n",
      "iteration: 1727 / 2400   loss: 0.14938696\n",
      "iteration: 1728 / 2400   loss: 0.288281\n",
      "iteration: 1729 / 2400   loss: 0.39142123\n",
      "iteration: 1730 / 2400   loss: 0.7683863\n",
      "iteration: 1731 / 2400   loss: 0.10594733\n",
      "iteration: 1732 / 2400   loss: 0.2545613\n",
      "iteration: 1733 / 2400   loss: 0.07749443\n",
      "iteration: 1734 / 2400   loss: 0.45764482\n",
      "iteration: 1735 / 2400   loss: 0.13842611\n",
      "iteration: 1736 / 2400   loss: 0.19106212\n",
      "iteration: 1737 / 2400   loss: 0.2807582\n",
      "iteration: 1738 / 2400   loss: 0.30100727\n",
      "iteration: 1739 / 2400   loss: 0.4207614\n",
      "iteration: 1740 / 2400   loss: 0.32445014\n",
      "iteration: 1741 / 2400   loss: 0.21339974\n",
      "iteration: 1742 / 2400   loss: 0.6316192\n",
      "iteration: 1743 / 2400   loss: 0.4259922\n",
      "iteration: 1744 / 2400   loss: 0.26049575\n",
      "iteration: 1745 / 2400   loss: 0.091894835\n",
      "iteration: 1746 / 2400   loss: 0.070079535\n",
      "iteration: 1747 / 2400   loss: 0.40591148\n",
      "iteration: 1748 / 2400   loss: 0.16941345\n",
      "iteration: 1749 / 2400   loss: 0.20874414\n",
      "iteration: 1750 / 2400   loss: 0.22623435\n",
      "iteration: 1751 / 2400   loss: 0.19771466\n",
      "iteration: 1752 / 2400   loss: 0.6155761\n",
      "iteration: 1753 / 2400   loss: 0.29829583\n",
      "iteration: 1754 / 2400   loss: 0.13918155\n",
      "iteration: 1755 / 2400   loss: 0.54908395\n",
      "iteration: 1756 / 2400   loss: 0.062439453\n",
      "iteration: 1757 / 2400   loss: 0.2096715\n",
      "iteration: 1758 / 2400   loss: 0.25863326\n",
      "iteration: 1759 / 2400   loss: 0.67916197\n",
      "iteration: 1760 / 2400   loss: 0.42443085\n",
      "iteration: 1761 / 2400   loss: 0.546457\n",
      "iteration: 1762 / 2400   loss: 0.39260182\n",
      "iteration: 1763 / 2400   loss: 0.16429628\n",
      "iteration: 1764 / 2400   loss: 0.26832482\n",
      "iteration: 1765 / 2400   loss: 0.59522057\n",
      "iteration: 1766 / 2400   loss: 0.33729595\n",
      "iteration: 1767 / 2400   loss: 0.4798088\n",
      "iteration: 1768 / 2400   loss: 0.2234974\n",
      "iteration: 1769 / 2400   loss: 0.12525968\n",
      "iteration: 1770 / 2400   loss: 0.30796477\n",
      "iteration: 1771 / 2400   loss: 0.17975792\n",
      "iteration: 1772 / 2400   loss: 0.34575656\n",
      "iteration: 1773 / 2400   loss: 0.124978036\n",
      "iteration: 1774 / 2400   loss: 0.35454425\n",
      "iteration: 1775 / 2400   loss: 0.24520028\n",
      "iteration: 1776 / 2400   loss: 0.21774448\n",
      "iteration: 1777 / 2400   loss: 0.3996061\n",
      "iteration: 1778 / 2400   loss: 0.19917084\n",
      "iteration: 1779 / 2400   loss: 0.0949623\n",
      "iteration: 1780 / 2400   loss: 0.38709202\n",
      "iteration: 1781 / 2400   loss: 0.11885446\n",
      "iteration: 1782 / 2400   loss: 0.16252552\n",
      "iteration: 1783 / 2400   loss: 0.11965194\n",
      "iteration: 1784 / 2400   loss: 0.18081692\n",
      "iteration: 1785 / 2400   loss: 0.20579146\n",
      "iteration: 1786 / 2400   loss: 0.18635906\n",
      "iteration: 1787 / 2400   loss: 0.121831425\n",
      "iteration: 1788 / 2400   loss: 0.23371227\n",
      "iteration: 1789 / 2400   loss: 0.20493816\n",
      "iteration: 1790 / 2400   loss: 0.7657579\n",
      "iteration: 1791 / 2400   loss: 0.18606701\n",
      "iteration: 1792 / 2400   loss: 0.14156705\n",
      "iteration: 1793 / 2400   loss: 0.27029255\n",
      "iteration: 1794 / 2400   loss: 0.24913612\n",
      "iteration: 1795 / 2400   loss: 0.5171778\n",
      "iteration: 1796 / 2400   loss: 0.3684082\n",
      "iteration: 1797 / 2400   loss: 0.09030805\n",
      "iteration: 1798 / 2400   loss: 0.55069464\n",
      "iteration: 1799 / 2400   loss: 0.14843343\n",
      "iteration: 1800 / 2400   loss: 0.18740761\n",
      "iteration: 1801 / 2400   loss: 0.1320424\n",
      "iteration: 1802 / 2400   loss: 0.8402569\n",
      "iteration: 1803 / 2400   loss: 0.1452269\n",
      "iteration: 1804 / 2400   loss: 0.20134757\n",
      "iteration: 1805 / 2400   loss: 0.15683891\n",
      "iteration: 1806 / 2400   loss: 0.1153704\n",
      "iteration: 1807 / 2400   loss: 0.11402096\n",
      "iteration: 1808 / 2400   loss: 0.33005956\n",
      "iteration: 1809 / 2400   loss: 0.1787922\n",
      "iteration: 1810 / 2400   loss: 0.12916462\n",
      "iteration: 1811 / 2400   loss: 0.09889813\n",
      "iteration: 1812 / 2400   loss: 0.09962856\n",
      "iteration: 1813 / 2400   loss: 0.07175777\n",
      "iteration: 1814 / 2400   loss: 0.1735139\n",
      "iteration: 1815 / 2400   loss: 0.19776621\n",
      "iteration: 1816 / 2400   loss: 0.085091\n",
      "iteration: 1817 / 2400   loss: 0.29920667\n",
      "iteration: 1818 / 2400   loss: 0.40995744\n",
      "iteration: 1819 / 2400   loss: 0.44780877\n",
      "iteration: 1820 / 2400   loss: 0.23695107\n",
      "iteration: 1821 / 2400   loss: 0.46429378\n",
      "iteration: 1822 / 2400   loss: 0.587828\n",
      "iteration: 1823 / 2400   loss: 0.1950782\n",
      "iteration: 1824 / 2400   loss: 0.33342072\n",
      "iteration: 1825 / 2400   loss: 0.3367555\n",
      "iteration: 1826 / 2400   loss: 0.07612837\n",
      "iteration: 1827 / 2400   loss: 0.47188133\n",
      "iteration: 1828 / 2400   loss: 0.32492092\n",
      "iteration: 1829 / 2400   loss: 0.1473553\n",
      "iteration: 1830 / 2400   loss: 0.32515717\n",
      "iteration: 1831 / 2400   loss: 0.2773068\n",
      "iteration: 1832 / 2400   loss: 0.47734833\n",
      "iteration: 1833 / 2400   loss: 0.32025895\n",
      "iteration: 1834 / 2400   loss: 0.21235794\n",
      "iteration: 1835 / 2400   loss: 0.48623025\n",
      "iteration: 1836 / 2400   loss: 0.50165987\n",
      "iteration: 1837 / 2400   loss: 0.5808305\n",
      "iteration: 1838 / 2400   loss: 0.23881966\n",
      "iteration: 1839 / 2400   loss: 0.31708986\n",
      "iteration: 1840 / 2400   loss: 0.2069948\n",
      "iteration: 1841 / 2400   loss: 0.46127197\n",
      "iteration: 1842 / 2400   loss: 0.20564263\n",
      "iteration: 1843 / 2400   loss: 0.6107956\n",
      "iteration: 1844 / 2400   loss: 0.28631124\n",
      "iteration: 1845 / 2400   loss: 0.4090662\n",
      "iteration: 1846 / 2400   loss: 0.34897405\n",
      "iteration: 1847 / 2400   loss: 0.52586114\n",
      "iteration: 1848 / 2400   loss: 0.47695625\n",
      "iteration: 1849 / 2400   loss: 0.058324154\n",
      "iteration: 1850 / 2400   loss: 0.6648081\n",
      "iteration: 1851 / 2400   loss: 0.31636286\n",
      "iteration: 1852 / 2400   loss: 0.5008217\n",
      "iteration: 1853 / 2400   loss: 0.5701178\n",
      "iteration: 1854 / 2400   loss: 0.10409418\n",
      "iteration: 1855 / 2400   loss: 0.68765587\n",
      "iteration: 1856 / 2400   loss: 0.40300494\n",
      "iteration: 1857 / 2400   loss: 0.70697427\n",
      "iteration: 1858 / 2400   loss: 0.61590976\n",
      "iteration: 1859 / 2400   loss: 0.2913892\n",
      "iteration: 1860 / 2400   loss: 0.09546934\n",
      "iteration: 1861 / 2400   loss: 0.110975936\n",
      "iteration: 1862 / 2400   loss: 0.07981529\n",
      "iteration: 1863 / 2400   loss: 0.21269104\n",
      "iteration: 1864 / 2400   loss: 0.1680032\n",
      "iteration: 1865 / 2400   loss: 0.15155078\n",
      "iteration: 1866 / 2400   loss: 0.064633705\n",
      "iteration: 1867 / 2400   loss: 0.07890625\n",
      "iteration: 1868 / 2400   loss: 0.34686103\n",
      "iteration: 1869 / 2400   loss: 0.54309916\n",
      "iteration: 1870 / 2400   loss: 0.36826175\n",
      "iteration: 1871 / 2400   loss: 0.12961723\n",
      "iteration: 1872 / 2400   loss: 0.21080196\n",
      "iteration: 1873 / 2400   loss: 0.1789736\n",
      "iteration: 1874 / 2400   loss: 0.26194373\n",
      "iteration: 1875 / 2400   loss: 0.18213502\n",
      "iteration: 1876 / 2400   loss: 0.18429798\n",
      "iteration: 1877 / 2400   loss: 0.21706153\n",
      "iteration: 1878 / 2400   loss: 0.16941361\n",
      "iteration: 1879 / 2400   loss: 0.32912654\n",
      "iteration: 1880 / 2400   loss: 0.13833542\n",
      "iteration: 1881 / 2400   loss: 0.4146185\n",
      "iteration: 1882 / 2400   loss: 0.674184\n",
      "iteration: 1883 / 2400   loss: 0.09075314\n",
      "iteration: 1884 / 2400   loss: 0.18852463\n",
      "iteration: 1885 / 2400   loss: 0.11393935\n",
      "iteration: 1886 / 2400   loss: 0.14525814\n",
      "iteration: 1887 / 2400   loss: 0.2906114\n",
      "iteration: 1888 / 2400   loss: 0.15719053\n",
      "iteration: 1889 / 2400   loss: 0.24553171\n",
      "iteration: 1890 / 2400   loss: 0.6410177\n",
      "iteration: 1891 / 2400   loss: 0.39881805\n",
      "iteration: 1892 / 2400   loss: 0.42545947\n",
      "iteration: 1893 / 2400   loss: 0.2850161\n",
      "iteration: 1894 / 2400   loss: 0.19246028\n",
      "iteration: 1895 / 2400   loss: 0.2133967\n",
      "iteration: 1896 / 2400   loss: 0.3148858\n",
      "iteration: 1897 / 2400   loss: 0.5450442\n",
      "iteration: 1898 / 2400   loss: 0.49443826\n",
      "iteration: 1899 / 2400   loss: 0.27299666\n",
      "iteration: 1900 / 2400   loss: 0.41298094\n",
      "iteration: 1901 / 2400   loss: 0.40421864\n",
      "iteration: 1902 / 2400   loss: 0.14055736\n",
      "iteration: 1903 / 2400   loss: 0.39608768\n",
      "iteration: 1904 / 2400   loss: 0.61174685\n",
      "iteration: 1905 / 2400   loss: 0.40868062\n",
      "iteration: 1906 / 2400   loss: 0.32150078\n",
      "iteration: 1907 / 2400   loss: 0.29022995\n",
      "iteration: 1908 / 2400   loss: 0.5793519\n",
      "iteration: 1909 / 2400   loss: 0.39140466\n",
      "iteration: 1910 / 2400   loss: 0.27423903\n",
      "iteration: 1911 / 2400   loss: 0.37075156\n",
      "iteration: 1912 / 2400   loss: 0.08051695\n",
      "iteration: 1913 / 2400   loss: 0.0867282\n",
      "iteration: 1914 / 2400   loss: 0.2103995\n",
      "iteration: 1915 / 2400   loss: 0.23199844\n",
      "iteration: 1916 / 2400   loss: 0.12210341\n",
      "iteration: 1917 / 2400   loss: 0.30623722\n",
      "iteration: 1918 / 2400   loss: 0.7245445\n",
      "iteration: 1919 / 2400   loss: 0.15567368\n",
      "iteration: 1920 / 2400   loss: 0.09929657\n",
      "iteration: 1921 / 2400   loss: 0.40273452\n",
      "iteration: 1922 / 2400   loss: 0.15720539\n",
      "iteration: 1923 / 2400   loss: 0.2087448\n",
      "iteration: 1924 / 2400   loss: 0.30980435\n",
      "iteration: 1925 / 2400   loss: 0.18681106\n",
      "iteration: 1926 / 2400   loss: 0.06723398\n",
      "iteration: 1927 / 2400   loss: 0.4690391\n",
      "iteration: 1928 / 2400   loss: 0.105993785\n",
      "iteration: 1929 / 2400   loss: 0.19490524\n",
      "iteration: 1930 / 2400   loss: 0.118101485\n",
      "iteration: 1931 / 2400   loss: 0.6049672\n",
      "iteration: 1932 / 2400   loss: 0.31754524\n",
      "iteration: 1933 / 2400   loss: 0.32950976\n",
      "iteration: 1934 / 2400   loss: 0.2845712\n",
      "iteration: 1935 / 2400   loss: 0.36245835\n",
      "iteration: 1936 / 2400   loss: 0.507848\n",
      "iteration: 1937 / 2400   loss: 0.13063893\n",
      "iteration: 1938 / 2400   loss: 0.07727644\n",
      "iteration: 1939 / 2400   loss: 0.07497664\n",
      "iteration: 1940 / 2400   loss: 0.19505623\n",
      "iteration: 1941 / 2400   loss: 0.22526367\n",
      "iteration: 1942 / 2400   loss: 0.45135298\n",
      "iteration: 1943 / 2400   loss: 0.18375774\n",
      "iteration: 1944 / 2400   loss: 0.39478278\n",
      "iteration: 1945 / 2400   loss: 0.12220995\n",
      "iteration: 1946 / 2400   loss: 0.1272196\n",
      "iteration: 1947 / 2400   loss: 0.20634338\n",
      "iteration: 1948 / 2400   loss: 0.5241264\n",
      "iteration: 1949 / 2400   loss: 0.31664905\n",
      "iteration: 1950 / 2400   loss: 0.0655509\n",
      "iteration: 1951 / 2400   loss: 0.27331656\n",
      "iteration: 1952 / 2400   loss: 0.1082485\n",
      "iteration: 1953 / 2400   loss: 0.45505914\n",
      "iteration: 1954 / 2400   loss: 0.08332684\n",
      "iteration: 1955 / 2400   loss: 0.24624702\n",
      "iteration: 1956 / 2400   loss: 0.23979434\n",
      "iteration: 1957 / 2400   loss: 0.20545894\n",
      "iteration: 1958 / 2400   loss: 0.32782286\n",
      "iteration: 1959 / 2400   loss: 0.56489784\n",
      "iteration: 1960 / 2400   loss: 0.48175216\n",
      "iteration: 1961 / 2400   loss: 0.39776343\n",
      "iteration: 1962 / 2400   loss: 0.38323134\n",
      "iteration: 1963 / 2400   loss: 0.65891296\n",
      "iteration: 1964 / 2400   loss: 0.29283693\n",
      "iteration: 1965 / 2400   loss: 0.23719646\n",
      "iteration: 1966 / 2400   loss: 0.6388736\n",
      "iteration: 1967 / 2400   loss: 0.48892337\n",
      "iteration: 1968 / 2400   loss: 0.34212685\n",
      "iteration: 1969 / 2400   loss: 0.46817973\n",
      "iteration: 1970 / 2400   loss: 0.4365879\n",
      "iteration: 1971 / 2400   loss: 0.14531706\n",
      "iteration: 1972 / 2400   loss: 0.40484208\n",
      "iteration: 1973 / 2400   loss: 0.116731234\n",
      "iteration: 1974 / 2400   loss: 0.055389356\n",
      "iteration: 1975 / 2400   loss: 0.21626832\n",
      "iteration: 1976 / 2400   loss: 0.0915829\n",
      "iteration: 1977 / 2400   loss: 0.26458028\n",
      "iteration: 1978 / 2400   loss: 0.2501806\n",
      "iteration: 1979 / 2400   loss: 0.49767524\n",
      "iteration: 1980 / 2400   loss: 0.86045486\n",
      "iteration: 1981 / 2400   loss: 0.6135834\n",
      "iteration: 1982 / 2400   loss: 0.6977016\n",
      "iteration: 1983 / 2400   loss: 0.49092767\n",
      "iteration: 1984 / 2400   loss: 0.43452477\n",
      "iteration: 1985 / 2400   loss: 0.1273707\n",
      "iteration: 1986 / 2400   loss: 0.2983075\n",
      "iteration: 1987 / 2400   loss: 0.5249895\n",
      "iteration: 1988 / 2400   loss: 0.24472004\n",
      "iteration: 1989 / 2400   loss: 0.18979752\n",
      "iteration: 1990 / 2400   loss: 0.21608262\n",
      "iteration: 1991 / 2400   loss: 0.13212782\n",
      "iteration: 1992 / 2400   loss: 0.43414783\n",
      "iteration: 1993 / 2400   loss: 0.16239113\n",
      "iteration: 1994 / 2400   loss: 0.27403858\n",
      "iteration: 1995 / 2400   loss: 0.17008828\n",
      "iteration: 1996 / 2400   loss: 0.42674953\n",
      "iteration: 1997 / 2400   loss: 0.71374786\n",
      "iteration: 1998 / 2400   loss: 0.23962232\n",
      "iteration: 1999 / 2400   loss: 0.18817736\n",
      "iteration: 2000 / 2400   loss: 0.24915744\n",
      "iteration: 2001 / 2400   loss: 0.2001167\n",
      "iteration: 2002 / 2400   loss: 0.23965997\n",
      "iteration: 2003 / 2400   loss: 0.14992274\n",
      "iteration: 2004 / 2400   loss: 0.8154541\n",
      "iteration: 2005 / 2400   loss: 0.36978194\n",
      "iteration: 2006 / 2400   loss: 0.25308993\n",
      "iteration: 2007 / 2400   loss: 0.11222822\n",
      "iteration: 2008 / 2400   loss: 0.117439955\n",
      "iteration: 2009 / 2400   loss: 0.55405664\n",
      "iteration: 2010 / 2400   loss: 0.36143768\n",
      "iteration: 2011 / 2400   loss: 0.09080677\n",
      "iteration: 2012 / 2400   loss: 0.074116684\n",
      "iteration: 2013 / 2400   loss: 0.2869476\n",
      "iteration: 2014 / 2400   loss: 0.14839435\n",
      "iteration: 2015 / 2400   loss: 0.7780706\n",
      "iteration: 2016 / 2400   loss: 0.9331453\n",
      "iteration: 2017 / 2400   loss: 0.34838018\n",
      "iteration: 2018 / 2400   loss: 0.37809318\n",
      "iteration: 2019 / 2400   loss: 0.44782627\n",
      "iteration: 2020 / 2400   loss: 0.10378368\n",
      "iteration: 2021 / 2400   loss: 0.35689682\n",
      "iteration: 2022 / 2400   loss: 0.085319564\n",
      "iteration: 2023 / 2400   loss: 0.7744637\n",
      "iteration: 2024 / 2400   loss: 0.07692041\n",
      "iteration: 2025 / 2400   loss: 0.14436723\n",
      "iteration: 2026 / 2400   loss: 0.9422869\n",
      "iteration: 2027 / 2400   loss: 0.119942166\n",
      "iteration: 2028 / 2400   loss: 0.2798707\n",
      "iteration: 2029 / 2400   loss: 0.37775186\n",
      "iteration: 2030 / 2400   loss: 0.21505135\n",
      "iteration: 2031 / 2400   loss: 0.15645587\n",
      "iteration: 2032 / 2400   loss: 0.49198672\n",
      "iteration: 2033 / 2400   loss: 0.42667848\n",
      "iteration: 2034 / 2400   loss: 0.4583174\n",
      "iteration: 2035 / 2400   loss: 0.022687225\n",
      "iteration: 2036 / 2400   loss: 0.51550955\n",
      "iteration: 2037 / 2400   loss: 0.35772896\n",
      "iteration: 2038 / 2400   loss: 0.24840136\n",
      "iteration: 2039 / 2400   loss: 0.15642777\n",
      "iteration: 2040 / 2400   loss: 0.16794682\n",
      "iteration: 2041 / 2400   loss: 0.21246181\n",
      "iteration: 2042 / 2400   loss: 0.0717109\n",
      "iteration: 2043 / 2400   loss: 0.33038464\n",
      "iteration: 2044 / 2400   loss: 0.14145117\n",
      "iteration: 2045 / 2400   loss: 0.32575297\n",
      "iteration: 2046 / 2400   loss: 0.11393471\n",
      "iteration: 2047 / 2400   loss: 0.5507447\n",
      "iteration: 2048 / 2400   loss: 0.13619156\n",
      "iteration: 2049 / 2400   loss: 0.25221118\n",
      "iteration: 2050 / 2400   loss: 0.9015107\n",
      "iteration: 2051 / 2400   loss: 0.24502802\n",
      "iteration: 2052 / 2400   loss: 0.25668094\n",
      "iteration: 2053 / 2400   loss: 0.25924915\n",
      "iteration: 2054 / 2400   loss: 0.23994583\n",
      "iteration: 2055 / 2400   loss: 0.08164114\n",
      "iteration: 2056 / 2400   loss: 0.07346735\n",
      "iteration: 2057 / 2400   loss: 0.25637758\n",
      "iteration: 2058 / 2400   loss: 0.38994518\n",
      "iteration: 2059 / 2400   loss: 0.31995776\n",
      "iteration: 2060 / 2400   loss: 0.1525635\n",
      "iteration: 2061 / 2400   loss: 0.122004956\n",
      "iteration: 2062 / 2400   loss: 0.90412885\n",
      "iteration: 2063 / 2400   loss: 0.076772116\n",
      "iteration: 2064 / 2400   loss: 0.12549923\n",
      "iteration: 2065 / 2400   loss: 0.2894959\n",
      "iteration: 2066 / 2400   loss: 0.06833332\n",
      "iteration: 2067 / 2400   loss: 0.25631398\n",
      "iteration: 2068 / 2400   loss: 0.23273899\n",
      "iteration: 2069 / 2400   loss: 0.08146647\n",
      "iteration: 2070 / 2400   loss: 0.16589983\n",
      "iteration: 2071 / 2400   loss: 0.17901368\n",
      "iteration: 2072 / 2400   loss: 0.14683115\n",
      "iteration: 2073 / 2400   loss: 0.14035906\n",
      "iteration: 2074 / 2400   loss: 0.13580678\n",
      "iteration: 2075 / 2400   loss: 0.1943991\n",
      "iteration: 2076 / 2400   loss: 0.068251744\n",
      "iteration: 2077 / 2400   loss: 0.146839\n",
      "iteration: 2078 / 2400   loss: 0.21181694\n",
      "iteration: 2079 / 2400   loss: 0.26783803\n",
      "iteration: 2080 / 2400   loss: 0.26086053\n",
      "iteration: 2081 / 2400   loss: 0.1952744\n",
      "iteration: 2082 / 2400   loss: 0.20875917\n",
      "iteration: 2083 / 2400   loss: 0.19663396\n",
      "iteration: 2084 / 2400   loss: 0.7211431\n",
      "iteration: 2085 / 2400   loss: 0.5326916\n",
      "iteration: 2086 / 2400   loss: 0.7160126\n",
      "iteration: 2087 / 2400   loss: 0.6221694\n",
      "iteration: 2088 / 2400   loss: 0.15503569\n",
      "iteration: 2089 / 2400   loss: 0.647643\n",
      "iteration: 2090 / 2400   loss: 0.4348262\n",
      "iteration: 2091 / 2400   loss: 0.15291484\n",
      "iteration: 2092 / 2400   loss: 0.26674312\n",
      "iteration: 2093 / 2400   loss: 0.54238844\n",
      "iteration: 2094 / 2400   loss: 0.10103928\n",
      "iteration: 2095 / 2400   loss: 0.13076518\n",
      "iteration: 2096 / 2400   loss: 0.24216518\n",
      "iteration: 2097 / 2400   loss: 0.095774986\n",
      "iteration: 2098 / 2400   loss: 0.19326699\n",
      "iteration: 2099 / 2400   loss: 0.35707933\n",
      "iteration: 2100 / 2400   loss: 0.07973689\n",
      "iteration: 2101 / 2400   loss: 0.18456398\n",
      "iteration: 2102 / 2400   loss: 0.14583276\n",
      "iteration: 2103 / 2400   loss: 0.14708762\n",
      "iteration: 2104 / 2400   loss: 0.0789468\n",
      "iteration: 2105 / 2400   loss: 0.38280678\n",
      "iteration: 2106 / 2400   loss: 0.21730357\n",
      "iteration: 2107 / 2400   loss: 0.26010144\n",
      "iteration: 2108 / 2400   loss: 0.39921165\n",
      "iteration: 2109 / 2400   loss: 0.38081577\n",
      "iteration: 2110 / 2400   loss: 0.5529612\n",
      "iteration: 2111 / 2400   loss: 0.20192955\n",
      "iteration: 2112 / 2400   loss: 0.062387668\n",
      "iteration: 2113 / 2400   loss: 0.5041453\n",
      "iteration: 2114 / 2400   loss: 0.683864\n",
      "iteration: 2115 / 2400   loss: 0.31590444\n",
      "iteration: 2116 / 2400   loss: 0.296265\n",
      "iteration: 2117 / 2400   loss: 0.2628648\n",
      "iteration: 2118 / 2400   loss: 0.48487258\n",
      "iteration: 2119 / 2400   loss: 0.38854071\n",
      "iteration: 2120 / 2400   loss: 0.25279748\n",
      "iteration: 2121 / 2400   loss: 0.23448768\n",
      "iteration: 2122 / 2400   loss: 0.26106322\n",
      "iteration: 2123 / 2400   loss: 0.16015136\n",
      "iteration: 2124 / 2400   loss: 0.14698759\n",
      "iteration: 2125 / 2400   loss: 0.31756207\n",
      "iteration: 2126 / 2400   loss: 0.24730697\n",
      "iteration: 2127 / 2400   loss: 0.46801665\n",
      "iteration: 2128 / 2400   loss: 0.286643\n",
      "iteration: 2129 / 2400   loss: 0.41659477\n",
      "iteration: 2130 / 2400   loss: 0.23494941\n",
      "iteration: 2131 / 2400   loss: 0.09697319\n",
      "iteration: 2132 / 2400   loss: 0.23323712\n",
      "iteration: 2133 / 2400   loss: 0.18884788\n",
      "iteration: 2134 / 2400   loss: 0.06753224\n",
      "iteration: 2135 / 2400   loss: 0.2449918\n",
      "iteration: 2136 / 2400   loss: 0.3739852\n",
      "iteration: 2137 / 2400   loss: 0.20234427\n",
      "iteration: 2138 / 2400   loss: 0.14931683\n",
      "iteration: 2139 / 2400   loss: 0.21178441\n",
      "iteration: 2140 / 2400   loss: 0.30329406\n",
      "iteration: 2141 / 2400   loss: 0.122158594\n",
      "iteration: 2142 / 2400   loss: 0.14655684\n",
      "iteration: 2143 / 2400   loss: 0.36657533\n",
      "iteration: 2144 / 2400   loss: 0.32633698\n",
      "iteration: 2145 / 2400   loss: 0.12796056\n",
      "iteration: 2146 / 2400   loss: 0.60051346\n",
      "iteration: 2147 / 2400   loss: 0.16283637\n",
      "iteration: 2148 / 2400   loss: 0.37561733\n",
      "iteration: 2149 / 2400   loss: 0.20252451\n",
      "iteration: 2150 / 2400   loss: 0.43293953\n",
      "iteration: 2151 / 2400   loss: 0.32160217\n",
      "iteration: 2152 / 2400   loss: 0.114632055\n",
      "iteration: 2153 / 2400   loss: 0.21249947\n",
      "iteration: 2154 / 2400   loss: 0.080244996\n",
      "iteration: 2155 / 2400   loss: 0.52353454\n",
      "iteration: 2156 / 2400   loss: 0.16185193\n",
      "iteration: 2157 / 2400   loss: 0.5157415\n",
      "iteration: 2158 / 2400   loss: 0.19058022\n",
      "iteration: 2159 / 2400   loss: 0.14902551\n",
      "iteration: 2160 / 2400   loss: 0.5373145\n",
      "iteration: 2161 / 2400   loss: 0.101591274\n",
      "iteration: 2162 / 2400   loss: 0.38355544\n",
      "iteration: 2163 / 2400   loss: 0.30227166\n",
      "iteration: 2164 / 2400   loss: 0.25253734\n",
      "iteration: 2165 / 2400   loss: 0.13686182\n",
      "iteration: 2166 / 2400   loss: 0.05875635\n",
      "iteration: 2167 / 2400   loss: 0.18567425\n",
      "iteration: 2168 / 2400   loss: 0.08184987\n",
      "iteration: 2169 / 2400   loss: 0.030129299\n",
      "iteration: 2170 / 2400   loss: 0.23462003\n",
      "iteration: 2171 / 2400   loss: 0.6030849\n",
      "iteration: 2172 / 2400   loss: 0.26281807\n",
      "iteration: 2173 / 2400   loss: 0.15126173\n",
      "iteration: 2174 / 2400   loss: 0.04010849\n",
      "iteration: 2175 / 2400   loss: 0.20048966\n",
      "iteration: 2176 / 2400   loss: 0.15709552\n",
      "iteration: 2177 / 2400   loss: 0.08867777\n",
      "iteration: 2178 / 2400   loss: 0.20398346\n",
      "iteration: 2179 / 2400   loss: 0.14144124\n",
      "iteration: 2180 / 2400   loss: 0.11337072\n",
      "iteration: 2181 / 2400   loss: 1.0894511\n",
      "iteration: 2182 / 2400   loss: 0.28972062\n",
      "iteration: 2183 / 2400   loss: 0.19663963\n",
      "iteration: 2184 / 2400   loss: 0.24587277\n",
      "iteration: 2185 / 2400   loss: 0.272668\n",
      "iteration: 2186 / 2400   loss: 0.047006533\n",
      "iteration: 2187 / 2400   loss: 0.12856863\n",
      "iteration: 2188 / 2400   loss: 0.09522861\n",
      "iteration: 2189 / 2400   loss: 0.15642829\n",
      "iteration: 2190 / 2400   loss: 0.14116901\n",
      "iteration: 2191 / 2400   loss: 0.13979004\n",
      "iteration: 2192 / 2400   loss: 0.4325021\n",
      "iteration: 2193 / 2400   loss: 0.03452257\n",
      "iteration: 2194 / 2400   loss: 0.09978404\n",
      "iteration: 2195 / 2400   loss: 0.33495098\n",
      "iteration: 2196 / 2400   loss: 0.24617882\n",
      "iteration: 2197 / 2400   loss: 0.2541661\n",
      "iteration: 2198 / 2400   loss: 0.5430905\n",
      "iteration: 2199 / 2400   loss: 0.5732994\n",
      "iteration: 2200 / 2400   loss: 0.12639152\n",
      "iteration: 2201 / 2400   loss: 0.13358155\n",
      "iteration: 2202 / 2400   loss: 0.2382946\n",
      "iteration: 2203 / 2400   loss: 0.1634029\n",
      "iteration: 2204 / 2400   loss: 0.10579759\n",
      "iteration: 2205 / 2400   loss: 0.1235202\n",
      "iteration: 2206 / 2400   loss: 0.04701397\n",
      "iteration: 2207 / 2400   loss: 0.3664966\n",
      "iteration: 2208 / 2400   loss: 0.15414482\n",
      "iteration: 2209 / 2400   loss: 0.075481966\n",
      "iteration: 2210 / 2400   loss: 0.13360894\n",
      "iteration: 2211 / 2400   loss: 0.06650139\n",
      "iteration: 2212 / 2400   loss: 0.41044545\n",
      "iteration: 2213 / 2400   loss: 0.13703829\n",
      "iteration: 2214 / 2400   loss: 0.47580805\n",
      "iteration: 2215 / 2400   loss: 0.3273982\n",
      "iteration: 2216 / 2400   loss: 0.098029345\n",
      "iteration: 2217 / 2400   loss: 0.14767899\n",
      "iteration: 2218 / 2400   loss: 0.3226719\n",
      "iteration: 2219 / 2400   loss: 0.113143995\n",
      "iteration: 2220 / 2400   loss: 0.32272065\n",
      "iteration: 2221 / 2400   loss: 0.4013929\n",
      "iteration: 2222 / 2400   loss: 0.22479048\n",
      "iteration: 2223 / 2400   loss: 0.04361002\n",
      "iteration: 2224 / 2400   loss: 0.08425625\n",
      "iteration: 2225 / 2400   loss: 0.40849158\n",
      "iteration: 2226 / 2400   loss: 0.14475787\n",
      "iteration: 2227 / 2400   loss: 0.06245145\n",
      "iteration: 2228 / 2400   loss: 0.14815423\n",
      "iteration: 2229 / 2400   loss: 0.09099436\n",
      "iteration: 2230 / 2400   loss: 0.3462983\n",
      "iteration: 2231 / 2400   loss: 0.029712133\n",
      "iteration: 2232 / 2400   loss: 0.18867289\n",
      "iteration: 2233 / 2400   loss: 0.18639177\n",
      "iteration: 2234 / 2400   loss: 0.20792146\n",
      "iteration: 2235 / 2400   loss: 0.3995394\n",
      "iteration: 2236 / 2400   loss: 0.220369\n",
      "iteration: 2237 / 2400   loss: 0.34618455\n",
      "iteration: 2238 / 2400   loss: 0.051716976\n",
      "iteration: 2239 / 2400   loss: 0.13233575\n",
      "iteration: 2240 / 2400   loss: 0.09683852\n",
      "iteration: 2241 / 2400   loss: 0.17027149\n",
      "iteration: 2242 / 2400   loss: 0.053382225\n",
      "iteration: 2243 / 2400   loss: 0.1474388\n",
      "iteration: 2244 / 2400   loss: 0.23410143\n",
      "iteration: 2245 / 2400   loss: 0.08286576\n",
      "iteration: 2246 / 2400   loss: 0.13804336\n",
      "iteration: 2247 / 2400   loss: 0.3538298\n",
      "iteration: 2248 / 2400   loss: 0.22500366\n",
      "iteration: 2249 / 2400   loss: 0.5455118\n",
      "iteration: 2250 / 2400   loss: 0.08931062\n",
      "iteration: 2251 / 2400   loss: 0.16122994\n",
      "iteration: 2252 / 2400   loss: 0.102436535\n",
      "iteration: 2253 / 2400   loss: 0.2541062\n",
      "iteration: 2254 / 2400   loss: 0.17255905\n",
      "iteration: 2255 / 2400   loss: 0.35055184\n",
      "iteration: 2256 / 2400   loss: 0.30860367\n",
      "iteration: 2257 / 2400   loss: 0.101643346\n",
      "iteration: 2258 / 2400   loss: 0.3302893\n",
      "iteration: 2259 / 2400   loss: 0.47854072\n",
      "iteration: 2260 / 2400   loss: 0.42421305\n",
      "iteration: 2261 / 2400   loss: 0.107022464\n",
      "iteration: 2262 / 2400   loss: 0.047670726\n",
      "iteration: 2263 / 2400   loss: 0.09208561\n",
      "iteration: 2264 / 2400   loss: 0.34634018\n",
      "iteration: 2265 / 2400   loss: 0.16328572\n",
      "iteration: 2266 / 2400   loss: 0.17379436\n",
      "iteration: 2267 / 2400   loss: 0.12904346\n",
      "iteration: 2268 / 2400   loss: 0.03794592\n",
      "iteration: 2269 / 2400   loss: 0.3082444\n",
      "iteration: 2270 / 2400   loss: 0.1804039\n",
      "iteration: 2271 / 2400   loss: 0.13585576\n",
      "iteration: 2272 / 2400   loss: 0.067980185\n",
      "iteration: 2273 / 2400   loss: 0.13292713\n",
      "iteration: 2274 / 2400   loss: 0.15393576\n",
      "iteration: 2275 / 2400   loss: 0.17299266\n",
      "iteration: 2276 / 2400   loss: 0.13405137\n",
      "iteration: 2277 / 2400   loss: 0.19836901\n",
      "iteration: 2278 / 2400   loss: 0.07709453\n",
      "iteration: 2279 / 2400   loss: 0.04999071\n",
      "iteration: 2280 / 2400   loss: 0.2292945\n",
      "iteration: 2281 / 2400   loss: 0.33018017\n",
      "iteration: 2282 / 2400   loss: 0.18674576\n",
      "iteration: 2283 / 2400   loss: 0.1632537\n",
      "iteration: 2284 / 2400   loss: 0.36359993\n",
      "iteration: 2285 / 2400   loss: 0.25062278\n",
      "iteration: 2286 / 2400   loss: 0.10285319\n",
      "iteration: 2287 / 2400   loss: 0.3218433\n",
      "iteration: 2288 / 2400   loss: 0.20272619\n",
      "iteration: 2289 / 2400   loss: 0.2899813\n",
      "iteration: 2290 / 2400   loss: 0.44202864\n",
      "iteration: 2291 / 2400   loss: 0.38323852\n",
      "iteration: 2292 / 2400   loss: 0.10325877\n",
      "iteration: 2293 / 2400   loss: 0.31915885\n",
      "iteration: 2294 / 2400   loss: 0.23949543\n",
      "iteration: 2295 / 2400   loss: 0.10403207\n",
      "iteration: 2296 / 2400   loss: 0.41593385\n",
      "iteration: 2297 / 2400   loss: 0.22239766\n",
      "iteration: 2298 / 2400   loss: 0.2380743\n",
      "iteration: 2299 / 2400   loss: 0.36419258\n",
      "iteration: 2300 / 2400   loss: 0.41206893\n",
      "iteration: 2301 / 2400   loss: 0.12911138\n",
      "iteration: 2302 / 2400   loss: 0.2579056\n",
      "iteration: 2303 / 2400   loss: 0.11121887\n",
      "iteration: 2304 / 2400   loss: 0.09976894\n",
      "iteration: 2305 / 2400   loss: 0.30960932\n",
      "iteration: 2306 / 2400   loss: 0.15137796\n",
      "iteration: 2307 / 2400   loss: 0.7367424\n",
      "iteration: 2308 / 2400   loss: 0.17741817\n",
      "iteration: 2309 / 2400   loss: 0.3774447\n",
      "iteration: 2310 / 2400   loss: 0.2955196\n",
      "iteration: 2311 / 2400   loss: 0.5616115\n",
      "iteration: 2312 / 2400   loss: 0.15688004\n",
      "iteration: 2313 / 2400   loss: 0.3844545\n",
      "iteration: 2314 / 2400   loss: 0.09300911\n",
      "iteration: 2315 / 2400   loss: 0.18244904\n",
      "iteration: 2316 / 2400   loss: 0.21661164\n",
      "iteration: 2317 / 2400   loss: 0.053701278\n",
      "iteration: 2318 / 2400   loss: 0.17168224\n",
      "iteration: 2319 / 2400   loss: 0.356429\n",
      "iteration: 2320 / 2400   loss: 0.18329243\n",
      "iteration: 2321 / 2400   loss: 0.2918929\n",
      "iteration: 2322 / 2400   loss: 0.08485315\n",
      "iteration: 2323 / 2400   loss: 0.6116262\n",
      "iteration: 2324 / 2400   loss: 0.12674001\n",
      "iteration: 2325 / 2400   loss: 0.23123911\n",
      "iteration: 2326 / 2400   loss: 0.17812404\n",
      "iteration: 2327 / 2400   loss: 0.08709217\n",
      "iteration: 2328 / 2400   loss: 0.1882325\n",
      "iteration: 2329 / 2400   loss: 0.011847916\n",
      "iteration: 2330 / 2400   loss: 0.21953823\n",
      "iteration: 2331 / 2400   loss: 0.13910267\n",
      "iteration: 2332 / 2400   loss: 0.040307894\n",
      "iteration: 2333 / 2400   loss: 0.33042923\n",
      "iteration: 2334 / 2400   loss: 0.21223922\n",
      "iteration: 2335 / 2400   loss: 0.03388422\n",
      "iteration: 2336 / 2400   loss: 0.13015376\n",
      "iteration: 2337 / 2400   loss: 0.36448905\n",
      "iteration: 2338 / 2400   loss: 0.30160424\n",
      "iteration: 2339 / 2400   loss: 0.25023857\n",
      "iteration: 2340 / 2400   loss: 0.102206685\n",
      "iteration: 2341 / 2400   loss: 0.059869595\n",
      "iteration: 2342 / 2400   loss: 0.21683209\n",
      "iteration: 2343 / 2400   loss: 0.28343353\n",
      "iteration: 2344 / 2400   loss: 0.08915939\n",
      "iteration: 2345 / 2400   loss: 0.06654184\n",
      "iteration: 2346 / 2400   loss: 0.13883217\n",
      "iteration: 2347 / 2400   loss: 0.2399146\n",
      "iteration: 2348 / 2400   loss: 0.061559714\n",
      "iteration: 2349 / 2400   loss: 0.049088985\n",
      "iteration: 2350 / 2400   loss: 0.10903489\n",
      "iteration: 2351 / 2400   loss: 0.061293248\n",
      "iteration: 2352 / 2400   loss: 0.23035912\n",
      "iteration: 2353 / 2400   loss: 0.68988776\n",
      "iteration: 2354 / 2400   loss: 0.29264614\n",
      "iteration: 2355 / 2400   loss: 0.08630272\n",
      "iteration: 2356 / 2400   loss: 0.034551803\n",
      "iteration: 2357 / 2400   loss: 0.033561755\n",
      "iteration: 2358 / 2400   loss: 0.0136038875\n",
      "iteration: 2359 / 2400   loss: 0.06366015\n",
      "iteration: 2360 / 2400   loss: 0.08014539\n",
      "iteration: 2361 / 2400   loss: 0.045753393\n",
      "iteration: 2362 / 2400   loss: 0.1004859\n",
      "iteration: 2363 / 2400   loss: 0.29253894\n",
      "iteration: 2364 / 2400   loss: 0.184453\n",
      "iteration: 2365 / 2400   loss: 0.053114556\n",
      "iteration: 2366 / 2400   loss: 0.12609011\n",
      "iteration: 2367 / 2400   loss: 0.12920812\n",
      "iteration: 2368 / 2400   loss: 0.059419\n",
      "iteration: 2369 / 2400   loss: 0.10508623\n",
      "iteration: 2370 / 2400   loss: 0.061211366\n",
      "iteration: 2371 / 2400   loss: 0.08021592\n",
      "iteration: 2372 / 2400   loss: 0.5375454\n",
      "iteration: 2373 / 2400   loss: 0.22372437\n",
      "iteration: 2374 / 2400   loss: 0.5129149\n",
      "iteration: 2375 / 2400   loss: 0.8662497\n",
      "iteration: 2376 / 2400   loss: 0.544052\n",
      "iteration: 2377 / 2400   loss: 0.47580206\n",
      "iteration: 2378 / 2400   loss: 0.45253605\n",
      "iteration: 2379 / 2400   loss: 0.108340696\n",
      "iteration: 2380 / 2400   loss: 0.3091306\n",
      "iteration: 2381 / 2400   loss: 0.31914115\n",
      "iteration: 2382 / 2400   loss: 0.35652587\n",
      "iteration: 2383 / 2400   loss: 0.39763844\n",
      "iteration: 2384 / 2400   loss: 0.32058337\n",
      "iteration: 2385 / 2400   loss: 0.12427418\n",
      "iteration: 2386 / 2400   loss: 0.1717243\n",
      "iteration: 2387 / 2400   loss: 0.43761638\n",
      "iteration: 2388 / 2400   loss: 0.139035\n",
      "iteration: 2389 / 2400   loss: 1.5453726\n",
      "iteration: 2390 / 2400   loss: 1.6806068\n",
      "iteration: 2391 / 2400   loss: 0.2981105\n",
      "iteration: 2392 / 2400   loss: 0.5192166\n",
      "iteration: 2393 / 2400   loss: 0.41398773\n",
      "iteration: 2394 / 2400   loss: 0.25780243\n",
      "iteration: 2395 / 2400   loss: 0.09283141\n",
      "iteration: 2396 / 2400   loss: 0.036090717\n",
      "iteration: 2397 / 2400   loss: 0.88684076\n",
      "iteration: 2398 / 2400   loss: 0.61979175\n",
      "iteration: 2399 / 2400   loss: 0.16882296\n",
      "iteration: 2400 / 2400   loss: 0.20449463\n",
      "epoch: 1    test_acc: 0.9244\n",
      "iteration: 1 / 2400   loss: 0.4167795\n",
      "iteration: 2 / 2400   loss: 0.3628818\n",
      "iteration: 3 / 2400   loss: 0.24318348\n",
      "iteration: 4 / 2400   loss: 0.23147738\n",
      "iteration: 5 / 2400   loss: 0.06642369\n",
      "iteration: 6 / 2400   loss: 0.7938783\n",
      "iteration: 7 / 2400   loss: 0.32053527\n",
      "iteration: 8 / 2400   loss: 0.4057482\n",
      "iteration: 9 / 2400   loss: 0.3388871\n",
      "iteration: 10 / 2400   loss: 0.20867094\n",
      "iteration: 11 / 2400   loss: 0.43336365\n",
      "iteration: 12 / 2400   loss: 0.21243332\n",
      "iteration: 13 / 2400   loss: 0.10040308\n",
      "iteration: 14 / 2400   loss: 0.16124843\n",
      "iteration: 15 / 2400   loss: 0.045450192\n",
      "iteration: 16 / 2400   loss: 0.48906794\n",
      "iteration: 17 / 2400   loss: 0.20534047\n",
      "iteration: 18 / 2400   loss: 0.12250967\n",
      "iteration: 19 / 2400   loss: 0.16188286\n",
      "iteration: 20 / 2400   loss: 0.17080489\n",
      "iteration: 21 / 2400   loss: 0.2647149\n",
      "iteration: 22 / 2400   loss: 0.4066325\n",
      "iteration: 23 / 2400   loss: 0.13671216\n",
      "iteration: 24 / 2400   loss: 0.24902311\n",
      "iteration: 25 / 2400   loss: 0.13825609\n",
      "iteration: 26 / 2400   loss: 0.32518363\n",
      "iteration: 27 / 2400   loss: 0.26379886\n",
      "iteration: 28 / 2400   loss: 0.122795925\n",
      "iteration: 29 / 2400   loss: 0.15934052\n",
      "iteration: 30 / 2400   loss: 0.08545475\n",
      "iteration: 31 / 2400   loss: 0.0683821\n",
      "iteration: 32 / 2400   loss: 0.1420904\n",
      "iteration: 33 / 2400   loss: 0.1668607\n",
      "iteration: 34 / 2400   loss: 0.28937513\n",
      "iteration: 35 / 2400   loss: 0.3682878\n",
      "iteration: 36 / 2400   loss: 0.084996395\n",
      "iteration: 37 / 2400   loss: 0.23577589\n",
      "iteration: 38 / 2400   loss: 0.36578068\n",
      "iteration: 39 / 2400   loss: 0.27838925\n",
      "iteration: 40 / 2400   loss: 0.09316665\n",
      "iteration: 41 / 2400   loss: 0.48387176\n",
      "iteration: 42 / 2400   loss: 0.39764225\n",
      "iteration: 43 / 2400   loss: 0.31228772\n",
      "iteration: 44 / 2400   loss: 0.26491475\n",
      "iteration: 45 / 2400   loss: 0.18823719\n",
      "iteration: 46 / 2400   loss: 0.5599106\n",
      "iteration: 47 / 2400   loss: 0.21201243\n",
      "iteration: 48 / 2400   loss: 0.16524635\n",
      "iteration: 49 / 2400   loss: 0.17293234\n",
      "iteration: 50 / 2400   loss: 0.5292939\n",
      "iteration: 51 / 2400   loss: 0.5971494\n",
      "iteration: 52 / 2400   loss: 0.29017085\n",
      "iteration: 53 / 2400   loss: 0.12458357\n",
      "iteration: 54 / 2400   loss: 0.38343903\n",
      "iteration: 55 / 2400   loss: 0.64509887\n",
      "iteration: 56 / 2400   loss: 0.11189993\n",
      "iteration: 57 / 2400   loss: 0.2834725\n",
      "iteration: 58 / 2400   loss: 0.08282389\n",
      "iteration: 59 / 2400   loss: 0.16047886\n",
      "iteration: 60 / 2400   loss: 0.1437962\n",
      "iteration: 61 / 2400   loss: 0.47527367\n",
      "iteration: 62 / 2400   loss: 0.2193762\n",
      "iteration: 63 / 2400   loss: 0.16623516\n",
      "iteration: 64 / 2400   loss: 0.46566665\n",
      "iteration: 65 / 2400   loss: 0.64596385\n",
      "iteration: 66 / 2400   loss: 0.016281739\n",
      "iteration: 67 / 2400   loss: 0.24120755\n",
      "iteration: 68 / 2400   loss: 0.059093624\n",
      "iteration: 69 / 2400   loss: 0.062586844\n",
      "iteration: 70 / 2400   loss: 0.027259674\n",
      "iteration: 71 / 2400   loss: 0.2694663\n",
      "iteration: 72 / 2400   loss: 0.070216306\n",
      "iteration: 73 / 2400   loss: 0.05598383\n",
      "iteration: 74 / 2400   loss: 0.010216751\n",
      "iteration: 75 / 2400   loss: 0.3328514\n",
      "iteration: 76 / 2400   loss: 0.12658511\n",
      "iteration: 77 / 2400   loss: 0.33869308\n",
      "iteration: 78 / 2400   loss: 0.4356298\n",
      "iteration: 79 / 2400   loss: 0.1438176\n",
      "iteration: 80 / 2400   loss: 0.120140225\n",
      "iteration: 81 / 2400   loss: 0.0780777\n",
      "iteration: 82 / 2400   loss: 0.28815114\n",
      "iteration: 83 / 2400   loss: 0.104008414\n",
      "iteration: 84 / 2400   loss: 0.6149343\n",
      "iteration: 85 / 2400   loss: 0.03349081\n",
      "iteration: 86 / 2400   loss: 0.16429923\n",
      "iteration: 87 / 2400   loss: 0.107835524\n",
      "iteration: 88 / 2400   loss: 0.13704666\n",
      "iteration: 89 / 2400   loss: 0.3123407\n",
      "iteration: 90 / 2400   loss: 0.15474966\n",
      "iteration: 91 / 2400   loss: 0.24451645\n",
      "iteration: 92 / 2400   loss: 0.099943064\n",
      "iteration: 93 / 2400   loss: 0.16444756\n",
      "iteration: 94 / 2400   loss: 0.06861994\n",
      "iteration: 95 / 2400   loss: 0.21087885\n",
      "iteration: 96 / 2400   loss: 0.40945065\n",
      "iteration: 97 / 2400   loss: 0.6979376\n",
      "iteration: 98 / 2400   loss: 0.331218\n",
      "iteration: 99 / 2400   loss: 0.104119025\n",
      "iteration: 100 / 2400   loss: 0.17223181\n",
      "iteration: 101 / 2400   loss: 0.11015678\n",
      "iteration: 102 / 2400   loss: 0.11968436\n",
      "iteration: 103 / 2400   loss: 0.3430187\n",
      "iteration: 104 / 2400   loss: 0.36050588\n",
      "iteration: 105 / 2400   loss: 0.14514993\n",
      "iteration: 106 / 2400   loss: 0.20297176\n",
      "iteration: 107 / 2400   loss: 0.021158552\n",
      "iteration: 108 / 2400   loss: 0.27079692\n",
      "iteration: 109 / 2400   loss: 0.6766289\n",
      "iteration: 110 / 2400   loss: 0.14407474\n",
      "iteration: 111 / 2400   loss: 0.24601567\n",
      "iteration: 112 / 2400   loss: 0.1522976\n",
      "iteration: 113 / 2400   loss: 0.2722944\n",
      "iteration: 114 / 2400   loss: 0.26191846\n",
      "iteration: 115 / 2400   loss: 0.05418275\n",
      "iteration: 116 / 2400   loss: 0.05457617\n",
      "iteration: 117 / 2400   loss: 0.10519403\n",
      "iteration: 118 / 2400   loss: 0.27328515\n",
      "iteration: 119 / 2400   loss: 0.116515934\n",
      "iteration: 120 / 2400   loss: 0.069900066\n",
      "iteration: 121 / 2400   loss: 0.2781976\n",
      "iteration: 122 / 2400   loss: 0.21220867\n",
      "iteration: 123 / 2400   loss: 0.58873373\n",
      "iteration: 124 / 2400   loss: 0.123931296\n",
      "iteration: 125 / 2400   loss: 0.15164441\n",
      "iteration: 126 / 2400   loss: 0.17794764\n",
      "iteration: 127 / 2400   loss: 0.20266105\n",
      "iteration: 128 / 2400   loss: 0.045093603\n",
      "iteration: 129 / 2400   loss: 0.3200605\n",
      "iteration: 130 / 2400   loss: 0.05652693\n",
      "iteration: 131 / 2400   loss: 0.29523087\n",
      "iteration: 132 / 2400   loss: 0.04950195\n",
      "iteration: 133 / 2400   loss: 0.3027774\n",
      "iteration: 134 / 2400   loss: 0.046451412\n",
      "iteration: 135 / 2400   loss: 0.37495\n",
      "iteration: 136 / 2400   loss: 0.09637081\n",
      "iteration: 137 / 2400   loss: 0.22564417\n",
      "iteration: 138 / 2400   loss: 0.07453747\n",
      "iteration: 139 / 2400   loss: 0.16107483\n",
      "iteration: 140 / 2400   loss: 0.26802993\n",
      "iteration: 141 / 2400   loss: 0.8917102\n",
      "iteration: 142 / 2400   loss: 0.27821523\n",
      "iteration: 143 / 2400   loss: 0.14089338\n",
      "iteration: 144 / 2400   loss: 0.06833907\n",
      "iteration: 145 / 2400   loss: 0.09041104\n",
      "iteration: 146 / 2400   loss: 0.22606021\n",
      "iteration: 147 / 2400   loss: 0.12483106\n",
      "iteration: 148 / 2400   loss: 0.39532766\n",
      "iteration: 149 / 2400   loss: 0.17520237\n",
      "iteration: 150 / 2400   loss: 0.14628436\n",
      "iteration: 151 / 2400   loss: 0.2592496\n",
      "iteration: 152 / 2400   loss: 0.06444894\n",
      "iteration: 153 / 2400   loss: 0.07665655\n",
      "iteration: 154 / 2400   loss: 0.17338315\n",
      "iteration: 155 / 2400   loss: 0.14610016\n",
      "iteration: 156 / 2400   loss: 0.19076031\n",
      "iteration: 157 / 2400   loss: 0.21228783\n",
      "iteration: 158 / 2400   loss: 0.10937401\n",
      "iteration: 159 / 2400   loss: 0.13616519\n",
      "iteration: 160 / 2400   loss: 0.0929608\n",
      "iteration: 161 / 2400   loss: 0.16554825\n",
      "iteration: 162 / 2400   loss: 0.19479504\n",
      "iteration: 163 / 2400   loss: 0.8347929\n",
      "iteration: 164 / 2400   loss: 0.18605012\n",
      "iteration: 165 / 2400   loss: 0.19975601\n",
      "iteration: 166 / 2400   loss: 0.23825508\n",
      "iteration: 167 / 2400   loss: 0.37669113\n",
      "iteration: 168 / 2400   loss: 0.1474844\n",
      "iteration: 169 / 2400   loss: 0.23826519\n",
      "iteration: 170 / 2400   loss: 0.03471756\n",
      "iteration: 171 / 2400   loss: 0.268825\n",
      "iteration: 172 / 2400   loss: 0.45397285\n",
      "iteration: 173 / 2400   loss: 0.2573753\n",
      "iteration: 174 / 2400   loss: 0.1911636\n",
      "iteration: 175 / 2400   loss: 0.300918\n",
      "iteration: 176 / 2400   loss: 0.10777706\n",
      "iteration: 177 / 2400   loss: 0.08737438\n",
      "iteration: 178 / 2400   loss: 0.23317567\n",
      "iteration: 179 / 2400   loss: 0.16408142\n",
      "iteration: 180 / 2400   loss: 0.13121352\n",
      "iteration: 181 / 2400   loss: 0.74012053\n",
      "iteration: 182 / 2400   loss: 0.14383434\n",
      "iteration: 183 / 2400   loss: 0.04567238\n",
      "iteration: 184 / 2400   loss: 0.05987964\n",
      "iteration: 185 / 2400   loss: 0.07407271\n",
      "iteration: 186 / 2400   loss: 0.5413553\n",
      "iteration: 187 / 2400   loss: 0.43619597\n",
      "iteration: 188 / 2400   loss: 0.29722047\n",
      "iteration: 189 / 2400   loss: 0.22040778\n",
      "iteration: 190 / 2400   loss: 0.112989925\n",
      "iteration: 191 / 2400   loss: 0.14896618\n",
      "iteration: 192 / 2400   loss: 0.04371967\n",
      "iteration: 193 / 2400   loss: 0.19144458\n",
      "iteration: 194 / 2400   loss: 0.30314323\n",
      "iteration: 195 / 2400   loss: 0.18311249\n",
      "iteration: 196 / 2400   loss: 0.03407564\n",
      "iteration: 197 / 2400   loss: 0.2716968\n",
      "iteration: 198 / 2400   loss: 0.25516796\n",
      "iteration: 199 / 2400   loss: 0.4067427\n",
      "iteration: 200 / 2400   loss: 0.10470354\n",
      "iteration: 201 / 2400   loss: 0.052238237\n",
      "iteration: 202 / 2400   loss: 0.13576573\n",
      "iteration: 203 / 2400   loss: 0.15089728\n",
      "iteration: 204 / 2400   loss: 0.09825327\n",
      "iteration: 205 / 2400   loss: 0.31602195\n",
      "iteration: 206 / 2400   loss: 0.24360086\n",
      "iteration: 207 / 2400   loss: 0.25279745\n",
      "iteration: 208 / 2400   loss: 0.32366607\n",
      "iteration: 209 / 2400   loss: 0.10843585\n",
      "iteration: 210 / 2400   loss: 0.05638789\n",
      "iteration: 211 / 2400   loss: 0.14066869\n",
      "iteration: 212 / 2400   loss: 0.3366968\n",
      "iteration: 213 / 2400   loss: 0.34231296\n",
      "iteration: 214 / 2400   loss: 0.67521906\n",
      "iteration: 215 / 2400   loss: 0.23555069\n",
      "iteration: 216 / 2400   loss: 0.21009105\n",
      "iteration: 217 / 2400   loss: 0.5008344\n",
      "iteration: 218 / 2400   loss: 0.22923101\n",
      "iteration: 219 / 2400   loss: 0.091318004\n",
      "iteration: 220 / 2400   loss: 0.17988817\n",
      "iteration: 221 / 2400   loss: 0.07031768\n",
      "iteration: 222 / 2400   loss: 0.16322052\n",
      "iteration: 223 / 2400   loss: 0.35955963\n",
      "iteration: 224 / 2400   loss: 0.074878104\n",
      "iteration: 225 / 2400   loss: 0.124653645\n",
      "iteration: 226 / 2400   loss: 0.3055308\n",
      "iteration: 227 / 2400   loss: 0.29804644\n",
      "iteration: 228 / 2400   loss: 0.08772394\n",
      "iteration: 229 / 2400   loss: 0.19121026\n",
      "iteration: 230 / 2400   loss: 0.5202063\n",
      "iteration: 231 / 2400   loss: 0.19850647\n",
      "iteration: 232 / 2400   loss: 0.333161\n",
      "iteration: 233 / 2400   loss: 0.26708558\n",
      "iteration: 234 / 2400   loss: 0.66209686\n",
      "iteration: 235 / 2400   loss: 0.107455835\n",
      "iteration: 236 / 2400   loss: 0.931848\n",
      "iteration: 237 / 2400   loss: 0.30074218\n",
      "iteration: 238 / 2400   loss: 0.112062976\n",
      "iteration: 239 / 2400   loss: 0.19348057\n",
      "iteration: 240 / 2400   loss: 0.023752738\n",
      "iteration: 241 / 2400   loss: 0.1285396\n",
      "iteration: 242 / 2400   loss: 0.14448002\n",
      "iteration: 243 / 2400   loss: 0.04429899\n",
      "iteration: 244 / 2400   loss: 0.08929449\n",
      "iteration: 245 / 2400   loss: 0.1942692\n",
      "iteration: 246 / 2400   loss: 0.2827216\n",
      "iteration: 247 / 2400   loss: 0.21418792\n",
      "iteration: 248 / 2400   loss: 0.05341417\n",
      "iteration: 249 / 2400   loss: 0.43423232\n",
      "iteration: 250 / 2400   loss: 0.11911972\n",
      "iteration: 251 / 2400   loss: 0.7579682\n",
      "iteration: 252 / 2400   loss: 0.15471923\n",
      "iteration: 253 / 2400   loss: 0.13241062\n",
      "iteration: 254 / 2400   loss: 0.3244337\n",
      "iteration: 255 / 2400   loss: 0.112304755\n",
      "iteration: 256 / 2400   loss: 0.038768254\n",
      "iteration: 257 / 2400   loss: 0.32768288\n",
      "iteration: 258 / 2400   loss: 0.1685796\n",
      "iteration: 259 / 2400   loss: 0.50756323\n",
      "iteration: 260 / 2400   loss: 0.33654854\n",
      "iteration: 261 / 2400   loss: 0.24065676\n",
      "iteration: 262 / 2400   loss: 0.19916713\n",
      "iteration: 263 / 2400   loss: 0.095765114\n",
      "iteration: 264 / 2400   loss: 0.09685346\n",
      "iteration: 265 / 2400   loss: 0.27772287\n",
      "iteration: 266 / 2400   loss: 0.037924804\n",
      "iteration: 267 / 2400   loss: 0.2999056\n",
      "iteration: 268 / 2400   loss: 0.13463408\n",
      "iteration: 269 / 2400   loss: 0.4406263\n",
      "iteration: 270 / 2400   loss: 0.30866632\n",
      "iteration: 271 / 2400   loss: 0.0710718\n",
      "iteration: 272 / 2400   loss: 0.10844097\n",
      "iteration: 273 / 2400   loss: 0.45702094\n",
      "iteration: 274 / 2400   loss: 0.56143963\n",
      "iteration: 275 / 2400   loss: 0.14873622\n",
      "iteration: 276 / 2400   loss: 0.6553702\n",
      "iteration: 277 / 2400   loss: 0.37487742\n",
      "iteration: 278 / 2400   loss: 0.12856978\n",
      "iteration: 279 / 2400   loss: 0.20677811\n",
      "iteration: 280 / 2400   loss: 0.1130558\n",
      "iteration: 281 / 2400   loss: 0.37942556\n",
      "iteration: 282 / 2400   loss: 0.15518713\n",
      "iteration: 283 / 2400   loss: 0.2610983\n",
      "iteration: 284 / 2400   loss: 0.11553753\n",
      "iteration: 285 / 2400   loss: 0.12223912\n",
      "iteration: 286 / 2400   loss: 0.26183042\n",
      "iteration: 287 / 2400   loss: 0.4783626\n",
      "iteration: 288 / 2400   loss: 0.2524424\n",
      "iteration: 289 / 2400   loss: 0.1811058\n",
      "iteration: 290 / 2400   loss: 0.6171828\n",
      "iteration: 291 / 2400   loss: 0.50447947\n",
      "iteration: 292 / 2400   loss: 0.2597678\n",
      "iteration: 293 / 2400   loss: 0.3093848\n",
      "iteration: 294 / 2400   loss: 0.33507386\n",
      "iteration: 295 / 2400   loss: 0.2630703\n",
      "iteration: 296 / 2400   loss: 0.23808603\n",
      "iteration: 297 / 2400   loss: 0.37778825\n",
      "iteration: 298 / 2400   loss: 0.25545904\n",
      "iteration: 299 / 2400   loss: 0.22917402\n",
      "iteration: 300 / 2400   loss: 0.16461237\n",
      "iteration: 301 / 2400   loss: 0.13111706\n",
      "iteration: 302 / 2400   loss: 0.19641025\n",
      "iteration: 303 / 2400   loss: 0.06426439\n",
      "iteration: 304 / 2400   loss: 0.43872455\n",
      "iteration: 305 / 2400   loss: 0.30462036\n",
      "iteration: 306 / 2400   loss: 0.20622915\n",
      "iteration: 307 / 2400   loss: 0.05344815\n",
      "iteration: 308 / 2400   loss: 0.20812233\n",
      "iteration: 309 / 2400   loss: 0.08750037\n",
      "iteration: 310 / 2400   loss: 0.29240265\n",
      "iteration: 311 / 2400   loss: 0.4463476\n",
      "iteration: 312 / 2400   loss: 0.12874009\n",
      "iteration: 313 / 2400   loss: 0.19291767\n",
      "iteration: 314 / 2400   loss: 0.27334926\n",
      "iteration: 315 / 2400   loss: 0.2610828\n",
      "iteration: 316 / 2400   loss: 0.6877237\n",
      "iteration: 317 / 2400   loss: 0.17196988\n",
      "iteration: 318 / 2400   loss: 0.16897114\n",
      "iteration: 319 / 2400   loss: 0.3867895\n",
      "iteration: 320 / 2400   loss: 0.5162673\n",
      "iteration: 321 / 2400   loss: 0.08740179\n",
      "iteration: 322 / 2400   loss: 0.24206375\n",
      "iteration: 323 / 2400   loss: 0.0951406\n",
      "iteration: 324 / 2400   loss: 0.13311923\n",
      "iteration: 325 / 2400   loss: 0.3370378\n",
      "iteration: 326 / 2400   loss: 0.2050131\n",
      "iteration: 327 / 2400   loss: 0.098338164\n",
      "iteration: 328 / 2400   loss: 0.23374718\n",
      "iteration: 329 / 2400   loss: 0.98739105\n",
      "iteration: 330 / 2400   loss: 0.1561054\n",
      "iteration: 331 / 2400   loss: 0.30357277\n",
      "iteration: 332 / 2400   loss: 0.19690575\n",
      "iteration: 333 / 2400   loss: 0.24411228\n",
      "iteration: 334 / 2400   loss: 0.098732665\n",
      "iteration: 335 / 2400   loss: 0.08325683\n",
      "iteration: 336 / 2400   loss: 0.14241795\n",
      "iteration: 337 / 2400   loss: 0.09291302\n",
      "iteration: 338 / 2400   loss: 0.57344884\n",
      "iteration: 339 / 2400   loss: 0.24690998\n",
      "iteration: 340 / 2400   loss: 0.36722904\n",
      "iteration: 341 / 2400   loss: 0.098765776\n",
      "iteration: 342 / 2400   loss: 0.09934185\n",
      "iteration: 343 / 2400   loss: 0.03928879\n",
      "iteration: 344 / 2400   loss: 0.045543328\n",
      "iteration: 345 / 2400   loss: 0.12450794\n",
      "iteration: 346 / 2400   loss: 0.3626541\n",
      "iteration: 347 / 2400   loss: 0.0852001\n",
      "iteration: 348 / 2400   loss: 0.43394214\n",
      "iteration: 349 / 2400   loss: 0.6087223\n",
      "iteration: 350 / 2400   loss: 0.48218596\n",
      "iteration: 351 / 2400   loss: 0.6823914\n",
      "iteration: 352 / 2400   loss: 0.21493126\n",
      "iteration: 353 / 2400   loss: 0.17985073\n",
      "iteration: 354 / 2400   loss: 0.10193935\n",
      "iteration: 355 / 2400   loss: 0.87364197\n",
      "iteration: 356 / 2400   loss: 0.5094822\n",
      "iteration: 357 / 2400   loss: 0.60520995\n",
      "iteration: 358 / 2400   loss: 0.22657156\n",
      "iteration: 359 / 2400   loss: 0.14253908\n",
      "iteration: 360 / 2400   loss: 0.12534727\n",
      "iteration: 361 / 2400   loss: 0.090996444\n",
      "iteration: 362 / 2400   loss: 0.26557234\n",
      "iteration: 363 / 2400   loss: 0.1245221\n",
      "iteration: 364 / 2400   loss: 0.2046861\n",
      "iteration: 365 / 2400   loss: 0.24839498\n",
      "iteration: 366 / 2400   loss: 0.14517443\n",
      "iteration: 367 / 2400   loss: 0.24843693\n",
      "iteration: 368 / 2400   loss: 0.14035162\n",
      "iteration: 369 / 2400   loss: 0.26442352\n",
      "iteration: 370 / 2400   loss: 0.19807456\n",
      "iteration: 371 / 2400   loss: 0.1515777\n",
      "iteration: 372 / 2400   loss: 0.23513481\n",
      "iteration: 373 / 2400   loss: 0.3026472\n",
      "iteration: 374 / 2400   loss: 0.21551718\n",
      "iteration: 375 / 2400   loss: 0.046488103\n",
      "iteration: 376 / 2400   loss: 0.340562\n",
      "iteration: 377 / 2400   loss: 0.117102586\n",
      "iteration: 378 / 2400   loss: 0.8704306\n",
      "iteration: 379 / 2400   loss: 0.37052345\n",
      "iteration: 380 / 2400   loss: 0.14569506\n",
      "iteration: 381 / 2400   loss: 0.14825968\n",
      "iteration: 382 / 2400   loss: 0.25119025\n",
      "iteration: 383 / 2400   loss: 0.6060986\n",
      "iteration: 384 / 2400   loss: 0.3420157\n",
      "iteration: 385 / 2400   loss: 0.492941\n",
      "iteration: 386 / 2400   loss: 0.36212993\n",
      "iteration: 387 / 2400   loss: 0.23487778\n",
      "iteration: 388 / 2400   loss: 0.08086735\n",
      "iteration: 389 / 2400   loss: 0.051899854\n",
      "iteration: 390 / 2400   loss: 0.19985309\n",
      "iteration: 391 / 2400   loss: 0.19228317\n",
      "iteration: 392 / 2400   loss: 0.14658028\n",
      "iteration: 393 / 2400   loss: 0.34427577\n",
      "iteration: 394 / 2400   loss: 0.02653593\n",
      "iteration: 395 / 2400   loss: 0.08548894\n",
      "iteration: 396 / 2400   loss: 0.0860104\n",
      "iteration: 397 / 2400   loss: 0.105326675\n",
      "iteration: 398 / 2400   loss: 0.12918304\n",
      "iteration: 399 / 2400   loss: 0.06342376\n",
      "iteration: 400 / 2400   loss: 0.06068678\n",
      "iteration: 401 / 2400   loss: 0.33376622\n",
      "iteration: 402 / 2400   loss: 0.44962785\n",
      "iteration: 403 / 2400   loss: 0.31228974\n",
      "iteration: 404 / 2400   loss: 0.10713358\n",
      "iteration: 405 / 2400   loss: 0.44005376\n",
      "iteration: 406 / 2400   loss: 0.045310117\n",
      "iteration: 407 / 2400   loss: 0.21171245\n",
      "iteration: 408 / 2400   loss: 0.26204106\n",
      "iteration: 409 / 2400   loss: 0.83183396\n",
      "iteration: 410 / 2400   loss: 0.7879671\n",
      "iteration: 411 / 2400   loss: 0.6393131\n",
      "iteration: 412 / 2400   loss: 0.3024506\n",
      "iteration: 413 / 2400   loss: 0.18425497\n",
      "iteration: 414 / 2400   loss: 0.03940032\n",
      "iteration: 415 / 2400   loss: 0.38528633\n",
      "iteration: 416 / 2400   loss: 0.25245097\n",
      "iteration: 417 / 2400   loss: 0.09368722\n",
      "iteration: 418 / 2400   loss: 0.705052\n",
      "iteration: 419 / 2400   loss: 0.20041117\n",
      "iteration: 420 / 2400   loss: 0.38680905\n",
      "iteration: 421 / 2400   loss: 0.18697369\n",
      "iteration: 422 / 2400   loss: 0.05948359\n",
      "iteration: 423 / 2400   loss: 0.1927844\n",
      "iteration: 424 / 2400   loss: 0.34452504\n",
      "iteration: 425 / 2400   loss: 0.11994855\n",
      "iteration: 426 / 2400   loss: 0.20721516\n",
      "iteration: 427 / 2400   loss: 0.09682662\n",
      "iteration: 428 / 2400   loss: 0.08508607\n",
      "iteration: 429 / 2400   loss: 0.5120834\n",
      "iteration: 430 / 2400   loss: 0.5316326\n",
      "iteration: 431 / 2400   loss: 0.2071891\n",
      "iteration: 432 / 2400   loss: 0.09405487\n",
      "iteration: 433 / 2400   loss: 0.14451091\n",
      "iteration: 434 / 2400   loss: 0.06289272\n",
      "iteration: 435 / 2400   loss: 0.62292236\n",
      "iteration: 436 / 2400   loss: 0.17695254\n",
      "iteration: 437 / 2400   loss: 0.035493754\n",
      "iteration: 438 / 2400   loss: 0.17236584\n",
      "iteration: 439 / 2400   loss: 0.0996124\n",
      "iteration: 440 / 2400   loss: 0.75015205\n",
      "iteration: 441 / 2400   loss: 0.0551243\n",
      "iteration: 442 / 2400   loss: 0.17524494\n",
      "iteration: 443 / 2400   loss: 0.21296167\n",
      "iteration: 444 / 2400   loss: 0.07314225\n",
      "iteration: 445 / 2400   loss: 0.11078271\n",
      "iteration: 446 / 2400   loss: 0.18852963\n",
      "iteration: 447 / 2400   loss: 0.19659607\n",
      "iteration: 448 / 2400   loss: 0.055631094\n",
      "iteration: 449 / 2400   loss: 0.28340802\n",
      "iteration: 450 / 2400   loss: 0.40219623\n",
      "iteration: 451 / 2400   loss: 0.018025398\n",
      "iteration: 452 / 2400   loss: 0.24644853\n",
      "iteration: 453 / 2400   loss: 0.09037525\n",
      "iteration: 454 / 2400   loss: 0.10281357\n",
      "iteration: 455 / 2400   loss: 0.12897456\n",
      "iteration: 456 / 2400   loss: 0.1439263\n",
      "iteration: 457 / 2400   loss: 0.12827413\n",
      "iteration: 458 / 2400   loss: 0.39267975\n",
      "iteration: 459 / 2400   loss: 0.031509504\n",
      "iteration: 460 / 2400   loss: 0.12489608\n",
      "iteration: 461 / 2400   loss: 0.15878128\n",
      "iteration: 462 / 2400   loss: 0.19176055\n",
      "iteration: 463 / 2400   loss: 0.53539574\n",
      "iteration: 464 / 2400   loss: 0.33889663\n",
      "iteration: 465 / 2400   loss: 0.54676294\n",
      "iteration: 466 / 2400   loss: 0.41045886\n",
      "iteration: 467 / 2400   loss: 0.19603422\n",
      "iteration: 468 / 2400   loss: 0.23905317\n",
      "iteration: 469 / 2400   loss: 0.24956453\n",
      "iteration: 470 / 2400   loss: 0.3053431\n",
      "iteration: 471 / 2400   loss: 0.28687227\n",
      "iteration: 472 / 2400   loss: 0.49869362\n",
      "iteration: 473 / 2400   loss: 0.1670729\n",
      "iteration: 474 / 2400   loss: 0.14124866\n",
      "iteration: 475 / 2400   loss: 0.13271293\n",
      "iteration: 476 / 2400   loss: 0.32385048\n",
      "iteration: 477 / 2400   loss: 0.052843504\n",
      "iteration: 478 / 2400   loss: 0.28136638\n",
      "iteration: 479 / 2400   loss: 0.16025864\n",
      "iteration: 480 / 2400   loss: 0.13868658\n",
      "iteration: 481 / 2400   loss: 0.14630066\n",
      "iteration: 482 / 2400   loss: 0.0682496\n",
      "iteration: 483 / 2400   loss: 0.1552445\n",
      "iteration: 484 / 2400   loss: 0.23527642\n",
      "iteration: 485 / 2400   loss: 0.10794005\n",
      "iteration: 486 / 2400   loss: 0.0629277\n",
      "iteration: 487 / 2400   loss: 0.26006344\n",
      "iteration: 488 / 2400   loss: 0.18620315\n",
      "iteration: 489 / 2400   loss: 0.23756313\n",
      "iteration: 490 / 2400   loss: 0.20147802\n",
      "iteration: 491 / 2400   loss: 0.33806595\n",
      "iteration: 492 / 2400   loss: 0.45337892\n",
      "iteration: 493 / 2400   loss: 0.3425696\n",
      "iteration: 494 / 2400   loss: 0.15444307\n",
      "iteration: 495 / 2400   loss: 0.09601042\n",
      "iteration: 496 / 2400   loss: 0.14025256\n",
      "iteration: 497 / 2400   loss: 0.24917191\n",
      "iteration: 498 / 2400   loss: 0.1180785\n",
      "iteration: 499 / 2400   loss: 0.16684918\n",
      "iteration: 500 / 2400   loss: 0.34863472\n",
      "iteration: 501 / 2400   loss: 0.14558491\n",
      "iteration: 502 / 2400   loss: 0.076804906\n",
      "iteration: 503 / 2400   loss: 0.267827\n",
      "iteration: 504 / 2400   loss: 0.31661758\n",
      "iteration: 505 / 2400   loss: 0.15934816\n",
      "iteration: 506 / 2400   loss: 0.18722695\n",
      "iteration: 507 / 2400   loss: 0.53403085\n",
      "iteration: 508 / 2400   loss: 0.44564778\n",
      "iteration: 509 / 2400   loss: 0.16955128\n",
      "iteration: 510 / 2400   loss: 0.16126567\n",
      "iteration: 511 / 2400   loss: 0.3318502\n",
      "iteration: 512 / 2400   loss: 0.18516825\n",
      "iteration: 513 / 2400   loss: 0.102931224\n",
      "iteration: 514 / 2400   loss: 0.5886982\n",
      "iteration: 515 / 2400   loss: 0.09024347\n",
      "iteration: 516 / 2400   loss: 0.11524841\n",
      "iteration: 517 / 2400   loss: 0.034941263\n",
      "iteration: 518 / 2400   loss: 0.23024017\n",
      "iteration: 519 / 2400   loss: 0.49530998\n",
      "iteration: 520 / 2400   loss: 0.22759417\n",
      "iteration: 521 / 2400   loss: 0.33866975\n",
      "iteration: 522 / 2400   loss: 0.32932416\n",
      "iteration: 523 / 2400   loss: 0.278519\n",
      "iteration: 524 / 2400   loss: 0.40008086\n",
      "iteration: 525 / 2400   loss: 0.12747347\n",
      "iteration: 526 / 2400   loss: 0.2552609\n",
      "iteration: 527 / 2400   loss: 0.27716607\n",
      "iteration: 528 / 2400   loss: 0.20460448\n",
      "iteration: 529 / 2400   loss: 0.032377597\n",
      "iteration: 530 / 2400   loss: 0.099081494\n",
      "iteration: 531 / 2400   loss: 0.025322685\n",
      "iteration: 532 / 2400   loss: 0.06469618\n",
      "iteration: 533 / 2400   loss: 0.48988736\n",
      "iteration: 534 / 2400   loss: 0.1350437\n",
      "iteration: 535 / 2400   loss: 0.20912948\n",
      "iteration: 536 / 2400   loss: 0.20324054\n",
      "iteration: 537 / 2400   loss: 0.23532419\n",
      "iteration: 538 / 2400   loss: 0.45660427\n",
      "iteration: 539 / 2400   loss: 0.25798708\n",
      "iteration: 540 / 2400   loss: 0.064857304\n",
      "iteration: 541 / 2400   loss: 0.31509933\n",
      "iteration: 542 / 2400   loss: 0.16553082\n",
      "iteration: 543 / 2400   loss: 0.0637652\n",
      "iteration: 544 / 2400   loss: 0.11073195\n",
      "iteration: 545 / 2400   loss: 0.06687507\n",
      "iteration: 546 / 2400   loss: 0.06052805\n",
      "iteration: 547 / 2400   loss: 0.27645707\n",
      "iteration: 548 / 2400   loss: 0.41144693\n",
      "iteration: 549 / 2400   loss: 0.31280842\n",
      "iteration: 550 / 2400   loss: 0.3887123\n",
      "iteration: 551 / 2400   loss: 0.24300705\n",
      "iteration: 552 / 2400   loss: 0.20660484\n",
      "iteration: 553 / 2400   loss: 0.058082562\n",
      "iteration: 554 / 2400   loss: 0.13670985\n",
      "iteration: 555 / 2400   loss: 0.2601342\n",
      "iteration: 556 / 2400   loss: 0.09846659\n",
      "iteration: 557 / 2400   loss: 0.5948227\n",
      "iteration: 558 / 2400   loss: 0.40029457\n",
      "iteration: 559 / 2400   loss: 0.12053516\n",
      "iteration: 560 / 2400   loss: 0.25980914\n",
      "iteration: 561 / 2400   loss: 0.43805444\n",
      "iteration: 562 / 2400   loss: 0.09147324\n",
      "iteration: 563 / 2400   loss: 0.14279763\n",
      "iteration: 564 / 2400   loss: 0.2727912\n",
      "iteration: 565 / 2400   loss: 0.183731\n",
      "iteration: 566 / 2400   loss: 0.20257893\n",
      "iteration: 567 / 2400   loss: 0.34953475\n",
      "iteration: 568 / 2400   loss: 0.15527292\n",
      "iteration: 569 / 2400   loss: 0.22758567\n",
      "iteration: 570 / 2400   loss: 0.09605512\n",
      "iteration: 571 / 2400   loss: 0.19474052\n",
      "iteration: 572 / 2400   loss: 0.24655919\n",
      "iteration: 573 / 2400   loss: 0.35118774\n",
      "iteration: 574 / 2400   loss: 0.39151657\n",
      "iteration: 575 / 2400   loss: 0.43968886\n",
      "iteration: 576 / 2400   loss: 0.09525575\n",
      "iteration: 577 / 2400   loss: 0.15045021\n",
      "iteration: 578 / 2400   loss: 0.109190054\n",
      "iteration: 579 / 2400   loss: 0.06920209\n",
      "iteration: 580 / 2400   loss: 0.08301271\n",
      "iteration: 581 / 2400   loss: 0.2655674\n",
      "iteration: 582 / 2400   loss: 0.19098444\n",
      "iteration: 583 / 2400   loss: 0.19603881\n",
      "iteration: 584 / 2400   loss: 0.49095467\n",
      "iteration: 585 / 2400   loss: 0.23720245\n",
      "iteration: 586 / 2400   loss: 0.18183227\n",
      "iteration: 587 / 2400   loss: 0.36997947\n",
      "iteration: 588 / 2400   loss: 0.27985272\n",
      "iteration: 589 / 2400   loss: 0.27196172\n",
      "iteration: 590 / 2400   loss: 0.16234672\n",
      "iteration: 591 / 2400   loss: 0.57292956\n",
      "iteration: 592 / 2400   loss: 0.55389315\n",
      "iteration: 593 / 2400   loss: 0.18107101\n",
      "iteration: 594 / 2400   loss: 0.103166826\n",
      "iteration: 595 / 2400   loss: 0.23792112\n",
      "iteration: 596 / 2400   loss: 0.29746187\n",
      "iteration: 597 / 2400   loss: 0.10448575\n",
      "iteration: 598 / 2400   loss: 0.069287576\n",
      "iteration: 599 / 2400   loss: 0.22978587\n",
      "iteration: 600 / 2400   loss: 0.12704498\n",
      "iteration: 601 / 2400   loss: 0.15241502\n",
      "iteration: 602 / 2400   loss: 0.09508275\n",
      "iteration: 603 / 2400   loss: 0.23655455\n",
      "iteration: 604 / 2400   loss: 0.06181986\n",
      "iteration: 605 / 2400   loss: 0.30713597\n",
      "iteration: 606 / 2400   loss: 0.14668515\n",
      "iteration: 607 / 2400   loss: 0.32558373\n",
      "iteration: 608 / 2400   loss: 0.21950608\n",
      "iteration: 609 / 2400   loss: 0.043112163\n",
      "iteration: 610 / 2400   loss: 0.1664943\n",
      "iteration: 611 / 2400   loss: 0.37472767\n",
      "iteration: 612 / 2400   loss: 0.11409769\n",
      "iteration: 613 / 2400   loss: 0.0901293\n",
      "iteration: 614 / 2400   loss: 0.13459991\n",
      "iteration: 615 / 2400   loss: 0.33415517\n",
      "iteration: 616 / 2400   loss: 0.13456182\n",
      "iteration: 617 / 2400   loss: 0.7054911\n",
      "iteration: 618 / 2400   loss: 0.047981597\n",
      "iteration: 619 / 2400   loss: 0.18975666\n",
      "iteration: 620 / 2400   loss: 0.27385688\n",
      "iteration: 621 / 2400   loss: 0.27234763\n",
      "iteration: 622 / 2400   loss: 0.08816028\n",
      "iteration: 623 / 2400   loss: 0.14025241\n",
      "iteration: 624 / 2400   loss: 0.17433125\n",
      "iteration: 625 / 2400   loss: 0.35030425\n",
      "iteration: 626 / 2400   loss: 0.12443199\n",
      "iteration: 627 / 2400   loss: 0.06620323\n",
      "iteration: 628 / 2400   loss: 0.10786874\n",
      "iteration: 629 / 2400   loss: 0.4062986\n",
      "iteration: 630 / 2400   loss: 0.4167991\n",
      "iteration: 631 / 2400   loss: 0.5258421\n",
      "iteration: 632 / 2400   loss: 0.24914353\n",
      "iteration: 633 / 2400   loss: 0.08583012\n",
      "iteration: 634 / 2400   loss: 0.48850372\n",
      "iteration: 635 / 2400   loss: 0.3977805\n",
      "iteration: 636 / 2400   loss: 0.5610698\n",
      "iteration: 637 / 2400   loss: 0.23768502\n",
      "iteration: 638 / 2400   loss: 0.3686602\n",
      "iteration: 639 / 2400   loss: 0.2336747\n",
      "iteration: 640 / 2400   loss: 0.39400908\n",
      "iteration: 641 / 2400   loss: 0.24775925\n",
      "iteration: 642 / 2400   loss: 0.5633085\n",
      "iteration: 643 / 2400   loss: 0.30070716\n",
      "iteration: 644 / 2400   loss: 0.035773125\n",
      "iteration: 645 / 2400   loss: 0.1089262\n",
      "iteration: 646 / 2400   loss: 0.3228212\n",
      "iteration: 647 / 2400   loss: 0.12915188\n",
      "iteration: 648 / 2400   loss: 0.22859722\n",
      "iteration: 649 / 2400   loss: 0.15311679\n",
      "iteration: 650 / 2400   loss: 0.29069316\n",
      "iteration: 651 / 2400   loss: 0.11098747\n",
      "iteration: 652 / 2400   loss: 0.40818155\n",
      "iteration: 653 / 2400   loss: 0.037871018\n",
      "iteration: 654 / 2400   loss: 0.036177788\n",
      "iteration: 655 / 2400   loss: 0.07058233\n",
      "iteration: 656 / 2400   loss: 0.6955234\n",
      "iteration: 657 / 2400   loss: 0.042752817\n",
      "iteration: 658 / 2400   loss: 0.17895165\n",
      "iteration: 659 / 2400   loss: 0.20281817\n",
      "iteration: 660 / 2400   loss: 0.2937305\n",
      "iteration: 661 / 2400   loss: 0.0653025\n",
      "iteration: 662 / 2400   loss: 0.12239859\n",
      "iteration: 663 / 2400   loss: 0.2818075\n",
      "iteration: 664 / 2400   loss: 0.070791125\n",
      "iteration: 665 / 2400   loss: 0.06836721\n",
      "iteration: 666 / 2400   loss: 0.2154645\n",
      "iteration: 667 / 2400   loss: 0.22349802\n",
      "iteration: 668 / 2400   loss: 0.5851221\n",
      "iteration: 669 / 2400   loss: 0.1470771\n",
      "iteration: 670 / 2400   loss: 0.05926722\n",
      "iteration: 671 / 2400   loss: 0.3654579\n",
      "iteration: 672 / 2400   loss: 0.22919068\n",
      "iteration: 673 / 2400   loss: 0.1137187\n",
      "iteration: 674 / 2400   loss: 0.15062413\n",
      "iteration: 675 / 2400   loss: 0.12360106\n",
      "iteration: 676 / 2400   loss: 0.17868866\n",
      "iteration: 677 / 2400   loss: 0.12143799\n",
      "iteration: 678 / 2400   loss: 0.17061852\n",
      "iteration: 679 / 2400   loss: 0.2629471\n",
      "iteration: 680 / 2400   loss: 0.19285077\n",
      "iteration: 681 / 2400   loss: 0.14869368\n",
      "iteration: 682 / 2400   loss: 0.32130486\n",
      "iteration: 683 / 2400   loss: 0.2909497\n",
      "iteration: 684 / 2400   loss: 0.21568705\n",
      "iteration: 685 / 2400   loss: 0.28206664\n",
      "iteration: 686 / 2400   loss: 0.20055306\n",
      "iteration: 687 / 2400   loss: 0.03530306\n",
      "iteration: 688 / 2400   loss: 0.35471866\n",
      "iteration: 689 / 2400   loss: 0.65366244\n",
      "iteration: 690 / 2400   loss: 0.25420147\n",
      "iteration: 691 / 2400   loss: 0.06762589\n",
      "iteration: 692 / 2400   loss: 0.12631838\n",
      "iteration: 693 / 2400   loss: 0.0334548\n",
      "iteration: 694 / 2400   loss: 0.022311067\n",
      "iteration: 695 / 2400   loss: 0.114230916\n",
      "iteration: 696 / 2400   loss: 0.44889474\n",
      "iteration: 697 / 2400   loss: 0.14773422\n",
      "iteration: 698 / 2400   loss: 0.06696879\n",
      "iteration: 699 / 2400   loss: 0.03965865\n",
      "iteration: 700 / 2400   loss: 0.54395944\n",
      "iteration: 701 / 2400   loss: 0.29559422\n",
      "iteration: 702 / 2400   loss: 0.7919327\n",
      "iteration: 703 / 2400   loss: 0.23564959\n",
      "iteration: 704 / 2400   loss: 0.62381655\n",
      "iteration: 705 / 2400   loss: 0.08562642\n",
      "iteration: 706 / 2400   loss: 0.16253883\n",
      "iteration: 707 / 2400   loss: 0.152507\n",
      "iteration: 708 / 2400   loss: 0.3382988\n",
      "iteration: 709 / 2400   loss: 0.38505322\n",
      "iteration: 710 / 2400   loss: 0.36902213\n",
      "iteration: 711 / 2400   loss: 0.50030655\n",
      "iteration: 712 / 2400   loss: 0.27329257\n",
      "iteration: 713 / 2400   loss: 0.47145244\n",
      "iteration: 714 / 2400   loss: 0.11979498\n",
      "iteration: 715 / 2400   loss: 0.108147494\n",
      "iteration: 716 / 2400   loss: 0.50813067\n",
      "iteration: 717 / 2400   loss: 0.44206864\n",
      "iteration: 718 / 2400   loss: 0.10730484\n",
      "iteration: 719 / 2400   loss: 0.18308991\n",
      "iteration: 720 / 2400   loss: 0.07497731\n",
      "iteration: 721 / 2400   loss: 0.21016221\n",
      "iteration: 722 / 2400   loss: 0.2783181\n",
      "iteration: 723 / 2400   loss: 0.17402793\n",
      "iteration: 724 / 2400   loss: 0.15006398\n",
      "iteration: 725 / 2400   loss: 0.19712357\n",
      "iteration: 726 / 2400   loss: 0.20156594\n",
      "iteration: 727 / 2400   loss: 0.23395015\n",
      "iteration: 728 / 2400   loss: 0.031102896\n",
      "iteration: 729 / 2400   loss: 0.08481829\n",
      "iteration: 730 / 2400   loss: 0.24068525\n",
      "iteration: 731 / 2400   loss: 0.04456611\n",
      "iteration: 732 / 2400   loss: 0.17999275\n",
      "iteration: 733 / 2400   loss: 0.16656445\n",
      "iteration: 734 / 2400   loss: 0.25505275\n",
      "iteration: 735 / 2400   loss: 0.051037654\n",
      "iteration: 736 / 2400   loss: 0.3278061\n",
      "iteration: 737 / 2400   loss: 0.46890396\n",
      "iteration: 738 / 2400   loss: 0.26617157\n",
      "iteration: 739 / 2400   loss: 0.12677959\n",
      "iteration: 740 / 2400   loss: 0.20775113\n",
      "iteration: 741 / 2400   loss: 0.21883392\n",
      "iteration: 742 / 2400   loss: 0.05186161\n",
      "iteration: 743 / 2400   loss: 0.13964245\n",
      "iteration: 744 / 2400   loss: 0.51054657\n",
      "iteration: 745 / 2400   loss: 0.072037846\n",
      "iteration: 746 / 2400   loss: 0.07207543\n",
      "iteration: 747 / 2400   loss: 0.13972789\n",
      "iteration: 748 / 2400   loss: 0.2712853\n",
      "iteration: 749 / 2400   loss: 0.33773378\n",
      "iteration: 750 / 2400   loss: 0.12008069\n",
      "iteration: 751 / 2400   loss: 0.2602994\n",
      "iteration: 752 / 2400   loss: 0.13366415\n",
      "iteration: 753 / 2400   loss: 0.1102037\n",
      "iteration: 754 / 2400   loss: 0.16883549\n",
      "iteration: 755 / 2400   loss: 0.27026588\n",
      "iteration: 756 / 2400   loss: 0.01847725\n",
      "iteration: 757 / 2400   loss: 0.056220006\n",
      "iteration: 758 / 2400   loss: 0.09963619\n",
      "iteration: 759 / 2400   loss: 0.0728261\n",
      "iteration: 760 / 2400   loss: 0.15480323\n",
      "iteration: 761 / 2400   loss: 0.15705171\n",
      "iteration: 762 / 2400   loss: 0.12703706\n",
      "iteration: 763 / 2400   loss: 0.09060675\n",
      "iteration: 764 / 2400   loss: 0.27965415\n",
      "iteration: 765 / 2400   loss: 0.23203857\n",
      "iteration: 766 / 2400   loss: 0.1043176\n",
      "iteration: 767 / 2400   loss: 0.15209115\n",
      "iteration: 768 / 2400   loss: 0.5969614\n",
      "iteration: 769 / 2400   loss: 0.10659119\n",
      "iteration: 770 / 2400   loss: 0.37071845\n",
      "iteration: 771 / 2400   loss: 0.27402833\n",
      "iteration: 772 / 2400   loss: 0.1776493\n",
      "iteration: 773 / 2400   loss: 0.18578663\n",
      "iteration: 774 / 2400   loss: 0.1933403\n",
      "iteration: 775 / 2400   loss: 0.3870724\n",
      "iteration: 776 / 2400   loss: 0.08712674\n",
      "iteration: 777 / 2400   loss: 0.09515768\n",
      "iteration: 778 / 2400   loss: 0.14990346\n",
      "iteration: 779 / 2400   loss: 0.09027833\n",
      "iteration: 780 / 2400   loss: 0.3266258\n",
      "iteration: 781 / 2400   loss: 0.15825644\n",
      "iteration: 782 / 2400   loss: 0.34798637\n",
      "iteration: 783 / 2400   loss: 0.14336164\n",
      "iteration: 784 / 2400   loss: 0.22608088\n",
      "iteration: 785 / 2400   loss: 0.1280148\n",
      "iteration: 786 / 2400   loss: 0.050744027\n",
      "iteration: 787 / 2400   loss: 0.03449539\n",
      "iteration: 788 / 2400   loss: 0.06986647\n",
      "iteration: 789 / 2400   loss: 0.082938366\n",
      "iteration: 790 / 2400   loss: 0.09262763\n",
      "iteration: 791 / 2400   loss: 0.029786386\n",
      "iteration: 792 / 2400   loss: 0.25759843\n",
      "iteration: 793 / 2400   loss: 0.4811914\n",
      "iteration: 794 / 2400   loss: 0.11081733\n",
      "iteration: 795 / 2400   loss: 0.19407701\n",
      "iteration: 796 / 2400   loss: 0.15550114\n",
      "iteration: 797 / 2400   loss: 0.36654803\n",
      "iteration: 798 / 2400   loss: 0.43614808\n",
      "iteration: 799 / 2400   loss: 0.042041093\n",
      "iteration: 800 / 2400   loss: 0.026568526\n",
      "iteration: 801 / 2400   loss: 0.32350224\n",
      "iteration: 802 / 2400   loss: 0.40122467\n",
      "iteration: 803 / 2400   loss: 0.59862536\n",
      "iteration: 804 / 2400   loss: 0.072118066\n",
      "iteration: 805 / 2400   loss: 0.09371687\n",
      "iteration: 806 / 2400   loss: 0.036647692\n",
      "iteration: 807 / 2400   loss: 0.5603561\n",
      "iteration: 808 / 2400   loss: 0.08403573\n",
      "iteration: 809 / 2400   loss: 0.13790599\n",
      "iteration: 810 / 2400   loss: 0.2467678\n",
      "iteration: 811 / 2400   loss: 0.25118214\n",
      "iteration: 812 / 2400   loss: 0.18106824\n",
      "iteration: 813 / 2400   loss: 0.3330935\n",
      "iteration: 814 / 2400   loss: 0.053097714\n",
      "iteration: 815 / 2400   loss: 0.5800231\n",
      "iteration: 816 / 2400   loss: 0.19433405\n",
      "iteration: 817 / 2400   loss: 0.08387621\n",
      "iteration: 818 / 2400   loss: 0.14664255\n",
      "iteration: 819 / 2400   loss: 0.081354864\n",
      "iteration: 820 / 2400   loss: 0.056945104\n",
      "iteration: 821 / 2400   loss: 0.16623372\n",
      "iteration: 822 / 2400   loss: 0.27290142\n",
      "iteration: 823 / 2400   loss: 0.40999633\n",
      "iteration: 824 / 2400   loss: 0.10768256\n",
      "iteration: 825 / 2400   loss: 0.14398769\n",
      "iteration: 826 / 2400   loss: 0.31771088\n",
      "iteration: 827 / 2400   loss: 0.82377493\n",
      "iteration: 828 / 2400   loss: 0.09151671\n",
      "iteration: 829 / 2400   loss: 0.2898581\n",
      "iteration: 830 / 2400   loss: 0.15329093\n",
      "iteration: 831 / 2400   loss: 0.33089188\n",
      "iteration: 832 / 2400   loss: 0.5849662\n",
      "iteration: 833 / 2400   loss: 0.19717933\n",
      "iteration: 834 / 2400   loss: 0.097555265\n",
      "iteration: 835 / 2400   loss: 0.25546777\n",
      "iteration: 836 / 2400   loss: 0.14557725\n",
      "iteration: 837 / 2400   loss: 0.22522278\n",
      "iteration: 838 / 2400   loss: 0.1940041\n",
      "iteration: 839 / 2400   loss: 0.1995967\n",
      "iteration: 840 / 2400   loss: 0.11745239\n",
      "iteration: 841 / 2400   loss: 0.3053983\n",
      "iteration: 842 / 2400   loss: 0.11254755\n",
      "iteration: 843 / 2400   loss: 0.2997093\n",
      "iteration: 844 / 2400   loss: 0.031346418\n",
      "iteration: 845 / 2400   loss: 0.044231366\n",
      "iteration: 846 / 2400   loss: 0.1931403\n",
      "iteration: 847 / 2400   loss: 0.23372665\n",
      "iteration: 848 / 2400   loss: 0.02895586\n",
      "iteration: 849 / 2400   loss: 0.046289004\n",
      "iteration: 850 / 2400   loss: 0.033256054\n",
      "iteration: 851 / 2400   loss: 0.05322109\n",
      "iteration: 852 / 2400   loss: 0.073409535\n",
      "iteration: 853 / 2400   loss: 0.29104316\n",
      "iteration: 854 / 2400   loss: 0.27343902\n",
      "iteration: 855 / 2400   loss: 0.37418976\n",
      "iteration: 856 / 2400   loss: 0.23220932\n",
      "iteration: 857 / 2400   loss: 0.1562759\n",
      "iteration: 858 / 2400   loss: 0.22557043\n",
      "iteration: 859 / 2400   loss: 0.16527843\n",
      "iteration: 860 / 2400   loss: 0.06400108\n",
      "iteration: 861 / 2400   loss: 0.033202246\n",
      "iteration: 862 / 2400   loss: 0.07006918\n",
      "iteration: 863 / 2400   loss: 0.13893323\n",
      "iteration: 864 / 2400   loss: 0.4045894\n",
      "iteration: 865 / 2400   loss: 0.19414681\n",
      "iteration: 866 / 2400   loss: 0.1402529\n",
      "iteration: 867 / 2400   loss: 0.11816441\n",
      "iteration: 868 / 2400   loss: 0.10931594\n",
      "iteration: 869 / 2400   loss: 0.19356468\n",
      "iteration: 870 / 2400   loss: 0.10640251\n",
      "iteration: 871 / 2400   loss: 0.20164591\n",
      "iteration: 872 / 2400   loss: 0.065030985\n",
      "iteration: 873 / 2400   loss: 0.044476327\n",
      "iteration: 874 / 2400   loss: 0.032382067\n",
      "iteration: 875 / 2400   loss: 0.07682477\n",
      "iteration: 876 / 2400   loss: 0.14114033\n",
      "iteration: 877 / 2400   loss: 0.07608365\n",
      "iteration: 878 / 2400   loss: 0.14944641\n",
      "iteration: 879 / 2400   loss: 0.05838909\n",
      "iteration: 880 / 2400   loss: 0.1895705\n",
      "iteration: 881 / 2400   loss: 0.053922586\n",
      "iteration: 882 / 2400   loss: 0.059755087\n",
      "iteration: 883 / 2400   loss: 0.19507018\n",
      "iteration: 884 / 2400   loss: 0.15412469\n",
      "iteration: 885 / 2400   loss: 0.19531904\n",
      "iteration: 886 / 2400   loss: 0.33305627\n",
      "iteration: 887 / 2400   loss: 0.18825695\n",
      "iteration: 888 / 2400   loss: 0.24422096\n",
      "iteration: 889 / 2400   loss: 0.43521538\n",
      "iteration: 890 / 2400   loss: 0.1377319\n",
      "iteration: 891 / 2400   loss: 0.18059075\n",
      "iteration: 892 / 2400   loss: 0.15946132\n",
      "iteration: 893 / 2400   loss: 0.32505393\n",
      "iteration: 894 / 2400   loss: 0.04760355\n",
      "iteration: 895 / 2400   loss: 0.089687005\n",
      "iteration: 896 / 2400   loss: 0.04040948\n",
      "iteration: 897 / 2400   loss: 0.09069391\n",
      "iteration: 898 / 2400   loss: 0.27569017\n",
      "iteration: 899 / 2400   loss: 0.07972744\n",
      "iteration: 900 / 2400   loss: 0.4056748\n",
      "iteration: 901 / 2400   loss: 0.14645404\n",
      "iteration: 902 / 2400   loss: 1.063664\n",
      "iteration: 903 / 2400   loss: 0.23416266\n",
      "iteration: 904 / 2400   loss: 0.19313507\n",
      "iteration: 905 / 2400   loss: 0.3713105\n",
      "iteration: 906 / 2400   loss: 0.7503813\n",
      "iteration: 907 / 2400   loss: 0.19904049\n",
      "iteration: 908 / 2400   loss: 0.17173885\n",
      "iteration: 909 / 2400   loss: 0.2893792\n",
      "iteration: 910 / 2400   loss: 0.34706992\n",
      "iteration: 911 / 2400   loss: 0.17974015\n",
      "iteration: 912 / 2400   loss: 0.24187751\n",
      "iteration: 913 / 2400   loss: 0.39602295\n",
      "iteration: 914 / 2400   loss: 0.31271827\n",
      "iteration: 915 / 2400   loss: 0.07395406\n",
      "iteration: 916 / 2400   loss: 0.32180774\n",
      "iteration: 917 / 2400   loss: 0.11724205\n",
      "iteration: 918 / 2400   loss: 0.100615\n",
      "iteration: 919 / 2400   loss: 0.033195134\n",
      "iteration: 920 / 2400   loss: 0.27416405\n",
      "iteration: 921 / 2400   loss: 0.23072729\n",
      "iteration: 922 / 2400   loss: 0.46378058\n",
      "iteration: 923 / 2400   loss: 0.13366859\n",
      "iteration: 924 / 2400   loss: 0.26519024\n",
      "iteration: 925 / 2400   loss: 0.25040692\n",
      "iteration: 926 / 2400   loss: 0.058849696\n",
      "iteration: 927 / 2400   loss: 0.05865341\n",
      "iteration: 928 / 2400   loss: 0.25103492\n",
      "iteration: 929 / 2400   loss: 0.22314905\n",
      "iteration: 930 / 2400   loss: 0.05769643\n",
      "iteration: 931 / 2400   loss: 0.31991473\n",
      "iteration: 932 / 2400   loss: 0.18103878\n",
      "iteration: 933 / 2400   loss: 0.17040055\n",
      "iteration: 934 / 2400   loss: 0.08689725\n",
      "iteration: 935 / 2400   loss: 0.07218113\n",
      "iteration: 936 / 2400   loss: 0.3592863\n",
      "iteration: 937 / 2400   loss: 0.14110322\n",
      "iteration: 938 / 2400   loss: 0.11994017\n",
      "iteration: 939 / 2400   loss: 0.362542\n",
      "iteration: 940 / 2400   loss: 0.102941\n",
      "iteration: 941 / 2400   loss: 0.09145542\n",
      "iteration: 942 / 2400   loss: 0.12105784\n",
      "iteration: 943 / 2400   loss: 0.05236864\n",
      "iteration: 944 / 2400   loss: 0.15977861\n",
      "iteration: 945 / 2400   loss: 0.13995403\n",
      "iteration: 946 / 2400   loss: 0.13312621\n",
      "iteration: 947 / 2400   loss: 0.13959706\n",
      "iteration: 948 / 2400   loss: 0.12757038\n",
      "iteration: 949 / 2400   loss: 0.16038628\n",
      "iteration: 950 / 2400   loss: 0.59280336\n",
      "iteration: 951 / 2400   loss: 0.12151935\n",
      "iteration: 952 / 2400   loss: 0.059038334\n",
      "iteration: 953 / 2400   loss: 0.45494688\n",
      "iteration: 954 / 2400   loss: 0.03771384\n",
      "iteration: 955 / 2400   loss: 0.26221377\n",
      "iteration: 956 / 2400   loss: 0.06767542\n",
      "iteration: 957 / 2400   loss: 0.18724403\n",
      "iteration: 958 / 2400   loss: 0.05157928\n",
      "iteration: 959 / 2400   loss: 0.15965196\n",
      "iteration: 960 / 2400   loss: 0.12415403\n",
      "iteration: 961 / 2400   loss: 0.083847545\n",
      "iteration: 962 / 2400   loss: 0.58062345\n",
      "iteration: 963 / 2400   loss: 0.29258156\n",
      "iteration: 964 / 2400   loss: 0.09353433\n",
      "iteration: 965 / 2400   loss: 0.12188803\n",
      "iteration: 966 / 2400   loss: 0.14081076\n",
      "iteration: 967 / 2400   loss: 0.25097406\n",
      "iteration: 968 / 2400   loss: 0.1384923\n",
      "iteration: 969 / 2400   loss: 0.43896943\n",
      "iteration: 970 / 2400   loss: 0.16276339\n",
      "iteration: 971 / 2400   loss: 0.25126916\n",
      "iteration: 972 / 2400   loss: 0.17780231\n",
      "iteration: 973 / 2400   loss: 0.23122211\n",
      "iteration: 974 / 2400   loss: 0.034309074\n",
      "iteration: 975 / 2400   loss: 0.096331984\n",
      "iteration: 976 / 2400   loss: 0.026807595\n",
      "iteration: 977 / 2400   loss: 0.20718232\n",
      "iteration: 978 / 2400   loss: 0.056059457\n",
      "iteration: 979 / 2400   loss: 0.17195393\n",
      "iteration: 980 / 2400   loss: 0.25179714\n",
      "iteration: 981 / 2400   loss: 0.34799325\n",
      "iteration: 982 / 2400   loss: 0.17706673\n",
      "iteration: 983 / 2400   loss: 0.1247085\n",
      "iteration: 984 / 2400   loss: 0.63232124\n",
      "iteration: 985 / 2400   loss: 0.35319373\n",
      "iteration: 986 / 2400   loss: 0.64434355\n",
      "iteration: 987 / 2400   loss: 0.27451864\n",
      "iteration: 988 / 2400   loss: 0.2566689\n",
      "iteration: 989 / 2400   loss: 0.27572212\n",
      "iteration: 990 / 2400   loss: 0.18948229\n",
      "iteration: 991 / 2400   loss: 0.12606879\n",
      "iteration: 992 / 2400   loss: 0.30340537\n",
      "iteration: 993 / 2400   loss: 0.07065688\n",
      "iteration: 994 / 2400   loss: 0.11905652\n",
      "iteration: 995 / 2400   loss: 0.32666668\n",
      "iteration: 996 / 2400   loss: 0.22140811\n",
      "iteration: 997 / 2400   loss: 0.0651525\n",
      "iteration: 998 / 2400   loss: 0.2950866\n",
      "iteration: 999 / 2400   loss: 0.13065471\n",
      "iteration: 1000 / 2400   loss: 0.1203114\n",
      "iteration: 1001 / 2400   loss: 0.09521777\n",
      "iteration: 1002 / 2400   loss: 0.30473033\n",
      "iteration: 1003 / 2400   loss: 0.10058115\n",
      "iteration: 1004 / 2400   loss: 0.06030353\n",
      "iteration: 1005 / 2400   loss: 0.04742769\n",
      "iteration: 1006 / 2400   loss: 0.08704491\n",
      "iteration: 1007 / 2400   loss: 0.14928429\n",
      "iteration: 1008 / 2400   loss: 0.12609214\n",
      "iteration: 1009 / 2400   loss: 0.3362582\n",
      "iteration: 1010 / 2400   loss: 0.29035178\n",
      "iteration: 1011 / 2400   loss: 0.09396072\n",
      "iteration: 1012 / 2400   loss: 0.19545262\n",
      "iteration: 1013 / 2400   loss: 0.3998612\n",
      "iteration: 1014 / 2400   loss: 0.097042926\n",
      "iteration: 1015 / 2400   loss: 0.07610502\n",
      "iteration: 1016 / 2400   loss: 0.015983762\n",
      "iteration: 1017 / 2400   loss: 0.11391321\n",
      "iteration: 1018 / 2400   loss: 0.038361546\n",
      "iteration: 1019 / 2400   loss: 0.08530817\n",
      "iteration: 1020 / 2400   loss: 0.31833556\n",
      "iteration: 1021 / 2400   loss: 0.0524685\n",
      "iteration: 1022 / 2400   loss: 0.31632903\n",
      "iteration: 1023 / 2400   loss: 0.18555865\n",
      "iteration: 1024 / 2400   loss: 0.11713014\n",
      "iteration: 1025 / 2400   loss: 0.15853748\n",
      "iteration: 1026 / 2400   loss: 0.14882495\n",
      "iteration: 1027 / 2400   loss: 0.086500235\n",
      "iteration: 1028 / 2400   loss: 0.34583962\n",
      "iteration: 1029 / 2400   loss: 0.18225648\n",
      "iteration: 1030 / 2400   loss: 0.13907264\n",
      "iteration: 1031 / 2400   loss: 0.16678075\n",
      "iteration: 1032 / 2400   loss: 0.3669657\n",
      "iteration: 1033 / 2400   loss: 0.58470064\n",
      "iteration: 1034 / 2400   loss: 0.14358951\n",
      "iteration: 1035 / 2400   loss: 0.10339567\n",
      "iteration: 1036 / 2400   loss: 0.15711388\n",
      "iteration: 1037 / 2400   loss: 0.43164885\n",
      "iteration: 1038 / 2400   loss: 0.35856155\n",
      "iteration: 1039 / 2400   loss: 0.23860271\n",
      "iteration: 1040 / 2400   loss: 0.26076758\n",
      "iteration: 1041 / 2400   loss: 0.06454397\n",
      "iteration: 1042 / 2400   loss: 0.08833346\n",
      "iteration: 1043 / 2400   loss: 0.2662412\n",
      "iteration: 1044 / 2400   loss: 0.11640315\n",
      "iteration: 1045 / 2400   loss: 0.11286109\n",
      "iteration: 1046 / 2400   loss: 0.096631125\n",
      "iteration: 1047 / 2400   loss: 0.14064911\n",
      "iteration: 1048 / 2400   loss: 0.035338175\n",
      "iteration: 1049 / 2400   loss: 0.34365213\n",
      "iteration: 1050 / 2400   loss: 0.04650056\n",
      "iteration: 1051 / 2400   loss: 0.28738654\n",
      "iteration: 1052 / 2400   loss: 0.12223759\n",
      "iteration: 1053 / 2400   loss: 0.2629845\n",
      "iteration: 1054 / 2400   loss: 0.084234536\n",
      "iteration: 1055 / 2400   loss: 0.20925196\n",
      "iteration: 1056 / 2400   loss: 0.92625886\n",
      "iteration: 1057 / 2400   loss: 0.18877198\n",
      "iteration: 1058 / 2400   loss: 0.23422495\n",
      "iteration: 1059 / 2400   loss: 0.18839619\n",
      "iteration: 1060 / 2400   loss: 0.25524613\n",
      "iteration: 1061 / 2400   loss: 0.45765916\n",
      "iteration: 1062 / 2400   loss: 0.16179383\n",
      "iteration: 1063 / 2400   loss: 0.3069499\n",
      "iteration: 1064 / 2400   loss: 0.17943898\n",
      "iteration: 1065 / 2400   loss: 0.28027925\n",
      "iteration: 1066 / 2400   loss: 0.48194382\n",
      "iteration: 1067 / 2400   loss: 0.09613331\n",
      "iteration: 1068 / 2400   loss: 0.100957654\n",
      "iteration: 1069 / 2400   loss: 0.29339412\n",
      "iteration: 1070 / 2400   loss: 0.26794922\n",
      "iteration: 1071 / 2400   loss: 0.6753328\n",
      "iteration: 1072 / 2400   loss: 0.1597021\n",
      "iteration: 1073 / 2400   loss: 0.1308375\n",
      "iteration: 1074 / 2400   loss: 0.10845184\n",
      "iteration: 1075 / 2400   loss: 0.2939212\n",
      "iteration: 1076 / 2400   loss: 0.049730368\n",
      "iteration: 1077 / 2400   loss: 0.043994874\n",
      "iteration: 1078 / 2400   loss: 0.24364473\n",
      "iteration: 1079 / 2400   loss: 0.069033116\n",
      "iteration: 1080 / 2400   loss: 0.050548878\n",
      "iteration: 1081 / 2400   loss: 0.109445\n",
      "iteration: 1082 / 2400   loss: 0.1336007\n",
      "iteration: 1083 / 2400   loss: 0.16620995\n",
      "iteration: 1084 / 2400   loss: 0.32964435\n",
      "iteration: 1085 / 2400   loss: 0.26520193\n",
      "iteration: 1086 / 2400   loss: 0.041853245\n",
      "iteration: 1087 / 2400   loss: 0.44010064\n",
      "iteration: 1088 / 2400   loss: 0.9360031\n",
      "iteration: 1089 / 2400   loss: 0.40167934\n",
      "iteration: 1090 / 2400   loss: 0.20212807\n",
      "iteration: 1091 / 2400   loss: 0.23945509\n",
      "iteration: 1092 / 2400   loss: 0.21024734\n",
      "iteration: 1093 / 2400   loss: 0.1735308\n",
      "iteration: 1094 / 2400   loss: 0.17128956\n",
      "iteration: 1095 / 2400   loss: 0.47597846\n",
      "iteration: 1096 / 2400   loss: 0.063256286\n",
      "iteration: 1097 / 2400   loss: 0.29697034\n",
      "iteration: 1098 / 2400   loss: 0.15622826\n",
      "iteration: 1099 / 2400   loss: 0.23160024\n",
      "iteration: 1100 / 2400   loss: 0.08320832\n",
      "iteration: 1101 / 2400   loss: 0.7628523\n",
      "iteration: 1102 / 2400   loss: 0.16089767\n",
      "iteration: 1103 / 2400   loss: 0.028855458\n",
      "iteration: 1104 / 2400   loss: 0.32677743\n",
      "iteration: 1105 / 2400   loss: 0.23660144\n",
      "iteration: 1106 / 2400   loss: 0.08656695\n",
      "iteration: 1107 / 2400   loss: 0.11648502\n",
      "iteration: 1108 / 2400   loss: 0.32125825\n",
      "iteration: 1109 / 2400   loss: 0.23968895\n",
      "iteration: 1110 / 2400   loss: 0.27687144\n",
      "iteration: 1111 / 2400   loss: 0.1934989\n",
      "iteration: 1112 / 2400   loss: 0.14582862\n",
      "iteration: 1113 / 2400   loss: 0.24465138\n",
      "iteration: 1114 / 2400   loss: 0.29937413\n",
      "iteration: 1115 / 2400   loss: 0.085762285\n",
      "iteration: 1116 / 2400   loss: 0.17776108\n",
      "iteration: 1117 / 2400   loss: 0.10832585\n",
      "iteration: 1118 / 2400   loss: 0.08611413\n",
      "iteration: 1119 / 2400   loss: 0.23181276\n",
      "iteration: 1120 / 2400   loss: 0.24126123\n",
      "iteration: 1121 / 2400   loss: 0.32374054\n",
      "iteration: 1122 / 2400   loss: 0.044296008\n",
      "iteration: 1123 / 2400   loss: 0.16527903\n",
      "iteration: 1124 / 2400   loss: 0.12640123\n",
      "iteration: 1125 / 2400   loss: 0.065848686\n",
      "iteration: 1126 / 2400   loss: 0.090872526\n",
      "iteration: 1127 / 2400   loss: 0.2149\n",
      "iteration: 1128 / 2400   loss: 0.3599573\n",
      "iteration: 1129 / 2400   loss: 0.04410528\n",
      "iteration: 1130 / 2400   loss: 0.17370148\n",
      "iteration: 1131 / 2400   loss: 0.10403389\n",
      "iteration: 1132 / 2400   loss: 0.102499716\n",
      "iteration: 1133 / 2400   loss: 0.10271531\n",
      "iteration: 1134 / 2400   loss: 0.11797122\n",
      "iteration: 1135 / 2400   loss: 0.4771192\n",
      "iteration: 1136 / 2400   loss: 0.61739767\n",
      "iteration: 1137 / 2400   loss: 0.5421402\n",
      "iteration: 1138 / 2400   loss: 0.058082856\n",
      "iteration: 1139 / 2400   loss: 0.15999131\n",
      "iteration: 1140 / 2400   loss: 0.028611831\n",
      "iteration: 1141 / 2400   loss: 0.16276222\n",
      "iteration: 1142 / 2400   loss: 0.36831105\n",
      "iteration: 1143 / 2400   loss: 0.28749552\n",
      "iteration: 1144 / 2400   loss: 0.4227217\n",
      "iteration: 1145 / 2400   loss: 0.12304044\n",
      "iteration: 1146 / 2400   loss: 0.3217422\n",
      "iteration: 1147 / 2400   loss: 0.63859487\n",
      "iteration: 1148 / 2400   loss: 0.13770433\n",
      "iteration: 1149 / 2400   loss: 0.20096084\n",
      "iteration: 1150 / 2400   loss: 0.16254601\n",
      "iteration: 1151 / 2400   loss: 0.081329815\n",
      "iteration: 1152 / 2400   loss: 0.10862153\n",
      "iteration: 1153 / 2400   loss: 0.14701675\n",
      "iteration: 1154 / 2400   loss: 0.22366966\n",
      "iteration: 1155 / 2400   loss: 0.0979567\n",
      "iteration: 1156 / 2400   loss: 0.11090107\n",
      "iteration: 1157 / 2400   loss: 0.03606429\n",
      "iteration: 1158 / 2400   loss: 0.11328183\n",
      "iteration: 1159 / 2400   loss: 0.15815921\n",
      "iteration: 1160 / 2400   loss: 0.31006408\n",
      "iteration: 1161 / 2400   loss: 0.08769032\n",
      "iteration: 1162 / 2400   loss: 0.07147761\n",
      "iteration: 1163 / 2400   loss: 0.3956045\n",
      "iteration: 1164 / 2400   loss: 0.12784933\n",
      "iteration: 1165 / 2400   loss: 0.4124377\n",
      "iteration: 1166 / 2400   loss: 0.105763815\n",
      "iteration: 1167 / 2400   loss: 0.106892385\n",
      "iteration: 1168 / 2400   loss: 0.42177933\n",
      "iteration: 1169 / 2400   loss: 0.20621082\n",
      "iteration: 1170 / 2400   loss: 0.16909736\n",
      "iteration: 1171 / 2400   loss: 0.019728309\n",
      "iteration: 1172 / 2400   loss: 0.3240609\n",
      "iteration: 1173 / 2400   loss: 0.51025915\n",
      "iteration: 1174 / 2400   loss: 0.16331455\n",
      "iteration: 1175 / 2400   loss: 0.24121337\n",
      "iteration: 1176 / 2400   loss: 0.26524132\n",
      "iteration: 1177 / 2400   loss: 0.11870123\n",
      "iteration: 1178 / 2400   loss: 0.61089826\n",
      "iteration: 1179 / 2400   loss: 0.2586283\n",
      "iteration: 1180 / 2400   loss: 0.2555445\n",
      "iteration: 1181 / 2400   loss: 0.1006637\n",
      "iteration: 1182 / 2400   loss: 0.117241666\n",
      "iteration: 1183 / 2400   loss: 0.28971586\n",
      "iteration: 1184 / 2400   loss: 0.22715889\n",
      "iteration: 1185 / 2400   loss: 0.19547504\n",
      "iteration: 1186 / 2400   loss: 0.21680886\n",
      "iteration: 1187 / 2400   loss: 0.33941525\n",
      "iteration: 1188 / 2400   loss: 0.053661127\n",
      "iteration: 1189 / 2400   loss: 0.6980532\n",
      "iteration: 1190 / 2400   loss: 0.19443898\n",
      "iteration: 1191 / 2400   loss: 0.30303344\n",
      "iteration: 1192 / 2400   loss: 0.12579547\n",
      "iteration: 1193 / 2400   loss: 0.11427708\n",
      "iteration: 1194 / 2400   loss: 0.3000897\n",
      "iteration: 1195 / 2400   loss: 0.25890607\n",
      "iteration: 1196 / 2400   loss: 0.43292424\n",
      "iteration: 1197 / 2400   loss: 0.46720046\n",
      "iteration: 1198 / 2400   loss: 0.27306783\n",
      "iteration: 1199 / 2400   loss: 0.27240947\n",
      "iteration: 1200 / 2400   loss: 0.15634474\n",
      "iteration: 1201 / 2400   loss: 0.23722424\n",
      "iteration: 1202 / 2400   loss: 0.28370243\n",
      "iteration: 1203 / 2400   loss: 0.24012084\n",
      "iteration: 1204 / 2400   loss: 0.15880427\n",
      "iteration: 1205 / 2400   loss: 0.25611073\n",
      "iteration: 1206 / 2400   loss: 0.21265039\n",
      "iteration: 1207 / 2400   loss: 0.25850117\n",
      "iteration: 1208 / 2400   loss: 0.3777216\n",
      "iteration: 1209 / 2400   loss: 0.18097802\n",
      "iteration: 1210 / 2400   loss: 0.12155676\n",
      "iteration: 1211 / 2400   loss: 0.075349055\n",
      "iteration: 1212 / 2400   loss: 0.05701273\n",
      "iteration: 1213 / 2400   loss: 0.1631726\n",
      "iteration: 1214 / 2400   loss: 0.17867444\n",
      "iteration: 1215 / 2400   loss: 0.17363721\n",
      "iteration: 1216 / 2400   loss: 0.034452267\n",
      "iteration: 1217 / 2400   loss: 0.05138901\n",
      "iteration: 1218 / 2400   loss: 0.2050771\n",
      "iteration: 1219 / 2400   loss: 0.28284782\n",
      "iteration: 1220 / 2400   loss: 0.0982807\n",
      "iteration: 1221 / 2400   loss: 0.40840623\n",
      "iteration: 1222 / 2400   loss: 0.108914785\n",
      "iteration: 1223 / 2400   loss: 0.24380842\n",
      "iteration: 1224 / 2400   loss: 0.19648674\n",
      "iteration: 1225 / 2400   loss: 0.25532827\n",
      "iteration: 1226 / 2400   loss: 0.10831452\n",
      "iteration: 1227 / 2400   loss: 0.24534768\n",
      "iteration: 1228 / 2400   loss: 0.16083431\n",
      "iteration: 1229 / 2400   loss: 0.15312018\n",
      "iteration: 1230 / 2400   loss: 0.099994965\n",
      "iteration: 1231 / 2400   loss: 0.18515053\n",
      "iteration: 1232 / 2400   loss: 0.2956612\n",
      "iteration: 1233 / 2400   loss: 0.12069981\n",
      "iteration: 1234 / 2400   loss: 0.19307251\n",
      "iteration: 1235 / 2400   loss: 0.24741909\n",
      "iteration: 1236 / 2400   loss: 0.439226\n",
      "iteration: 1237 / 2400   loss: 0.23397598\n",
      "iteration: 1238 / 2400   loss: 0.068429835\n",
      "iteration: 1239 / 2400   loss: 0.2687236\n",
      "iteration: 1240 / 2400   loss: 0.16440909\n",
      "iteration: 1241 / 2400   loss: 0.14638941\n",
      "iteration: 1242 / 2400   loss: 0.109510005\n",
      "iteration: 1243 / 2400   loss: 0.27492008\n",
      "iteration: 1244 / 2400   loss: 0.049414646\n",
      "iteration: 1245 / 2400   loss: 0.07517812\n",
      "iteration: 1246 / 2400   loss: 0.5145717\n",
      "iteration: 1247 / 2400   loss: 0.17151508\n",
      "iteration: 1248 / 2400   loss: 0.7453216\n",
      "iteration: 1249 / 2400   loss: 0.16462013\n",
      "iteration: 1250 / 2400   loss: 0.13812356\n",
      "iteration: 1251 / 2400   loss: 0.5080055\n",
      "iteration: 1252 / 2400   loss: 0.112382986\n",
      "iteration: 1253 / 2400   loss: 0.44088474\n",
      "iteration: 1254 / 2400   loss: 0.50990325\n",
      "iteration: 1255 / 2400   loss: 0.092370965\n",
      "iteration: 1256 / 2400   loss: 0.19980933\n",
      "iteration: 1257 / 2400   loss: 0.12468155\n",
      "iteration: 1258 / 2400   loss: 0.26452747\n",
      "iteration: 1259 / 2400   loss: 0.075097464\n",
      "iteration: 1260 / 2400   loss: 0.055638008\n",
      "iteration: 1261 / 2400   loss: 0.21276386\n",
      "iteration: 1262 / 2400   loss: 0.053158034\n",
      "iteration: 1263 / 2400   loss: 0.13650161\n",
      "iteration: 1264 / 2400   loss: 0.46441612\n",
      "iteration: 1265 / 2400   loss: 0.28200638\n",
      "iteration: 1266 / 2400   loss: 0.09589229\n",
      "iteration: 1267 / 2400   loss: 0.56683815\n",
      "iteration: 1268 / 2400   loss: 0.111503154\n",
      "iteration: 1269 / 2400   loss: 0.635521\n",
      "iteration: 1270 / 2400   loss: 0.20929718\n",
      "iteration: 1271 / 2400   loss: 0.14603023\n",
      "iteration: 1272 / 2400   loss: 0.21369308\n",
      "iteration: 1273 / 2400   loss: 0.03697264\n",
      "iteration: 1274 / 2400   loss: 0.045660943\n",
      "iteration: 1275 / 2400   loss: 0.23775311\n",
      "iteration: 1276 / 2400   loss: 0.18310253\n",
      "iteration: 1277 / 2400   loss: 0.10555783\n",
      "iteration: 1278 / 2400   loss: 0.2479982\n",
      "iteration: 1279 / 2400   loss: 0.41878656\n",
      "iteration: 1280 / 2400   loss: 0.07666912\n",
      "iteration: 1281 / 2400   loss: 0.30989134\n",
      "iteration: 1282 / 2400   loss: 0.037263166\n",
      "iteration: 1283 / 2400   loss: 0.47760236\n",
      "iteration: 1284 / 2400   loss: 0.06166503\n",
      "iteration: 1285 / 2400   loss: 0.19175257\n",
      "iteration: 1286 / 2400   loss: 0.13140759\n",
      "iteration: 1287 / 2400   loss: 0.20715098\n",
      "iteration: 1288 / 2400   loss: 0.2865877\n",
      "iteration: 1289 / 2400   loss: 0.31686887\n",
      "iteration: 1290 / 2400   loss: 0.15055527\n",
      "iteration: 1291 / 2400   loss: 0.497165\n",
      "iteration: 1292 / 2400   loss: 0.42777398\n",
      "iteration: 1293 / 2400   loss: 0.098308936\n",
      "iteration: 1294 / 2400   loss: 0.19226238\n",
      "iteration: 1295 / 2400   loss: 0.28930566\n",
      "iteration: 1296 / 2400   loss: 0.23726006\n",
      "iteration: 1297 / 2400   loss: 0.4286976\n",
      "iteration: 1298 / 2400   loss: 0.27214852\n",
      "iteration: 1299 / 2400   loss: 0.21473877\n",
      "iteration: 1300 / 2400   loss: 0.2952509\n",
      "iteration: 1301 / 2400   loss: 0.24048644\n",
      "iteration: 1302 / 2400   loss: 0.0462782\n",
      "iteration: 1303 / 2400   loss: 0.23222652\n",
      "iteration: 1304 / 2400   loss: 0.021757431\n",
      "iteration: 1305 / 2400   loss: 0.15353149\n",
      "iteration: 1306 / 2400   loss: 0.26873124\n",
      "iteration: 1307 / 2400   loss: 0.18144108\n",
      "iteration: 1308 / 2400   loss: 0.07103751\n",
      "iteration: 1309 / 2400   loss: 0.12407354\n",
      "iteration: 1310 / 2400   loss: 0.45147777\n",
      "iteration: 1311 / 2400   loss: 0.07744493\n",
      "iteration: 1312 / 2400   loss: 0.21113725\n",
      "iteration: 1313 / 2400   loss: 0.3525175\n",
      "iteration: 1314 / 2400   loss: 0.10357598\n",
      "iteration: 1315 / 2400   loss: 0.109050855\n",
      "iteration: 1316 / 2400   loss: 0.20372301\n",
      "iteration: 1317 / 2400   loss: 0.22276372\n",
      "iteration: 1318 / 2400   loss: 0.074957736\n",
      "iteration: 1319 / 2400   loss: 0.023956928\n",
      "iteration: 1320 / 2400   loss: 0.026404552\n",
      "iteration: 1321 / 2400   loss: 0.11811781\n",
      "iteration: 1322 / 2400   loss: 0.06819994\n",
      "iteration: 1323 / 2400   loss: 0.13878624\n",
      "iteration: 1324 / 2400   loss: 0.05269328\n",
      "iteration: 1325 / 2400   loss: 0.07431511\n",
      "iteration: 1326 / 2400   loss: 0.2670226\n",
      "iteration: 1327 / 2400   loss: 0.08197727\n",
      "iteration: 1328 / 2400   loss: 0.17816952\n",
      "iteration: 1329 / 2400   loss: 0.1501637\n",
      "iteration: 1330 / 2400   loss: 0.32507524\n",
      "iteration: 1331 / 2400   loss: 0.06841101\n",
      "iteration: 1332 / 2400   loss: 0.07800248\n",
      "iteration: 1333 / 2400   loss: 0.18572418\n",
      "iteration: 1334 / 2400   loss: 0.11557159\n",
      "iteration: 1335 / 2400   loss: 0.16014698\n",
      "iteration: 1336 / 2400   loss: 0.5814605\n",
      "iteration: 1337 / 2400   loss: 0.1514133\n",
      "iteration: 1338 / 2400   loss: 0.12181385\n",
      "iteration: 1339 / 2400   loss: 0.05998826\n",
      "iteration: 1340 / 2400   loss: 0.09309824\n",
      "iteration: 1341 / 2400   loss: 0.17756824\n",
      "iteration: 1342 / 2400   loss: 0.05254855\n",
      "iteration: 1343 / 2400   loss: 0.35175315\n",
      "iteration: 1344 / 2400   loss: 0.12336989\n",
      "iteration: 1345 / 2400   loss: 0.100455925\n",
      "iteration: 1346 / 2400   loss: 0.06660877\n",
      "iteration: 1347 / 2400   loss: 0.10667934\n",
      "iteration: 1348 / 2400   loss: 0.114190675\n",
      "iteration: 1349 / 2400   loss: 0.032824785\n",
      "iteration: 1350 / 2400   loss: 0.07032595\n",
      "iteration: 1351 / 2400   loss: 0.34225297\n",
      "iteration: 1352 / 2400   loss: 0.1479134\n",
      "iteration: 1353 / 2400   loss: 0.06668065\n",
      "iteration: 1354 / 2400   loss: 0.094979696\n",
      "iteration: 1355 / 2400   loss: 0.05945938\n",
      "iteration: 1356 / 2400   loss: 0.050106525\n",
      "iteration: 1357 / 2400   loss: 0.09868178\n",
      "iteration: 1358 / 2400   loss: 0.04645932\n",
      "iteration: 1359 / 2400   loss: 0.056168623\n",
      "iteration: 1360 / 2400   loss: 0.21597919\n",
      "iteration: 1361 / 2400   loss: 0.2503331\n",
      "iteration: 1362 / 2400   loss: 0.1413508\n",
      "iteration: 1363 / 2400   loss: 0.21610525\n",
      "iteration: 1364 / 2400   loss: 0.070545435\n",
      "iteration: 1365 / 2400   loss: 0.30970925\n",
      "iteration: 1366 / 2400   loss: 0.092398636\n",
      "iteration: 1367 / 2400   loss: 0.08134692\n",
      "iteration: 1368 / 2400   loss: 0.0647801\n",
      "iteration: 1369 / 2400   loss: 0.2202855\n",
      "iteration: 1370 / 2400   loss: 0.1410752\n",
      "iteration: 1371 / 2400   loss: 0.13293199\n",
      "iteration: 1372 / 2400   loss: 0.04539178\n",
      "iteration: 1373 / 2400   loss: 0.1976982\n",
      "iteration: 1374 / 2400   loss: 0.20043957\n",
      "iteration: 1375 / 2400   loss: 0.1973868\n",
      "iteration: 1376 / 2400   loss: 0.059630662\n",
      "iteration: 1377 / 2400   loss: 0.37961915\n",
      "iteration: 1378 / 2400   loss: 0.16054161\n",
      "iteration: 1379 / 2400   loss: 0.040484067\n",
      "iteration: 1380 / 2400   loss: 0.21872368\n",
      "iteration: 1381 / 2400   loss: 0.073534526\n",
      "iteration: 1382 / 2400   loss: 0.1381948\n",
      "iteration: 1383 / 2400   loss: 0.24962723\n",
      "iteration: 1384 / 2400   loss: 0.16302992\n",
      "iteration: 1385 / 2400   loss: 0.083892405\n",
      "iteration: 1386 / 2400   loss: 0.21585919\n",
      "iteration: 1387 / 2400   loss: 0.52892834\n",
      "iteration: 1388 / 2400   loss: 0.1979125\n",
      "iteration: 1389 / 2400   loss: 0.4633025\n",
      "iteration: 1390 / 2400   loss: 0.11613417\n",
      "iteration: 1391 / 2400   loss: 0.33111176\n",
      "iteration: 1392 / 2400   loss: 0.09450027\n",
      "iteration: 1393 / 2400   loss: 0.46803612\n",
      "iteration: 1394 / 2400   loss: 0.5432254\n",
      "iteration: 1395 / 2400   loss: 0.6923231\n",
      "iteration: 1396 / 2400   loss: 0.29828894\n",
      "iteration: 1397 / 2400   loss: 0.32017028\n",
      "iteration: 1398 / 2400   loss: 0.092917874\n",
      "iteration: 1399 / 2400   loss: 0.07178815\n",
      "iteration: 1400 / 2400   loss: 0.08318153\n",
      "iteration: 1401 / 2400   loss: 0.09657574\n",
      "iteration: 1402 / 2400   loss: 0.02841303\n",
      "iteration: 1403 / 2400   loss: 0.26918125\n",
      "iteration: 1404 / 2400   loss: 0.12858613\n",
      "iteration: 1405 / 2400   loss: 0.053375177\n",
      "iteration: 1406 / 2400   loss: 0.15914457\n",
      "iteration: 1407 / 2400   loss: 0.111777276\n",
      "iteration: 1408 / 2400   loss: 0.16486439\n",
      "iteration: 1409 / 2400   loss: 0.17124051\n",
      "iteration: 1410 / 2400   loss: 0.73333204\n",
      "iteration: 1411 / 2400   loss: 0.07348697\n",
      "iteration: 1412 / 2400   loss: 0.09270716\n",
      "iteration: 1413 / 2400   loss: 0.27699065\n",
      "iteration: 1414 / 2400   loss: 0.14250074\n",
      "iteration: 1415 / 2400   loss: 0.09112773\n",
      "iteration: 1416 / 2400   loss: 0.06379483\n",
      "iteration: 1417 / 2400   loss: 0.29795521\n",
      "iteration: 1418 / 2400   loss: 0.052354556\n",
      "iteration: 1419 / 2400   loss: 0.18929435\n",
      "iteration: 1420 / 2400   loss: 0.12760738\n",
      "iteration: 1421 / 2400   loss: 0.06794811\n",
      "iteration: 1422 / 2400   loss: 0.11035262\n",
      "iteration: 1423 / 2400   loss: 0.041231908\n",
      "iteration: 1424 / 2400   loss: 0.072099514\n",
      "iteration: 1425 / 2400   loss: 0.40410507\n",
      "iteration: 1426 / 2400   loss: 0.23267986\n",
      "iteration: 1427 / 2400   loss: 0.11159627\n",
      "iteration: 1428 / 2400   loss: 0.27166492\n",
      "iteration: 1429 / 2400   loss: 0.117971\n",
      "iteration: 1430 / 2400   loss: 0.17232674\n",
      "iteration: 1431 / 2400   loss: 0.23148866\n",
      "iteration: 1432 / 2400   loss: 0.06866176\n",
      "iteration: 1433 / 2400   loss: 0.047025923\n",
      "iteration: 1434 / 2400   loss: 0.07863628\n",
      "iteration: 1435 / 2400   loss: 0.20596693\n",
      "iteration: 1436 / 2400   loss: 0.27024707\n",
      "iteration: 1437 / 2400   loss: 0.429756\n",
      "iteration: 1438 / 2400   loss: 0.62129956\n",
      "iteration: 1439 / 2400   loss: 0.4941481\n",
      "iteration: 1440 / 2400   loss: 0.20346981\n",
      "iteration: 1441 / 2400   loss: 0.22247158\n",
      "iteration: 1442 / 2400   loss: 0.21346478\n",
      "iteration: 1443 / 2400   loss: 0.19059615\n",
      "iteration: 1444 / 2400   loss: 0.19617382\n",
      "iteration: 1445 / 2400   loss: 0.5121986\n",
      "iteration: 1446 / 2400   loss: 0.124571756\n",
      "iteration: 1447 / 2400   loss: 0.17756607\n",
      "iteration: 1448 / 2400   loss: 0.08633301\n",
      "iteration: 1449 / 2400   loss: 0.47408867\n",
      "iteration: 1450 / 2400   loss: 0.44786763\n",
      "iteration: 1451 / 2400   loss: 0.04992345\n",
      "iteration: 1452 / 2400   loss: 0.09521894\n",
      "iteration: 1453 / 2400   loss: 0.09088071\n",
      "iteration: 1454 / 2400   loss: 0.1349302\n",
      "iteration: 1455 / 2400   loss: 0.16961621\n",
      "iteration: 1456 / 2400   loss: 0.19804016\n",
      "iteration: 1457 / 2400   loss: 0.14944907\n",
      "iteration: 1458 / 2400   loss: 0.06277337\n",
      "iteration: 1459 / 2400   loss: 0.03805521\n",
      "iteration: 1460 / 2400   loss: 0.34893414\n",
      "iteration: 1461 / 2400   loss: 0.063822396\n",
      "iteration: 1462 / 2400   loss: 0.054576445\n",
      "iteration: 1463 / 2400   loss: 0.032160856\n",
      "iteration: 1464 / 2400   loss: 0.17735367\n",
      "iteration: 1465 / 2400   loss: 0.08308602\n",
      "iteration: 1466 / 2400   loss: 0.055364236\n",
      "iteration: 1467 / 2400   loss: 0.16928735\n",
      "iteration: 1468 / 2400   loss: 0.12196603\n",
      "iteration: 1469 / 2400   loss: 0.13971779\n",
      "iteration: 1470 / 2400   loss: 0.68856484\n",
      "iteration: 1471 / 2400   loss: 0.21608187\n",
      "iteration: 1472 / 2400   loss: 0.052262306\n",
      "iteration: 1473 / 2400   loss: 0.3688961\n",
      "iteration: 1474 / 2400   loss: 0.30972534\n",
      "iteration: 1475 / 2400   loss: 0.27757904\n",
      "iteration: 1476 / 2400   loss: 0.16159432\n",
      "iteration: 1477 / 2400   loss: 0.12181569\n",
      "iteration: 1478 / 2400   loss: 0.2095614\n",
      "iteration: 1479 / 2400   loss: 0.053751335\n",
      "iteration: 1480 / 2400   loss: 0.22495146\n",
      "iteration: 1481 / 2400   loss: 0.03537257\n",
      "iteration: 1482 / 2400   loss: 0.70740944\n",
      "iteration: 1483 / 2400   loss: 0.45707917\n",
      "iteration: 1484 / 2400   loss: 0.10607768\n",
      "iteration: 1485 / 2400   loss: 0.19358452\n",
      "iteration: 1486 / 2400   loss: 0.080936044\n",
      "iteration: 1487 / 2400   loss: 0.21690264\n",
      "iteration: 1488 / 2400   loss: 0.09003506\n",
      "iteration: 1489 / 2400   loss: 0.13776802\n",
      "iteration: 1490 / 2400   loss: 0.048107862\n",
      "iteration: 1491 / 2400   loss: 0.19892806\n",
      "iteration: 1492 / 2400   loss: 0.39450943\n",
      "iteration: 1493 / 2400   loss: 0.13331036\n",
      "iteration: 1494 / 2400   loss: 0.21515657\n",
      "iteration: 1495 / 2400   loss: 0.16611537\n",
      "iteration: 1496 / 2400   loss: 0.11361051\n",
      "iteration: 1497 / 2400   loss: 0.19185291\n",
      "iteration: 1498 / 2400   loss: 0.2540686\n",
      "iteration: 1499 / 2400   loss: 0.3421672\n",
      "iteration: 1500 / 2400   loss: 0.36488748\n",
      "iteration: 1501 / 2400   loss: 0.029537011\n",
      "iteration: 1502 / 2400   loss: 0.1749757\n",
      "iteration: 1503 / 2400   loss: 0.162747\n",
      "iteration: 1504 / 2400   loss: 0.25216734\n",
      "iteration: 1505 / 2400   loss: 0.14800388\n",
      "iteration: 1506 / 2400   loss: 0.07654943\n",
      "iteration: 1507 / 2400   loss: 0.082939364\n",
      "iteration: 1508 / 2400   loss: 0.19207507\n",
      "iteration: 1509 / 2400   loss: 0.50686365\n",
      "iteration: 1510 / 2400   loss: 0.16972762\n",
      "iteration: 1511 / 2400   loss: 0.25874683\n",
      "iteration: 1512 / 2400   loss: 0.04780197\n",
      "iteration: 1513 / 2400   loss: 0.18748881\n",
      "iteration: 1514 / 2400   loss: 0.16480064\n",
      "iteration: 1515 / 2400   loss: 0.0990161\n",
      "iteration: 1516 / 2400   loss: 0.13960262\n",
      "iteration: 1517 / 2400   loss: 0.34319457\n",
      "iteration: 1518 / 2400   loss: 0.12903324\n",
      "iteration: 1519 / 2400   loss: 0.10338609\n",
      "iteration: 1520 / 2400   loss: 0.05017109\n",
      "iteration: 1521 / 2400   loss: 0.2836878\n",
      "iteration: 1522 / 2400   loss: 0.06925754\n",
      "iteration: 1523 / 2400   loss: 0.11175776\n",
      "iteration: 1524 / 2400   loss: 0.19046211\n",
      "iteration: 1525 / 2400   loss: 0.06246288\n",
      "iteration: 1526 / 2400   loss: 0.18839946\n",
      "iteration: 1527 / 2400   loss: 0.26387537\n",
      "iteration: 1528 / 2400   loss: 0.26855862\n",
      "iteration: 1529 / 2400   loss: 0.042854253\n",
      "iteration: 1530 / 2400   loss: 0.08387586\n",
      "iteration: 1531 / 2400   loss: 0.15866263\n",
      "iteration: 1532 / 2400   loss: 0.04862816\n",
      "iteration: 1533 / 2400   loss: 0.078630306\n",
      "iteration: 1534 / 2400   loss: 0.065251544\n",
      "iteration: 1535 / 2400   loss: 0.29730883\n",
      "iteration: 1536 / 2400   loss: 0.46403286\n",
      "iteration: 1537 / 2400   loss: 0.16465603\n",
      "iteration: 1538 / 2400   loss: 0.093574524\n",
      "iteration: 1539 / 2400   loss: 0.3408176\n",
      "iteration: 1540 / 2400   loss: 0.21891476\n",
      "iteration: 1541 / 2400   loss: 0.42224723\n",
      "iteration: 1542 / 2400   loss: 0.19865999\n",
      "iteration: 1543 / 2400   loss: 0.12175506\n",
      "iteration: 1544 / 2400   loss: 0.18672474\n",
      "iteration: 1545 / 2400   loss: 0.27367637\n",
      "iteration: 1546 / 2400   loss: 0.06966862\n",
      "iteration: 1547 / 2400   loss: 0.12024149\n",
      "iteration: 1548 / 2400   loss: 0.07022203\n",
      "iteration: 1549 / 2400   loss: 0.2746334\n",
      "iteration: 1550 / 2400   loss: 0.039176714\n",
      "iteration: 1551 / 2400   loss: 0.4918851\n",
      "iteration: 1552 / 2400   loss: 0.10679354\n",
      "iteration: 1553 / 2400   loss: 0.12323264\n",
      "iteration: 1554 / 2400   loss: 0.08001671\n",
      "iteration: 1555 / 2400   loss: 0.046028186\n",
      "iteration: 1556 / 2400   loss: 0.20351048\n",
      "iteration: 1557 / 2400   loss: 0.26539478\n",
      "iteration: 1558 / 2400   loss: 0.051481344\n",
      "iteration: 1559 / 2400   loss: 0.16901606\n",
      "iteration: 1560 / 2400   loss: 0.21173376\n",
      "iteration: 1561 / 2400   loss: 0.118494056\n",
      "iteration: 1562 / 2400   loss: 0.10321975\n",
      "iteration: 1563 / 2400   loss: 0.063084885\n",
      "iteration: 1564 / 2400   loss: 0.062012814\n",
      "iteration: 1565 / 2400   loss: 0.0534396\n",
      "iteration: 1566 / 2400   loss: 0.10852654\n",
      "iteration: 1567 / 2400   loss: 0.11868088\n",
      "iteration: 1568 / 2400   loss: 0.14431547\n",
      "iteration: 1569 / 2400   loss: 0.20985724\n",
      "iteration: 1570 / 2400   loss: 0.118118875\n",
      "iteration: 1571 / 2400   loss: 0.26333755\n",
      "iteration: 1572 / 2400   loss: 0.18604259\n",
      "iteration: 1573 / 2400   loss: 0.17642033\n",
      "iteration: 1574 / 2400   loss: 0.16652821\n",
      "iteration: 1575 / 2400   loss: 0.2725838\n",
      "iteration: 1576 / 2400   loss: 0.3599178\n",
      "iteration: 1577 / 2400   loss: 0.32043248\n",
      "iteration: 1578 / 2400   loss: 0.20941605\n",
      "iteration: 1579 / 2400   loss: 0.1708496\n",
      "iteration: 1580 / 2400   loss: 0.34692904\n",
      "iteration: 1581 / 2400   loss: 0.0984948\n",
      "iteration: 1582 / 2400   loss: 0.19220637\n",
      "iteration: 1583 / 2400   loss: 0.017224312\n",
      "iteration: 1584 / 2400   loss: 0.08552743\n",
      "iteration: 1585 / 2400   loss: 0.22662458\n",
      "iteration: 1586 / 2400   loss: 0.28300697\n",
      "iteration: 1587 / 2400   loss: 0.18719426\n",
      "iteration: 1588 / 2400   loss: 0.1692021\n",
      "iteration: 1589 / 2400   loss: 0.24049392\n",
      "iteration: 1590 / 2400   loss: 0.26438192\n",
      "iteration: 1591 / 2400   loss: 0.22306377\n",
      "iteration: 1592 / 2400   loss: 0.16850667\n",
      "iteration: 1593 / 2400   loss: 0.24479884\n",
      "iteration: 1594 / 2400   loss: 0.14136736\n",
      "iteration: 1595 / 2400   loss: 0.18369098\n",
      "iteration: 1596 / 2400   loss: 0.15716098\n",
      "iteration: 1597 / 2400   loss: 0.10378326\n",
      "iteration: 1598 / 2400   loss: 0.14534032\n",
      "iteration: 1599 / 2400   loss: 0.12824291\n",
      "iteration: 1600 / 2400   loss: 0.2951988\n",
      "iteration: 1601 / 2400   loss: 0.525984\n",
      "iteration: 1602 / 2400   loss: 0.08336993\n",
      "iteration: 1603 / 2400   loss: 0.123575985\n",
      "iteration: 1604 / 2400   loss: 0.111047134\n",
      "iteration: 1605 / 2400   loss: 0.035154466\n",
      "iteration: 1606 / 2400   loss: 0.36284518\n",
      "iteration: 1607 / 2400   loss: 0.16210207\n",
      "iteration: 1608 / 2400   loss: 0.032012597\n",
      "iteration: 1609 / 2400   loss: 0.051894054\n",
      "iteration: 1610 / 2400   loss: 0.10496515\n",
      "iteration: 1611 / 2400   loss: 0.33311078\n",
      "iteration: 1612 / 2400   loss: 0.35255256\n",
      "iteration: 1613 / 2400   loss: 0.06973841\n",
      "iteration: 1614 / 2400   loss: 0.02543063\n",
      "iteration: 1615 / 2400   loss: 0.076426946\n",
      "iteration: 1616 / 2400   loss: 0.67395\n",
      "iteration: 1617 / 2400   loss: 0.017008515\n",
      "iteration: 1618 / 2400   loss: 0.2079749\n",
      "iteration: 1619 / 2400   loss: 0.2764344\n",
      "iteration: 1620 / 2400   loss: 0.20148633\n",
      "iteration: 1621 / 2400   loss: 0.032005213\n",
      "iteration: 1622 / 2400   loss: 0.33667976\n",
      "iteration: 1623 / 2400   loss: 0.036446113\n",
      "iteration: 1624 / 2400   loss: 0.055864267\n",
      "iteration: 1625 / 2400   loss: 0.050886404\n",
      "iteration: 1626 / 2400   loss: 0.5167803\n",
      "iteration: 1627 / 2400   loss: 0.47406372\n",
      "iteration: 1628 / 2400   loss: 0.07687345\n",
      "iteration: 1629 / 2400   loss: 0.5473509\n",
      "iteration: 1630 / 2400   loss: 0.0075003817\n",
      "iteration: 1631 / 2400   loss: 0.061852656\n",
      "iteration: 1632 / 2400   loss: 0.040512018\n",
      "iteration: 1633 / 2400   loss: 0.0681251\n",
      "iteration: 1634 / 2400   loss: 0.13937192\n",
      "iteration: 1635 / 2400   loss: 0.10312612\n",
      "iteration: 1636 / 2400   loss: 0.16745949\n",
      "iteration: 1637 / 2400   loss: 0.046782512\n",
      "iteration: 1638 / 2400   loss: 0.12382067\n",
      "iteration: 1639 / 2400   loss: 0.112428114\n",
      "iteration: 1640 / 2400   loss: 0.23924305\n",
      "iteration: 1641 / 2400   loss: 0.084713824\n",
      "iteration: 1642 / 2400   loss: 0.026134348\n",
      "iteration: 1643 / 2400   loss: 0.34477714\n",
      "iteration: 1644 / 2400   loss: 0.22647409\n",
      "iteration: 1645 / 2400   loss: 0.2724147\n",
      "iteration: 1646 / 2400   loss: 0.07288841\n",
      "iteration: 1647 / 2400   loss: 0.101757586\n",
      "iteration: 1648 / 2400   loss: 0.17910077\n",
      "iteration: 1649 / 2400   loss: 0.09446322\n",
      "iteration: 1650 / 2400   loss: 0.19332248\n",
      "iteration: 1651 / 2400   loss: 0.40777993\n",
      "iteration: 1652 / 2400   loss: 0.4149637\n",
      "iteration: 1653 / 2400   loss: 0.083327316\n",
      "iteration: 1654 / 2400   loss: 0.22529441\n",
      "iteration: 1655 / 2400   loss: 0.23313999\n",
      "iteration: 1656 / 2400   loss: 0.5186594\n",
      "iteration: 1657 / 2400   loss: 0.14903302\n",
      "iteration: 1658 / 2400   loss: 0.17803231\n",
      "iteration: 1659 / 2400   loss: 0.54207176\n",
      "iteration: 1660 / 2400   loss: 0.3037525\n",
      "iteration: 1661 / 2400   loss: 0.36402678\n",
      "iteration: 1662 / 2400   loss: 0.45334616\n",
      "iteration: 1663 / 2400   loss: 0.12132236\n",
      "iteration: 1664 / 2400   loss: 0.1888118\n",
      "iteration: 1665 / 2400   loss: 0.4476593\n",
      "iteration: 1666 / 2400   loss: 0.13614433\n",
      "iteration: 1667 / 2400   loss: 0.05572832\n",
      "iteration: 1668 / 2400   loss: 0.11515073\n",
      "iteration: 1669 / 2400   loss: 0.12350546\n",
      "iteration: 1670 / 2400   loss: 0.060237132\n",
      "iteration: 1671 / 2400   loss: 0.19713101\n",
      "iteration: 1672 / 2400   loss: 0.082395695\n",
      "iteration: 1673 / 2400   loss: 0.2640926\n",
      "iteration: 1674 / 2400   loss: 0.26531884\n",
      "iteration: 1675 / 2400   loss: 0.06788103\n",
      "iteration: 1676 / 2400   loss: 0.0877122\n",
      "iteration: 1677 / 2400   loss: 0.5016428\n",
      "iteration: 1678 / 2400   loss: 0.038217947\n",
      "iteration: 1679 / 2400   loss: 0.17091234\n",
      "iteration: 1680 / 2400   loss: 0.29188415\n",
      "iteration: 1681 / 2400   loss: 0.1623222\n",
      "iteration: 1682 / 2400   loss: 0.28817222\n",
      "iteration: 1683 / 2400   loss: 0.1374275\n",
      "iteration: 1684 / 2400   loss: 0.072806224\n",
      "iteration: 1685 / 2400   loss: 0.19615167\n",
      "iteration: 1686 / 2400   loss: 0.25238347\n",
      "iteration: 1687 / 2400   loss: 0.08356302\n",
      "iteration: 1688 / 2400   loss: 0.049164295\n",
      "iteration: 1689 / 2400   loss: 0.19195014\n",
      "iteration: 1690 / 2400   loss: 0.04307459\n",
      "iteration: 1691 / 2400   loss: 0.21546471\n",
      "iteration: 1692 / 2400   loss: 0.10390126\n",
      "iteration: 1693 / 2400   loss: 0.3825058\n",
      "iteration: 1694 / 2400   loss: 0.11526914\n",
      "iteration: 1695 / 2400   loss: 0.1386363\n",
      "iteration: 1696 / 2400   loss: 0.08849163\n",
      "iteration: 1697 / 2400   loss: 0.10640125\n",
      "iteration: 1698 / 2400   loss: 0.45873433\n",
      "iteration: 1699 / 2400   loss: 0.20963623\n",
      "iteration: 1700 / 2400   loss: 0.22200856\n",
      "iteration: 1701 / 2400   loss: 0.105403416\n",
      "iteration: 1702 / 2400   loss: 0.16600305\n",
      "iteration: 1703 / 2400   loss: 0.17159718\n",
      "iteration: 1704 / 2400   loss: 0.052573606\n",
      "iteration: 1705 / 2400   loss: 0.095701315\n",
      "iteration: 1706 / 2400   loss: 0.19480896\n",
      "iteration: 1707 / 2400   loss: 0.09459079\n",
      "iteration: 1708 / 2400   loss: 0.26311243\n",
      "iteration: 1709 / 2400   loss: 0.19177225\n",
      "iteration: 1710 / 2400   loss: 0.15913908\n",
      "iteration: 1711 / 2400   loss: 0.13842063\n",
      "iteration: 1712 / 2400   loss: 0.11738916\n",
      "iteration: 1713 / 2400   loss: 0.15039153\n",
      "iteration: 1714 / 2400   loss: 0.18454622\n",
      "iteration: 1715 / 2400   loss: 0.34981215\n",
      "iteration: 1716 / 2400   loss: 0.12889431\n",
      "iteration: 1717 / 2400   loss: 0.20719618\n",
      "iteration: 1718 / 2400   loss: 0.4894514\n",
      "iteration: 1719 / 2400   loss: 0.1660896\n",
      "iteration: 1720 / 2400   loss: 0.34200135\n",
      "iteration: 1721 / 2400   loss: 0.2691465\n",
      "iteration: 1722 / 2400   loss: 0.325635\n",
      "iteration: 1723 / 2400   loss: 0.26106593\n",
      "iteration: 1724 / 2400   loss: 0.15318036\n",
      "iteration: 1725 / 2400   loss: 0.37496543\n",
      "iteration: 1726 / 2400   loss: 0.06332381\n",
      "iteration: 1727 / 2400   loss: 0.10400704\n",
      "iteration: 1728 / 2400   loss: 0.12606399\n",
      "iteration: 1729 / 2400   loss: 0.2954906\n",
      "iteration: 1730 / 2400   loss: 0.49811783\n",
      "iteration: 1731 / 2400   loss: 0.07298196\n",
      "iteration: 1732 / 2400   loss: 0.17139836\n",
      "iteration: 1733 / 2400   loss: 0.04915206\n",
      "iteration: 1734 / 2400   loss: 0.17889172\n",
      "iteration: 1735 / 2400   loss: 0.038500234\n",
      "iteration: 1736 / 2400   loss: 0.13558124\n",
      "iteration: 1737 / 2400   loss: 0.19865984\n",
      "iteration: 1738 / 2400   loss: 0.07053936\n",
      "iteration: 1739 / 2400   loss: 0.27466434\n",
      "iteration: 1740 / 2400   loss: 0.105593376\n",
      "iteration: 1741 / 2400   loss: 0.21081345\n",
      "iteration: 1742 / 2400   loss: 0.65887296\n",
      "iteration: 1743 / 2400   loss: 0.29899\n",
      "iteration: 1744 / 2400   loss: 0.24962185\n",
      "iteration: 1745 / 2400   loss: 0.03809557\n",
      "iteration: 1746 / 2400   loss: 0.015799733\n",
      "iteration: 1747 / 2400   loss: 0.3466696\n",
      "iteration: 1748 / 2400   loss: 0.09979814\n",
      "iteration: 1749 / 2400   loss: 0.108910844\n",
      "iteration: 1750 / 2400   loss: 0.16754523\n",
      "iteration: 1751 / 2400   loss: 0.16590236\n",
      "iteration: 1752 / 2400   loss: 0.54286456\n",
      "iteration: 1753 / 2400   loss: 0.093967445\n",
      "iteration: 1754 / 2400   loss: 0.09739945\n",
      "iteration: 1755 / 2400   loss: 0.42598137\n",
      "iteration: 1756 / 2400   loss: 0.022876587\n",
      "iteration: 1757 / 2400   loss: 0.042229965\n",
      "iteration: 1758 / 2400   loss: 0.10933354\n",
      "iteration: 1759 / 2400   loss: 0.46968052\n",
      "iteration: 1760 / 2400   loss: 0.14029345\n",
      "iteration: 1761 / 2400   loss: 0.20702878\n",
      "iteration: 1762 / 2400   loss: 0.24388558\n",
      "iteration: 1763 / 2400   loss: 0.026090927\n",
      "iteration: 1764 / 2400   loss: 0.1777747\n",
      "iteration: 1765 / 2400   loss: 0.16824275\n",
      "iteration: 1766 / 2400   loss: 0.23925187\n",
      "iteration: 1767 / 2400   loss: 0.35515705\n",
      "iteration: 1768 / 2400   loss: 0.13028945\n",
      "iteration: 1769 / 2400   loss: 0.042102348\n",
      "iteration: 1770 / 2400   loss: 0.19208077\n",
      "iteration: 1771 / 2400   loss: 0.19377556\n",
      "iteration: 1772 / 2400   loss: 0.24625792\n",
      "iteration: 1773 / 2400   loss: 0.0745865\n",
      "iteration: 1774 / 2400   loss: 0.34100425\n",
      "iteration: 1775 / 2400   loss: 0.13415402\n",
      "iteration: 1776 / 2400   loss: 0.22055878\n",
      "iteration: 1777 / 2400   loss: 0.17460464\n",
      "iteration: 1778 / 2400   loss: 0.14630967\n",
      "iteration: 1779 / 2400   loss: 0.050765667\n",
      "iteration: 1780 / 2400   loss: 0.1930063\n",
      "iteration: 1781 / 2400   loss: 0.043131933\n",
      "iteration: 1782 / 2400   loss: 0.168921\n",
      "iteration: 1783 / 2400   loss: 0.08311827\n",
      "iteration: 1784 / 2400   loss: 0.10671342\n",
      "iteration: 1785 / 2400   loss: 0.05848549\n",
      "iteration: 1786 / 2400   loss: 0.1274122\n",
      "iteration: 1787 / 2400   loss: 0.08893606\n",
      "iteration: 1788 / 2400   loss: 0.04970973\n",
      "iteration: 1789 / 2400   loss: 0.11889325\n",
      "iteration: 1790 / 2400   loss: 0.5184614\n",
      "iteration: 1791 / 2400   loss: 0.19712923\n",
      "iteration: 1792 / 2400   loss: 0.13287172\n",
      "iteration: 1793 / 2400   loss: 0.15541266\n",
      "iteration: 1794 / 2400   loss: 0.18164639\n",
      "iteration: 1795 / 2400   loss: 0.13471417\n",
      "iteration: 1796 / 2400   loss: 0.30999652\n",
      "iteration: 1797 / 2400   loss: 0.037388727\n",
      "iteration: 1798 / 2400   loss: 0.35751277\n",
      "iteration: 1799 / 2400   loss: 0.060989905\n",
      "iteration: 1800 / 2400   loss: 0.08910325\n",
      "iteration: 1801 / 2400   loss: 0.031153727\n",
      "iteration: 1802 / 2400   loss: 0.65160006\n",
      "iteration: 1803 / 2400   loss: 0.18307811\n",
      "iteration: 1804 / 2400   loss: 0.14424007\n",
      "iteration: 1805 / 2400   loss: 0.037913237\n",
      "iteration: 1806 / 2400   loss: 0.053358383\n",
      "iteration: 1807 / 2400   loss: 0.041353732\n",
      "iteration: 1808 / 2400   loss: 0.2684758\n",
      "iteration: 1809 / 2400   loss: 0.022660542\n",
      "iteration: 1810 / 2400   loss: 0.066979945\n",
      "iteration: 1811 / 2400   loss: 0.08497883\n",
      "iteration: 1812 / 2400   loss: 0.10031155\n",
      "iteration: 1813 / 2400   loss: 0.19589376\n",
      "iteration: 1814 / 2400   loss: 0.10441604\n",
      "iteration: 1815 / 2400   loss: 0.16116528\n",
      "iteration: 1816 / 2400   loss: 0.050756093\n",
      "iteration: 1817 / 2400   loss: 0.19759281\n",
      "iteration: 1818 / 2400   loss: 0.27677166\n",
      "iteration: 1819 / 2400   loss: 0.38331756\n",
      "iteration: 1820 / 2400   loss: 0.1426696\n",
      "iteration: 1821 / 2400   loss: 0.28983578\n",
      "iteration: 1822 / 2400   loss: 0.2988646\n",
      "iteration: 1823 / 2400   loss: 0.08225016\n",
      "iteration: 1824 / 2400   loss: 0.17132074\n",
      "iteration: 1825 / 2400   loss: 0.2743367\n",
      "iteration: 1826 / 2400   loss: 0.019627037\n",
      "iteration: 1827 / 2400   loss: 0.18781409\n",
      "iteration: 1828 / 2400   loss: 0.10435958\n",
      "iteration: 1829 / 2400   loss: 0.10343269\n",
      "iteration: 1830 / 2400   loss: 0.29438186\n",
      "iteration: 1831 / 2400   loss: 0.29528338\n",
      "iteration: 1832 / 2400   loss: 0.41687593\n",
      "iteration: 1833 / 2400   loss: 0.33090717\n",
      "iteration: 1834 / 2400   loss: 0.09090828\n",
      "iteration: 1835 / 2400   loss: 0.39550722\n",
      "iteration: 1836 / 2400   loss: 0.19364093\n",
      "iteration: 1837 / 2400   loss: 0.6073387\n",
      "iteration: 1838 / 2400   loss: 0.13897161\n",
      "iteration: 1839 / 2400   loss: 0.25254193\n",
      "iteration: 1840 / 2400   loss: 0.14680625\n",
      "iteration: 1841 / 2400   loss: 0.4114511\n",
      "iteration: 1842 / 2400   loss: 0.14815372\n",
      "iteration: 1843 / 2400   loss: 0.35227475\n",
      "iteration: 1844 / 2400   loss: 0.21940278\n",
      "iteration: 1845 / 2400   loss: 0.27234265\n",
      "iteration: 1846 / 2400   loss: 0.2501291\n",
      "iteration: 1847 / 2400   loss: 0.28133166\n",
      "iteration: 1848 / 2400   loss: 0.3426677\n",
      "iteration: 1849 / 2400   loss: 0.035292845\n",
      "iteration: 1850 / 2400   loss: 0.4830416\n",
      "iteration: 1851 / 2400   loss: 0.1871236\n",
      "iteration: 1852 / 2400   loss: 0.36165187\n",
      "iteration: 1853 / 2400   loss: 0.41120628\n",
      "iteration: 1854 / 2400   loss: 0.038524132\n",
      "iteration: 1855 / 2400   loss: 0.5328164\n",
      "iteration: 1856 / 2400   loss: 0.258619\n",
      "iteration: 1857 / 2400   loss: 0.5291618\n",
      "iteration: 1858 / 2400   loss: 0.3941065\n",
      "iteration: 1859 / 2400   loss: 0.12853354\n",
      "iteration: 1860 / 2400   loss: 0.12410535\n",
      "iteration: 1861 / 2400   loss: 0.14843799\n",
      "iteration: 1862 / 2400   loss: 0.055921745\n",
      "iteration: 1863 / 2400   loss: 0.08555559\n",
      "iteration: 1864 / 2400   loss: 0.09492251\n",
      "iteration: 1865 / 2400   loss: 0.044328574\n",
      "iteration: 1866 / 2400   loss: 0.016891155\n",
      "iteration: 1867 / 2400   loss: 0.022825489\n",
      "iteration: 1868 / 2400   loss: 0.2394049\n",
      "iteration: 1869 / 2400   loss: 0.344282\n",
      "iteration: 1870 / 2400   loss: 0.28818196\n",
      "iteration: 1871 / 2400   loss: 0.04787483\n",
      "iteration: 1872 / 2400   loss: 0.21157557\n",
      "iteration: 1873 / 2400   loss: 0.17767066\n",
      "iteration: 1874 / 2400   loss: 0.2355918\n",
      "iteration: 1875 / 2400   loss: 0.14523375\n",
      "iteration: 1876 / 2400   loss: 0.121112846\n",
      "iteration: 1877 / 2400   loss: 0.11053229\n",
      "iteration: 1878 / 2400   loss: 0.037404183\n",
      "iteration: 1879 / 2400   loss: 0.15647627\n",
      "iteration: 1880 / 2400   loss: 0.09530494\n",
      "iteration: 1881 / 2400   loss: 0.28514647\n",
      "iteration: 1882 / 2400   loss: 0.52663857\n",
      "iteration: 1883 / 2400   loss: 0.038052198\n",
      "iteration: 1884 / 2400   loss: 0.18702032\n",
      "iteration: 1885 / 2400   loss: 0.034561157\n",
      "iteration: 1886 / 2400   loss: 0.0730037\n",
      "iteration: 1887 / 2400   loss: 0.21560559\n",
      "iteration: 1888 / 2400   loss: 0.056949187\n",
      "iteration: 1889 / 2400   loss: 0.105956174\n",
      "iteration: 1890 / 2400   loss: 0.71107\n",
      "iteration: 1891 / 2400   loss: 0.34293512\n",
      "iteration: 1892 / 2400   loss: 0.35654068\n",
      "iteration: 1893 / 2400   loss: 0.17302112\n",
      "iteration: 1894 / 2400   loss: 0.17056853\n",
      "iteration: 1895 / 2400   loss: 0.14260764\n",
      "iteration: 1896 / 2400   loss: 0.19478412\n",
      "iteration: 1897 / 2400   loss: 0.3419525\n",
      "iteration: 1898 / 2400   loss: 0.45223314\n",
      "iteration: 1899 / 2400   loss: 0.19737346\n",
      "iteration: 1900 / 2400   loss: 0.35489044\n",
      "iteration: 1901 / 2400   loss: 0.35040298\n",
      "iteration: 1902 / 2400   loss: 0.12742504\n",
      "iteration: 1903 / 2400   loss: 0.17209372\n",
      "iteration: 1904 / 2400   loss: 0.5627554\n",
      "iteration: 1905 / 2400   loss: 0.32725915\n",
      "iteration: 1906 / 2400   loss: 0.11076071\n",
      "iteration: 1907 / 2400   loss: 0.17973322\n",
      "iteration: 1908 / 2400   loss: 0.41325462\n",
      "iteration: 1909 / 2400   loss: 0.16695583\n",
      "iteration: 1910 / 2400   loss: 0.12565087\n",
      "iteration: 1911 / 2400   loss: 0.24728835\n",
      "iteration: 1912 / 2400   loss: 0.029431658\n",
      "iteration: 1913 / 2400   loss: 0.045514498\n",
      "iteration: 1914 / 2400   loss: 0.2579933\n",
      "iteration: 1915 / 2400   loss: 0.1959809\n",
      "iteration: 1916 / 2400   loss: 0.05668707\n",
      "iteration: 1917 / 2400   loss: 0.29690903\n",
      "iteration: 1918 / 2400   loss: 0.50355476\n",
      "iteration: 1919 / 2400   loss: 0.18363293\n",
      "iteration: 1920 / 2400   loss: 0.032293223\n",
      "iteration: 1921 / 2400   loss: 0.28559905\n",
      "iteration: 1922 / 2400   loss: 0.124235764\n",
      "iteration: 1923 / 2400   loss: 0.16377962\n",
      "iteration: 1924 / 2400   loss: 0.08380847\n",
      "iteration: 1925 / 2400   loss: 0.114709355\n",
      "iteration: 1926 / 2400   loss: 0.032612886\n",
      "iteration: 1927 / 2400   loss: 0.19956426\n",
      "iteration: 1928 / 2400   loss: 0.038991224\n",
      "iteration: 1929 / 2400   loss: 0.20694888\n",
      "iteration: 1930 / 2400   loss: 0.09984888\n",
      "iteration: 1931 / 2400   loss: 0.5995083\n",
      "iteration: 1932 / 2400   loss: 0.18338028\n",
      "iteration: 1933 / 2400   loss: 0.27400258\n",
      "iteration: 1934 / 2400   loss: 0.16785687\n",
      "iteration: 1935 / 2400   loss: 0.24032436\n",
      "iteration: 1936 / 2400   loss: 0.3623604\n",
      "iteration: 1937 / 2400   loss: 0.039462928\n",
      "iteration: 1938 / 2400   loss: 0.045153636\n",
      "iteration: 1939 / 2400   loss: 0.1165359\n",
      "iteration: 1940 / 2400   loss: 0.10242117\n",
      "iteration: 1941 / 2400   loss: 0.23521969\n",
      "iteration: 1942 / 2400   loss: 0.1824462\n",
      "iteration: 1943 / 2400   loss: 0.091362484\n",
      "iteration: 1944 / 2400   loss: 0.23189627\n",
      "iteration: 1945 / 2400   loss: 0.11202938\n",
      "iteration: 1946 / 2400   loss: 0.11928885\n",
      "iteration: 1947 / 2400   loss: 0.05558975\n",
      "iteration: 1948 / 2400   loss: 0.35659507\n",
      "iteration: 1949 / 2400   loss: 0.19132227\n",
      "iteration: 1950 / 2400   loss: 0.0092020035\n",
      "iteration: 1951 / 2400   loss: 0.112674065\n",
      "iteration: 1952 / 2400   loss: 0.05651885\n",
      "iteration: 1953 / 2400   loss: 0.26254165\n",
      "iteration: 1954 / 2400   loss: 0.033653174\n",
      "iteration: 1955 / 2400   loss: 0.18123424\n",
      "iteration: 1956 / 2400   loss: 0.12162856\n",
      "iteration: 1957 / 2400   loss: 0.12539192\n",
      "iteration: 1958 / 2400   loss: 0.34006783\n",
      "iteration: 1959 / 2400   loss: 0.40681762\n",
      "iteration: 1960 / 2400   loss: 0.43021816\n",
      "iteration: 1961 / 2400   loss: 0.3384724\n",
      "iteration: 1962 / 2400   loss: 0.16254458\n",
      "iteration: 1963 / 2400   loss: 0.32972705\n",
      "iteration: 1964 / 2400   loss: 0.14081842\n",
      "iteration: 1965 / 2400   loss: 0.2722763\n",
      "iteration: 1966 / 2400   loss: 0.54714566\n",
      "iteration: 1967 / 2400   loss: 0.40281403\n",
      "iteration: 1968 / 2400   loss: 0.3681465\n",
      "iteration: 1969 / 2400   loss: 0.3240677\n",
      "iteration: 1970 / 2400   loss: 0.3511385\n",
      "iteration: 1971 / 2400   loss: 0.10296112\n",
      "iteration: 1972 / 2400   loss: 0.34198326\n",
      "iteration: 1973 / 2400   loss: 0.054844476\n",
      "iteration: 1974 / 2400   loss: 0.028720837\n",
      "iteration: 1975 / 2400   loss: 0.09932083\n",
      "iteration: 1976 / 2400   loss: 0.07264944\n",
      "iteration: 1977 / 2400   loss: 0.15317202\n",
      "iteration: 1978 / 2400   loss: 0.13523063\n",
      "iteration: 1979 / 2400   loss: 0.3114851\n",
      "iteration: 1980 / 2400   loss: 0.73321617\n",
      "iteration: 1981 / 2400   loss: 0.59674\n",
      "iteration: 1982 / 2400   loss: 0.85883874\n",
      "iteration: 1983 / 2400   loss: 0.36747333\n",
      "iteration: 1984 / 2400   loss: 0.33202207\n",
      "iteration: 1985 / 2400   loss: 0.02359006\n",
      "iteration: 1986 / 2400   loss: 0.16703579\n",
      "iteration: 1987 / 2400   loss: 0.35817\n",
      "iteration: 1988 / 2400   loss: 0.19080341\n",
      "iteration: 1989 / 2400   loss: 0.15845187\n",
      "iteration: 1990 / 2400   loss: 0.12243568\n",
      "iteration: 1991 / 2400   loss: 0.16835064\n",
      "iteration: 1992 / 2400   loss: 0.2535938\n",
      "iteration: 1993 / 2400   loss: 0.0761058\n",
      "iteration: 1994 / 2400   loss: 0.20094135\n",
      "iteration: 1995 / 2400   loss: 0.21648961\n",
      "iteration: 1996 / 2400   loss: 0.5055217\n",
      "iteration: 1997 / 2400   loss: 0.6933082\n",
      "iteration: 1998 / 2400   loss: 0.11078562\n",
      "iteration: 1999 / 2400   loss: 0.17085575\n",
      "iteration: 2000 / 2400   loss: 0.16082744\n",
      "iteration: 2001 / 2400   loss: 0.109843425\n",
      "iteration: 2002 / 2400   loss: 0.19933803\n",
      "iteration: 2003 / 2400   loss: 0.1277644\n",
      "iteration: 2004 / 2400   loss: 0.68164355\n",
      "iteration: 2005 / 2400   loss: 0.29573956\n",
      "iteration: 2006 / 2400   loss: 0.2028322\n",
      "iteration: 2007 / 2400   loss: 0.03528084\n",
      "iteration: 2008 / 2400   loss: 0.107267134\n",
      "iteration: 2009 / 2400   loss: 0.57089496\n",
      "iteration: 2010 / 2400   loss: 0.40281516\n",
      "iteration: 2011 / 2400   loss: 0.026630316\n",
      "iteration: 2012 / 2400   loss: 0.036298636\n",
      "iteration: 2013 / 2400   loss: 0.1585348\n",
      "iteration: 2014 / 2400   loss: 0.122068055\n",
      "iteration: 2015 / 2400   loss: 0.36840934\n",
      "iteration: 2016 / 2400   loss: 0.54307294\n",
      "iteration: 2017 / 2400   loss: 0.21745063\n",
      "iteration: 2018 / 2400   loss: 0.3893284\n",
      "iteration: 2019 / 2400   loss: 0.3747091\n",
      "iteration: 2020 / 2400   loss: 0.08955815\n",
      "iteration: 2021 / 2400   loss: 0.2255822\n",
      "iteration: 2022 / 2400   loss: 0.05532758\n",
      "iteration: 2023 / 2400   loss: 0.46483147\n",
      "iteration: 2024 / 2400   loss: 0.035435658\n",
      "iteration: 2025 / 2400   loss: 0.05919657\n",
      "iteration: 2026 / 2400   loss: 0.568639\n",
      "iteration: 2027 / 2400   loss: 0.04495019\n",
      "iteration: 2028 / 2400   loss: 0.27522328\n",
      "iteration: 2029 / 2400   loss: 0.1917963\n",
      "iteration: 2030 / 2400   loss: 0.13731298\n",
      "iteration: 2031 / 2400   loss: 0.08696669\n",
      "iteration: 2032 / 2400   loss: 0.18272221\n",
      "iteration: 2033 / 2400   loss: 0.29832092\n",
      "iteration: 2034 / 2400   loss: 0.33959112\n",
      "iteration: 2035 / 2400   loss: 0.011422081\n",
      "iteration: 2036 / 2400   loss: 0.3850885\n",
      "iteration: 2037 / 2400   loss: 0.31739146\n",
      "iteration: 2038 / 2400   loss: 0.1763546\n",
      "iteration: 2039 / 2400   loss: 0.08513488\n",
      "iteration: 2040 / 2400   loss: 0.1315364\n",
      "iteration: 2041 / 2400   loss: 0.10329809\n",
      "iteration: 2042 / 2400   loss: 0.051325064\n",
      "iteration: 2043 / 2400   loss: 0.19733444\n",
      "iteration: 2044 / 2400   loss: 0.04805091\n",
      "iteration: 2045 / 2400   loss: 0.1174457\n",
      "iteration: 2046 / 2400   loss: 0.23286644\n",
      "iteration: 2047 / 2400   loss: 0.156523\n",
      "iteration: 2048 / 2400   loss: 0.065340005\n",
      "iteration: 2049 / 2400   loss: 0.099269584\n",
      "iteration: 2050 / 2400   loss: 0.95953476\n",
      "iteration: 2051 / 2400   loss: 0.24418041\n",
      "iteration: 2052 / 2400   loss: 0.13136587\n",
      "iteration: 2053 / 2400   loss: 0.11858863\n",
      "iteration: 2054 / 2400   loss: 0.16244781\n",
      "iteration: 2055 / 2400   loss: 0.08238606\n",
      "iteration: 2056 / 2400   loss: 0.07271902\n",
      "iteration: 2057 / 2400   loss: 0.08762826\n",
      "iteration: 2058 / 2400   loss: 0.23763739\n",
      "iteration: 2059 / 2400   loss: 0.22926077\n",
      "iteration: 2060 / 2400   loss: 0.14229779\n",
      "iteration: 2061 / 2400   loss: 0.16278377\n",
      "iteration: 2062 / 2400   loss: 0.7213915\n",
      "iteration: 2063 / 2400   loss: 0.039674226\n",
      "iteration: 2064 / 2400   loss: 0.06716701\n",
      "iteration: 2065 / 2400   loss: 0.41581872\n",
      "iteration: 2066 / 2400   loss: 0.04395666\n",
      "iteration: 2067 / 2400   loss: 0.058427874\n",
      "iteration: 2068 / 2400   loss: 0.05419362\n",
      "iteration: 2069 / 2400   loss: 0.050587177\n",
      "iteration: 2070 / 2400   loss: 0.07968441\n",
      "iteration: 2071 / 2400   loss: 0.10676295\n",
      "iteration: 2072 / 2400   loss: 0.093447044\n",
      "iteration: 2073 / 2400   loss: 0.056242485\n",
      "iteration: 2074 / 2400   loss: 0.033891458\n",
      "iteration: 2075 / 2400   loss: 0.21899474\n",
      "iteration: 2076 / 2400   loss: 0.021312123\n",
      "iteration: 2077 / 2400   loss: 0.055013314\n",
      "iteration: 2078 / 2400   loss: 0.23021288\n",
      "iteration: 2079 / 2400   loss: 0.13727817\n",
      "iteration: 2080 / 2400   loss: 0.20008022\n",
      "iteration: 2081 / 2400   loss: 0.14900315\n",
      "iteration: 2082 / 2400   loss: 0.14102252\n",
      "iteration: 2083 / 2400   loss: 0.120567754\n",
      "iteration: 2084 / 2400   loss: 0.3918302\n",
      "iteration: 2085 / 2400   loss: 0.20075847\n",
      "iteration: 2086 / 2400   loss: 0.47129807\n",
      "iteration: 2087 / 2400   loss: 0.47325638\n",
      "iteration: 2088 / 2400   loss: 0.053790446\n",
      "iteration: 2089 / 2400   loss: 0.27984992\n",
      "iteration: 2090 / 2400   loss: 0.22140303\n",
      "iteration: 2091 / 2400   loss: 0.04154461\n",
      "iteration: 2092 / 2400   loss: 0.14380771\n",
      "iteration: 2093 / 2400   loss: 0.33199173\n",
      "iteration: 2094 / 2400   loss: 0.058908843\n",
      "iteration: 2095 / 2400   loss: 0.061891925\n",
      "iteration: 2096 / 2400   loss: 0.056476716\n",
      "iteration: 2097 / 2400   loss: 0.12001702\n",
      "iteration: 2098 / 2400   loss: 0.17542437\n",
      "iteration: 2099 / 2400   loss: 0.31585613\n",
      "iteration: 2100 / 2400   loss: 0.034831792\n",
      "iteration: 2101 / 2400   loss: 0.18718827\n",
      "iteration: 2102 / 2400   loss: 0.042850103\n",
      "iteration: 2103 / 2400   loss: 0.06969327\n",
      "iteration: 2104 / 2400   loss: 0.031730853\n",
      "iteration: 2105 / 2400   loss: 0.17972818\n",
      "iteration: 2106 / 2400   loss: 0.20261405\n",
      "iteration: 2107 / 2400   loss: 0.23987032\n",
      "iteration: 2108 / 2400   loss: 0.39193222\n",
      "iteration: 2109 / 2400   loss: 0.32326674\n",
      "iteration: 2110 / 2400   loss: 0.3601115\n",
      "iteration: 2111 / 2400   loss: 0.10475963\n",
      "iteration: 2112 / 2400   loss: 0.024690123\n",
      "iteration: 2113 / 2400   loss: 0.6085076\n",
      "iteration: 2114 / 2400   loss: 0.4551746\n",
      "iteration: 2115 / 2400   loss: 0.13537017\n",
      "iteration: 2116 / 2400   loss: 0.20268379\n",
      "iteration: 2117 / 2400   loss: 0.20304412\n",
      "iteration: 2118 / 2400   loss: 0.5099405\n",
      "iteration: 2119 / 2400   loss: 0.40164536\n",
      "iteration: 2120 / 2400   loss: 0.24259956\n",
      "iteration: 2121 / 2400   loss: 0.20161247\n",
      "iteration: 2122 / 2400   loss: 0.19159324\n",
      "iteration: 2123 / 2400   loss: 0.09065853\n",
      "iteration: 2124 / 2400   loss: 0.062576205\n",
      "iteration: 2125 / 2400   loss: 0.25508344\n",
      "iteration: 2126 / 2400   loss: 0.15159088\n",
      "iteration: 2127 / 2400   loss: 0.44068295\n",
      "iteration: 2128 / 2400   loss: 0.26149562\n",
      "iteration: 2129 / 2400   loss: 0.3502302\n",
      "iteration: 2130 / 2400   loss: 0.119842835\n",
      "iteration: 2131 / 2400   loss: 0.05231451\n",
      "iteration: 2132 / 2400   loss: 0.14777538\n",
      "iteration: 2133 / 2400   loss: 0.10800038\n",
      "iteration: 2134 / 2400   loss: 0.024047079\n",
      "iteration: 2135 / 2400   loss: 0.08886328\n",
      "iteration: 2136 / 2400   loss: 0.11061049\n",
      "iteration: 2137 / 2400   loss: 0.15790644\n",
      "iteration: 2138 / 2400   loss: 0.040729254\n",
      "iteration: 2139 / 2400   loss: 0.14818321\n",
      "iteration: 2140 / 2400   loss: 0.15693492\n",
      "iteration: 2141 / 2400   loss: 0.06362414\n",
      "iteration: 2142 / 2400   loss: 0.045721237\n",
      "iteration: 2143 / 2400   loss: 0.3416975\n",
      "iteration: 2144 / 2400   loss: 0.18832445\n",
      "iteration: 2145 / 2400   loss: 0.09632356\n",
      "iteration: 2146 / 2400   loss: 0.47438186\n",
      "iteration: 2147 / 2400   loss: 0.061612204\n",
      "iteration: 2148 / 2400   loss: 0.24103275\n",
      "iteration: 2149 / 2400   loss: 0.09354335\n",
      "iteration: 2150 / 2400   loss: 0.3108463\n",
      "iteration: 2151 / 2400   loss: 0.13194805\n",
      "iteration: 2152 / 2400   loss: 0.10296606\n",
      "iteration: 2153 / 2400   loss: 0.40448666\n",
      "iteration: 2154 / 2400   loss: 0.10168208\n",
      "iteration: 2155 / 2400   loss: 0.5817111\n",
      "iteration: 2156 / 2400   loss: 0.07403385\n",
      "iteration: 2157 / 2400   loss: 0.345643\n",
      "iteration: 2158 / 2400   loss: 0.11869454\n",
      "iteration: 2159 / 2400   loss: 0.0979056\n",
      "iteration: 2160 / 2400   loss: 0.34954697\n",
      "iteration: 2161 / 2400   loss: 0.030891724\n",
      "iteration: 2162 / 2400   loss: 0.22830144\n",
      "iteration: 2163 / 2400   loss: 0.21273006\n",
      "iteration: 2164 / 2400   loss: 0.2075272\n",
      "iteration: 2165 / 2400   loss: 0.051188327\n",
      "iteration: 2166 / 2400   loss: 0.027969008\n",
      "iteration: 2167 / 2400   loss: 0.07937441\n",
      "iteration: 2168 / 2400   loss: 0.050323714\n",
      "iteration: 2169 / 2400   loss: 0.014732418\n",
      "iteration: 2170 / 2400   loss: 0.11470671\n",
      "iteration: 2171 / 2400   loss: 0.20547637\n",
      "iteration: 2172 / 2400   loss: 0.24775964\n",
      "iteration: 2173 / 2400   loss: 0.099875815\n",
      "iteration: 2174 / 2400   loss: 0.016645756\n",
      "iteration: 2175 / 2400   loss: 0.06305984\n",
      "iteration: 2176 / 2400   loss: 0.117745616\n",
      "iteration: 2177 / 2400   loss: 0.026987743\n",
      "iteration: 2178 / 2400   loss: 0.15785816\n",
      "iteration: 2179 / 2400   loss: 0.09370306\n",
      "iteration: 2180 / 2400   loss: 0.047391035\n",
      "iteration: 2181 / 2400   loss: 0.7762211\n",
      "iteration: 2182 / 2400   loss: 0.1385306\n",
      "iteration: 2183 / 2400   loss: 0.21005224\n",
      "iteration: 2184 / 2400   loss: 0.18683094\n",
      "iteration: 2185 / 2400   loss: 0.06800541\n",
      "iteration: 2186 / 2400   loss: 0.021904193\n",
      "iteration: 2187 / 2400   loss: 0.1467594\n",
      "iteration: 2188 / 2400   loss: 0.09870533\n",
      "iteration: 2189 / 2400   loss: 0.13471319\n",
      "iteration: 2190 / 2400   loss: 0.10696939\n",
      "iteration: 2191 / 2400   loss: 0.0822571\n",
      "iteration: 2192 / 2400   loss: 0.31175902\n",
      "iteration: 2193 / 2400   loss: 0.022712994\n",
      "iteration: 2194 / 2400   loss: 0.14359829\n",
      "iteration: 2195 / 2400   loss: 0.32899654\n",
      "iteration: 2196 / 2400   loss: 0.19613789\n",
      "iteration: 2197 / 2400   loss: 0.13786045\n",
      "iteration: 2198 / 2400   loss: 0.3433076\n",
      "iteration: 2199 / 2400   loss: 0.5089453\n",
      "iteration: 2200 / 2400   loss: 0.05828434\n",
      "iteration: 2201 / 2400   loss: 0.050869208\n",
      "iteration: 2202 / 2400   loss: 0.23234427\n",
      "iteration: 2203 / 2400   loss: 0.101444416\n",
      "iteration: 2204 / 2400   loss: 0.078526996\n",
      "iteration: 2205 / 2400   loss: 0.04515588\n",
      "iteration: 2206 / 2400   loss: 0.024549142\n",
      "iteration: 2207 / 2400   loss: 0.25780234\n",
      "iteration: 2208 / 2400   loss: 0.073131725\n",
      "iteration: 2209 / 2400   loss: 0.025020923\n",
      "iteration: 2210 / 2400   loss: 0.14026621\n",
      "iteration: 2211 / 2400   loss: 0.03210695\n",
      "iteration: 2212 / 2400   loss: 0.35236478\n",
      "iteration: 2213 / 2400   loss: 0.09530488\n",
      "iteration: 2214 / 2400   loss: 0.351241\n",
      "iteration: 2215 / 2400   loss: 0.1716583\n",
      "iteration: 2216 / 2400   loss: 0.021818247\n",
      "iteration: 2217 / 2400   loss: 0.111732766\n",
      "iteration: 2218 / 2400   loss: 0.20072453\n",
      "iteration: 2219 / 2400   loss: 0.034595154\n",
      "iteration: 2220 / 2400   loss: 0.31783232\n",
      "iteration: 2221 / 2400   loss: 0.2950877\n",
      "iteration: 2222 / 2400   loss: 0.14804828\n",
      "iteration: 2223 / 2400   loss: 0.024353705\n",
      "iteration: 2224 / 2400   loss: 0.026257649\n",
      "iteration: 2225 / 2400   loss: 0.25551274\n",
      "iteration: 2226 / 2400   loss: 0.11267586\n",
      "iteration: 2227 / 2400   loss: 0.024348516\n",
      "iteration: 2228 / 2400   loss: 0.06540955\n",
      "iteration: 2229 / 2400   loss: 0.16713102\n",
      "iteration: 2230 / 2400   loss: 0.258682\n",
      "iteration: 2231 / 2400   loss: 0.01454874\n",
      "iteration: 2232 / 2400   loss: 0.08905895\n",
      "iteration: 2233 / 2400   loss: 0.099268444\n",
      "iteration: 2234 / 2400   loss: 0.15894502\n",
      "iteration: 2235 / 2400   loss: 0.16918308\n",
      "iteration: 2236 / 2400   loss: 0.091044284\n",
      "iteration: 2237 / 2400   loss: 0.28046352\n",
      "iteration: 2238 / 2400   loss: 0.030896721\n",
      "iteration: 2239 / 2400   loss: 0.13279319\n",
      "iteration: 2240 / 2400   loss: 0.021625461\n",
      "iteration: 2241 / 2400   loss: 0.116643645\n",
      "iteration: 2242 / 2400   loss: 0.047361355\n",
      "iteration: 2243 / 2400   loss: 0.09789677\n",
      "iteration: 2244 / 2400   loss: 0.25539976\n",
      "iteration: 2245 / 2400   loss: 0.06875567\n",
      "iteration: 2246 / 2400   loss: 0.091288805\n",
      "iteration: 2247 / 2400   loss: 0.28066993\n",
      "iteration: 2248 / 2400   loss: 0.11921107\n",
      "iteration: 2249 / 2400   loss: 0.39583603\n",
      "iteration: 2250 / 2400   loss: 0.040037822\n",
      "iteration: 2251 / 2400   loss: 0.08514108\n",
      "iteration: 2252 / 2400   loss: 0.071817756\n",
      "iteration: 2253 / 2400   loss: 0.09133431\n",
      "iteration: 2254 / 2400   loss: 0.1005753\n",
      "iteration: 2255 / 2400   loss: 0.24804252\n",
      "iteration: 2256 / 2400   loss: 0.31794015\n",
      "iteration: 2257 / 2400   loss: 0.064722724\n",
      "iteration: 2258 / 2400   loss: 0.13201095\n",
      "iteration: 2259 / 2400   loss: 0.2718522\n",
      "iteration: 2260 / 2400   loss: 0.29649505\n",
      "iteration: 2261 / 2400   loss: 0.11474449\n",
      "iteration: 2262 / 2400   loss: 0.041185215\n",
      "iteration: 2263 / 2400   loss: 0.06880745\n",
      "iteration: 2264 / 2400   loss: 0.25764847\n",
      "iteration: 2265 / 2400   loss: 0.13512596\n",
      "iteration: 2266 / 2400   loss: 0.07115278\n",
      "iteration: 2267 / 2400   loss: 0.07571589\n",
      "iteration: 2268 / 2400   loss: 0.011112022\n",
      "iteration: 2269 / 2400   loss: 0.35342926\n",
      "iteration: 2270 / 2400   loss: 0.10387052\n",
      "iteration: 2271 / 2400   loss: 0.08877611\n",
      "iteration: 2272 / 2400   loss: 0.008693524\n",
      "iteration: 2273 / 2400   loss: 0.05174626\n",
      "iteration: 2274 / 2400   loss: 0.09933341\n",
      "iteration: 2275 / 2400   loss: 0.11115293\n",
      "iteration: 2276 / 2400   loss: 0.12327263\n",
      "iteration: 2277 / 2400   loss: 0.100632705\n",
      "iteration: 2278 / 2400   loss: 0.02386772\n",
      "iteration: 2279 / 2400   loss: 0.029128037\n",
      "iteration: 2280 / 2400   loss: 0.078059204\n",
      "iteration: 2281 / 2400   loss: 0.22016941\n",
      "iteration: 2282 / 2400   loss: 0.11022461\n",
      "iteration: 2283 / 2400   loss: 0.14479946\n",
      "iteration: 2284 / 2400   loss: 0.30610767\n",
      "iteration: 2285 / 2400   loss: 0.1956962\n",
      "iteration: 2286 / 2400   loss: 0.046585675\n",
      "iteration: 2287 / 2400   loss: 0.11815728\n",
      "iteration: 2288 / 2400   loss: 0.112296574\n",
      "iteration: 2289 / 2400   loss: 0.10980041\n",
      "iteration: 2290 / 2400   loss: 0.3312765\n",
      "iteration: 2291 / 2400   loss: 0.29389244\n",
      "iteration: 2292 / 2400   loss: 0.054502085\n",
      "iteration: 2293 / 2400   loss: 0.21856335\n",
      "iteration: 2294 / 2400   loss: 0.07873392\n",
      "iteration: 2295 / 2400   loss: 0.048221227\n",
      "iteration: 2296 / 2400   loss: 0.23439436\n",
      "iteration: 2297 / 2400   loss: 0.14125253\n",
      "iteration: 2298 / 2400   loss: 0.06975904\n",
      "iteration: 2299 / 2400   loss: 0.3145285\n",
      "iteration: 2300 / 2400   loss: 0.30165288\n",
      "iteration: 2301 / 2400   loss: 0.057676304\n",
      "iteration: 2302 / 2400   loss: 0.23522377\n",
      "iteration: 2303 / 2400   loss: 0.05555909\n",
      "iteration: 2304 / 2400   loss: 0.05796586\n",
      "iteration: 2305 / 2400   loss: 0.1986981\n",
      "iteration: 2306 / 2400   loss: 0.05044656\n",
      "iteration: 2307 / 2400   loss: 0.6231303\n",
      "iteration: 2308 / 2400   loss: 0.12338252\n",
      "iteration: 2309 / 2400   loss: 0.23884372\n",
      "iteration: 2310 / 2400   loss: 0.2754183\n",
      "iteration: 2311 / 2400   loss: 0.4298166\n",
      "iteration: 2312 / 2400   loss: 0.069331415\n",
      "iteration: 2313 / 2400   loss: 0.28741825\n",
      "iteration: 2314 / 2400   loss: 0.08718149\n",
      "iteration: 2315 / 2400   loss: 0.14061232\n",
      "iteration: 2316 / 2400   loss: 0.09895773\n",
      "iteration: 2317 / 2400   loss: 0.060419157\n",
      "iteration: 2318 / 2400   loss: 0.08856773\n",
      "iteration: 2319 / 2400   loss: 0.38118523\n",
      "iteration: 2320 / 2400   loss: 0.06347356\n",
      "iteration: 2321 / 2400   loss: 0.17266144\n",
      "iteration: 2322 / 2400   loss: 0.09313322\n",
      "iteration: 2323 / 2400   loss: 0.33833063\n",
      "iteration: 2324 / 2400   loss: 0.12777473\n",
      "iteration: 2325 / 2400   loss: 0.13132769\n",
      "iteration: 2326 / 2400   loss: 0.081166364\n",
      "iteration: 2327 / 2400   loss: 0.039870974\n",
      "iteration: 2328 / 2400   loss: 0.096479006\n",
      "iteration: 2329 / 2400   loss: 0.0070900344\n",
      "iteration: 2330 / 2400   loss: 0.112191275\n",
      "iteration: 2331 / 2400   loss: 0.091627315\n",
      "iteration: 2332 / 2400   loss: 0.011250629\n",
      "iteration: 2333 / 2400   loss: 0.2028682\n",
      "iteration: 2334 / 2400   loss: 0.1483353\n",
      "iteration: 2335 / 2400   loss: 0.020961761\n",
      "iteration: 2336 / 2400   loss: 0.08744324\n",
      "iteration: 2337 / 2400   loss: 0.16256699\n",
      "iteration: 2338 / 2400   loss: 0.11529613\n",
      "iteration: 2339 / 2400   loss: 0.21681106\n",
      "iteration: 2340 / 2400   loss: 0.094215415\n",
      "iteration: 2341 / 2400   loss: 0.08543324\n",
      "iteration: 2342 / 2400   loss: 0.24984872\n",
      "iteration: 2343 / 2400   loss: 0.18010789\n",
      "iteration: 2344 / 2400   loss: 0.06913781\n",
      "iteration: 2345 / 2400   loss: 0.026199542\n",
      "iteration: 2346 / 2400   loss: 0.09252872\n",
      "iteration: 2347 / 2400   loss: 0.116526715\n",
      "iteration: 2348 / 2400   loss: 0.047117613\n",
      "iteration: 2349 / 2400   loss: 0.104165174\n",
      "iteration: 2350 / 2400   loss: 0.050297737\n",
      "iteration: 2351 / 2400   loss: 0.044713117\n",
      "iteration: 2352 / 2400   loss: 0.21913643\n",
      "iteration: 2353 / 2400   loss: 0.7006566\n",
      "iteration: 2354 / 2400   loss: 0.27941832\n",
      "iteration: 2355 / 2400   loss: 0.19922827\n",
      "iteration: 2356 / 2400   loss: 0.05098816\n",
      "iteration: 2357 / 2400   loss: 0.012412253\n",
      "iteration: 2358 / 2400   loss: 0.008360204\n",
      "iteration: 2359 / 2400   loss: 0.06804368\n",
      "iteration: 2360 / 2400   loss: 0.042834833\n",
      "iteration: 2361 / 2400   loss: 0.04549345\n",
      "iteration: 2362 / 2400   loss: 0.044934787\n",
      "iteration: 2363 / 2400   loss: 0.14560291\n",
      "iteration: 2364 / 2400   loss: 0.09379646\n",
      "iteration: 2365 / 2400   loss: 0.03343521\n",
      "iteration: 2366 / 2400   loss: 0.10895453\n",
      "iteration: 2367 / 2400   loss: 0.028725682\n",
      "iteration: 2368 / 2400   loss: 0.030986976\n",
      "iteration: 2369 / 2400   loss: 0.1044251\n",
      "iteration: 2370 / 2400   loss: 0.022511978\n",
      "iteration: 2371 / 2400   loss: 0.06280756\n",
      "iteration: 2372 / 2400   loss: 0.39733565\n",
      "iteration: 2373 / 2400   loss: 0.14143471\n",
      "iteration: 2374 / 2400   loss: 0.2685148\n",
      "iteration: 2375 / 2400   loss: 0.35062227\n",
      "iteration: 2376 / 2400   loss: 0.59250087\n",
      "iteration: 2377 / 2400   loss: 0.41989747\n",
      "iteration: 2378 / 2400   loss: 0.14038856\n",
      "iteration: 2379 / 2400   loss: 0.12394906\n",
      "iteration: 2380 / 2400   loss: 0.0785721\n",
      "iteration: 2381 / 2400   loss: 0.0745336\n",
      "iteration: 2382 / 2400   loss: 0.08163971\n",
      "iteration: 2383 / 2400   loss: 0.18079968\n",
      "iteration: 2384 / 2400   loss: 0.16548565\n",
      "iteration: 2385 / 2400   loss: 0.07279848\n",
      "iteration: 2386 / 2400   loss: 0.06913162\n",
      "iteration: 2387 / 2400   loss: 0.2117913\n",
      "iteration: 2388 / 2400   loss: 0.06180321\n",
      "iteration: 2389 / 2400   loss: 1.247237\n",
      "iteration: 2390 / 2400   loss: 0.98842025\n",
      "iteration: 2391 / 2400   loss: 0.26652712\n",
      "iteration: 2392 / 2400   loss: 0.25516236\n",
      "iteration: 2393 / 2400   loss: 0.040183164\n",
      "iteration: 2394 / 2400   loss: 0.07592542\n",
      "iteration: 2395 / 2400   loss: 0.04808833\n",
      "iteration: 2396 / 2400   loss: 0.009605484\n",
      "iteration: 2397 / 2400   loss: 0.8516008\n",
      "iteration: 2398 / 2400   loss: 0.4414012\n",
      "iteration: 2399 / 2400   loss: 0.09735771\n",
      "iteration: 2400 / 2400   loss: 0.121483795\n",
      "epoch: 2    test_acc: 0.9464\n",
      "iteration: 1 / 2400   loss: 0.3091985\n",
      "iteration: 2 / 2400   loss: 0.23649766\n",
      "iteration: 3 / 2400   loss: 0.21655771\n",
      "iteration: 4 / 2400   loss: 0.13753611\n",
      "iteration: 5 / 2400   loss: 0.060337357\n",
      "iteration: 6 / 2400   loss: 0.55847406\n",
      "iteration: 7 / 2400   loss: 0.1206225\n",
      "iteration: 8 / 2400   loss: 0.2429056\n",
      "iteration: 9 / 2400   loss: 0.11806066\n",
      "iteration: 10 / 2400   loss: 0.06279146\n",
      "iteration: 11 / 2400   loss: 0.19650282\n",
      "iteration: 12 / 2400   loss: 0.10739261\n",
      "iteration: 13 / 2400   loss: 0.034452695\n",
      "iteration: 14 / 2400   loss: 0.080322534\n",
      "iteration: 15 / 2400   loss: 0.049497433\n",
      "iteration: 16 / 2400   loss: 0.2092722\n",
      "iteration: 17 / 2400   loss: 0.22347264\n",
      "iteration: 18 / 2400   loss: 0.08959243\n",
      "iteration: 19 / 2400   loss: 0.09522659\n",
      "iteration: 20 / 2400   loss: 0.17992721\n",
      "iteration: 21 / 2400   loss: 0.13253826\n",
      "iteration: 22 / 2400   loss: 0.19388966\n",
      "iteration: 23 / 2400   loss: 0.048531838\n",
      "iteration: 24 / 2400   loss: 0.12509766\n",
      "iteration: 25 / 2400   loss: 0.13906309\n",
      "iteration: 26 / 2400   loss: 0.2115258\n",
      "iteration: 27 / 2400   loss: 0.18693134\n",
      "iteration: 28 / 2400   loss: 0.029303588\n",
      "iteration: 29 / 2400   loss: 0.15889344\n",
      "iteration: 30 / 2400   loss: 0.045236196\n",
      "iteration: 31 / 2400   loss: 0.049643617\n",
      "iteration: 32 / 2400   loss: 0.09423461\n",
      "iteration: 33 / 2400   loss: 0.04822273\n",
      "iteration: 34 / 2400   loss: 0.08786228\n",
      "iteration: 35 / 2400   loss: 0.255306\n",
      "iteration: 36 / 2400   loss: 0.04245074\n",
      "iteration: 37 / 2400   loss: 0.13945377\n",
      "iteration: 38 / 2400   loss: 0.29740322\n",
      "iteration: 39 / 2400   loss: 0.24871369\n",
      "iteration: 40 / 2400   loss: 0.059484415\n",
      "iteration: 41 / 2400   loss: 0.27268925\n",
      "iteration: 42 / 2400   loss: 0.29642597\n",
      "iteration: 43 / 2400   loss: 0.18207878\n",
      "iteration: 44 / 2400   loss: 0.24442925\n",
      "iteration: 45 / 2400   loss: 0.20539467\n",
      "iteration: 46 / 2400   loss: 0.41740182\n",
      "iteration: 47 / 2400   loss: 0.14031275\n",
      "iteration: 48 / 2400   loss: 0.15144421\n",
      "iteration: 49 / 2400   loss: 0.16399856\n",
      "iteration: 50 / 2400   loss: 0.5393495\n",
      "iteration: 51 / 2400   loss: 0.20061615\n",
      "iteration: 52 / 2400   loss: 0.22013365\n",
      "iteration: 53 / 2400   loss: 0.12047172\n",
      "iteration: 54 / 2400   loss: 0.35005042\n",
      "iteration: 55 / 2400   loss: 0.77944684\n",
      "iteration: 56 / 2400   loss: 0.076385826\n",
      "iteration: 57 / 2400   loss: 0.3018393\n",
      "iteration: 58 / 2400   loss: 0.10150062\n",
      "iteration: 59 / 2400   loss: 0.09925087\n",
      "iteration: 60 / 2400   loss: 0.07887107\n",
      "iteration: 61 / 2400   loss: 0.38617384\n",
      "iteration: 62 / 2400   loss: 0.16701111\n",
      "iteration: 63 / 2400   loss: 0.11031712\n",
      "iteration: 64 / 2400   loss: 0.31925702\n",
      "iteration: 65 / 2400   loss: 0.56378156\n",
      "iteration: 66 / 2400   loss: 0.010588322\n",
      "iteration: 67 / 2400   loss: 0.17963046\n",
      "iteration: 68 / 2400   loss: 0.03567222\n",
      "iteration: 69 / 2400   loss: 0.06664032\n",
      "iteration: 70 / 2400   loss: 0.022512702\n",
      "iteration: 71 / 2400   loss: 0.35600418\n",
      "iteration: 72 / 2400   loss: 0.043440178\n",
      "iteration: 73 / 2400   loss: 0.041231517\n",
      "iteration: 74 / 2400   loss: 0.0052060317\n",
      "iteration: 75 / 2400   loss: 0.2770548\n",
      "iteration: 76 / 2400   loss: 0.10349638\n",
      "iteration: 77 / 2400   loss: 0.16813762\n",
      "iteration: 78 / 2400   loss: 0.38456857\n",
      "iteration: 79 / 2400   loss: 0.124995574\n",
      "iteration: 80 / 2400   loss: 0.056111164\n",
      "iteration: 81 / 2400   loss: 0.09022165\n",
      "iteration: 82 / 2400   loss: 0.16671726\n",
      "iteration: 83 / 2400   loss: 0.0473382\n",
      "iteration: 84 / 2400   loss: 0.30988425\n",
      "iteration: 85 / 2400   loss: 0.024866\n",
      "iteration: 86 / 2400   loss: 0.093912646\n",
      "iteration: 87 / 2400   loss: 0.04231335\n",
      "iteration: 88 / 2400   loss: 0.12530501\n",
      "iteration: 89 / 2400   loss: 0.2680862\n",
      "iteration: 90 / 2400   loss: 0.083006695\n",
      "iteration: 91 / 2400   loss: 0.26356056\n",
      "iteration: 92 / 2400   loss: 0.066812254\n",
      "iteration: 93 / 2400   loss: 0.08473706\n",
      "iteration: 94 / 2400   loss: 0.044713095\n",
      "iteration: 95 / 2400   loss: 0.11381154\n",
      "iteration: 96 / 2400   loss: 0.25889218\n",
      "iteration: 97 / 2400   loss: 0.37969077\n",
      "iteration: 98 / 2400   loss: 0.33400917\n",
      "iteration: 99 / 2400   loss: 0.0322551\n",
      "iteration: 100 / 2400   loss: 0.13448328\n",
      "iteration: 101 / 2400   loss: 0.037438754\n",
      "iteration: 102 / 2400   loss: 0.05396942\n",
      "iteration: 103 / 2400   loss: 0.13029513\n",
      "iteration: 104 / 2400   loss: 0.20744064\n",
      "iteration: 105 / 2400   loss: 0.15718791\n",
      "iteration: 106 / 2400   loss: 0.25777316\n",
      "iteration: 107 / 2400   loss: 0.006529455\n",
      "iteration: 108 / 2400   loss: 0.16567825\n",
      "iteration: 109 / 2400   loss: 0.47917533\n",
      "iteration: 110 / 2400   loss: 0.09718039\n",
      "iteration: 111 / 2400   loss: 0.17078486\n",
      "iteration: 112 / 2400   loss: 0.11126327\n",
      "iteration: 113 / 2400   loss: 0.15399733\n",
      "iteration: 114 / 2400   loss: 0.14103337\n",
      "iteration: 115 / 2400   loss: 0.029867774\n",
      "iteration: 116 / 2400   loss: 0.038803253\n",
      "iteration: 117 / 2400   loss: 0.04224532\n",
      "iteration: 118 / 2400   loss: 0.23330887\n",
      "iteration: 119 / 2400   loss: 0.070851974\n",
      "iteration: 120 / 2400   loss: 0.10403062\n",
      "iteration: 121 / 2400   loss: 0.2614842\n",
      "iteration: 122 / 2400   loss: 0.1421808\n",
      "iteration: 123 / 2400   loss: 0.5286077\n",
      "iteration: 124 / 2400   loss: 0.09886819\n",
      "iteration: 125 / 2400   loss: 0.101718634\n",
      "iteration: 126 / 2400   loss: 0.06634428\n",
      "iteration: 127 / 2400   loss: 0.11144185\n",
      "iteration: 128 / 2400   loss: 0.024323598\n",
      "iteration: 129 / 2400   loss: 0.18227836\n",
      "iteration: 130 / 2400   loss: 0.057475824\n",
      "iteration: 131 / 2400   loss: 0.30437806\n",
      "iteration: 132 / 2400   loss: 0.035331946\n",
      "iteration: 133 / 2400   loss: 0.15893209\n",
      "iteration: 134 / 2400   loss: 0.045704108\n",
      "iteration: 135 / 2400   loss: 0.11022661\n",
      "iteration: 136 / 2400   loss: 0.07522024\n",
      "iteration: 137 / 2400   loss: 0.26345664\n",
      "iteration: 138 / 2400   loss: 0.07279733\n",
      "iteration: 139 / 2400   loss: 0.09487265\n",
      "iteration: 140 / 2400   loss: 0.11593258\n",
      "iteration: 141 / 2400   loss: 0.627352\n",
      "iteration: 142 / 2400   loss: 0.1733391\n",
      "iteration: 143 / 2400   loss: 0.035257045\n",
      "iteration: 144 / 2400   loss: 0.0433064\n",
      "iteration: 145 / 2400   loss: 0.04468586\n",
      "iteration: 146 / 2400   loss: 0.21429442\n",
      "iteration: 147 / 2400   loss: 0.06840334\n",
      "iteration: 148 / 2400   loss: 0.265216\n",
      "iteration: 149 / 2400   loss: 0.22331612\n",
      "iteration: 150 / 2400   loss: 0.12618066\n",
      "iteration: 151 / 2400   loss: 0.14301029\n",
      "iteration: 152 / 2400   loss: 0.03130417\n",
      "iteration: 153 / 2400   loss: 0.07242851\n",
      "iteration: 154 / 2400   loss: 0.05798978\n",
      "iteration: 155 / 2400   loss: 0.10228624\n",
      "iteration: 156 / 2400   loss: 0.07124277\n",
      "iteration: 157 / 2400   loss: 0.2255106\n",
      "iteration: 158 / 2400   loss: 0.12046048\n",
      "iteration: 159 / 2400   loss: 0.039043292\n",
      "iteration: 160 / 2400   loss: 0.0353108\n",
      "iteration: 161 / 2400   loss: 0.08582397\n",
      "iteration: 162 / 2400   loss: 0.05062456\n",
      "iteration: 163 / 2400   loss: 0.56025386\n",
      "iteration: 164 / 2400   loss: 0.081563026\n",
      "iteration: 165 / 2400   loss: 0.19000582\n",
      "iteration: 166 / 2400   loss: 0.24673991\n",
      "iteration: 167 / 2400   loss: 0.35518688\n",
      "iteration: 168 / 2400   loss: 0.12422064\n",
      "iteration: 169 / 2400   loss: 0.115850575\n",
      "iteration: 170 / 2400   loss: 0.054187298\n",
      "iteration: 171 / 2400   loss: 0.07785782\n",
      "iteration: 172 / 2400   loss: 0.4440648\n",
      "iteration: 173 / 2400   loss: 0.2109212\n",
      "iteration: 174 / 2400   loss: 0.06397435\n",
      "iteration: 175 / 2400   loss: 0.23113376\n",
      "iteration: 176 / 2400   loss: 0.042731322\n",
      "iteration: 177 / 2400   loss: 0.03673746\n",
      "iteration: 178 / 2400   loss: 0.23960537\n",
      "iteration: 179 / 2400   loss: 0.11615555\n",
      "iteration: 180 / 2400   loss: 0.18408155\n",
      "iteration: 181 / 2400   loss: 0.3484865\n",
      "iteration: 182 / 2400   loss: 0.071152344\n",
      "iteration: 183 / 2400   loss: 0.014950219\n",
      "iteration: 184 / 2400   loss: 0.04368831\n",
      "iteration: 185 / 2400   loss: 0.03663416\n",
      "iteration: 186 / 2400   loss: 0.3571856\n",
      "iteration: 187 / 2400   loss: 0.368086\n",
      "iteration: 188 / 2400   loss: 0.14419495\n",
      "iteration: 189 / 2400   loss: 0.06868893\n",
      "iteration: 190 / 2400   loss: 0.13566017\n",
      "iteration: 191 / 2400   loss: 0.17671\n",
      "iteration: 192 / 2400   loss: 0.04226681\n",
      "iteration: 193 / 2400   loss: 0.13391137\n",
      "iteration: 194 / 2400   loss: 0.18191871\n",
      "iteration: 195 / 2400   loss: 0.14853951\n",
      "iteration: 196 / 2400   loss: 0.0163383\n",
      "iteration: 197 / 2400   loss: 0.14667684\n",
      "iteration: 198 / 2400   loss: 0.24146797\n",
      "iteration: 199 / 2400   loss: 0.2343558\n",
      "iteration: 200 / 2400   loss: 0.059087735\n",
      "iteration: 201 / 2400   loss: 0.050065633\n",
      "iteration: 202 / 2400   loss: 0.14895669\n",
      "iteration: 203 / 2400   loss: 0.1086451\n",
      "iteration: 204 / 2400   loss: 0.070128515\n",
      "iteration: 205 / 2400   loss: 0.2506952\n",
      "iteration: 206 / 2400   loss: 0.19354768\n",
      "iteration: 207 / 2400   loss: 0.13469128\n",
      "iteration: 208 / 2400   loss: 0.2773408\n",
      "iteration: 209 / 2400   loss: 0.13560301\n",
      "iteration: 210 / 2400   loss: 0.031625558\n",
      "iteration: 211 / 2400   loss: 0.12021967\n",
      "iteration: 212 / 2400   loss: 0.2557449\n",
      "iteration: 213 / 2400   loss: 0.24769743\n",
      "iteration: 214 / 2400   loss: 0.5956981\n",
      "iteration: 215 / 2400   loss: 0.20119934\n",
      "iteration: 216 / 2400   loss: 0.14429972\n",
      "iteration: 217 / 2400   loss: 0.31085\n",
      "iteration: 218 / 2400   loss: 0.19329615\n",
      "iteration: 219 / 2400   loss: 0.04357524\n",
      "iteration: 220 / 2400   loss: 0.08011987\n",
      "iteration: 221 / 2400   loss: 0.033511113\n",
      "iteration: 222 / 2400   loss: 0.1358432\n",
      "iteration: 223 / 2400   loss: 0.39334083\n",
      "iteration: 224 / 2400   loss: 0.030560922\n",
      "iteration: 225 / 2400   loss: 0.060269013\n",
      "iteration: 226 / 2400   loss: 0.21594945\n",
      "iteration: 227 / 2400   loss: 0.1860765\n",
      "iteration: 228 / 2400   loss: 0.08259739\n",
      "iteration: 229 / 2400   loss: 0.2080297\n",
      "iteration: 230 / 2400   loss: 0.25654364\n",
      "iteration: 231 / 2400   loss: 0.20024997\n",
      "iteration: 232 / 2400   loss: 0.28201085\n",
      "iteration: 233 / 2400   loss: 0.32451355\n",
      "iteration: 234 / 2400   loss: 0.35683832\n",
      "iteration: 235 / 2400   loss: 0.050488878\n",
      "iteration: 236 / 2400   loss: 0.92888\n",
      "iteration: 237 / 2400   loss: 0.23723835\n",
      "iteration: 238 / 2400   loss: 0.091006115\n",
      "iteration: 239 / 2400   loss: 0.2869111\n",
      "iteration: 240 / 2400   loss: 0.010473242\n",
      "iteration: 241 / 2400   loss: 0.08024553\n",
      "iteration: 242 / 2400   loss: 0.1126487\n",
      "iteration: 243 / 2400   loss: 0.054605465\n",
      "iteration: 244 / 2400   loss: 0.03299825\n",
      "iteration: 245 / 2400   loss: 0.14598349\n",
      "iteration: 246 / 2400   loss: 0.33292916\n",
      "iteration: 247 / 2400   loss: 0.19373684\n",
      "iteration: 248 / 2400   loss: 0.029165793\n",
      "iteration: 249 / 2400   loss: 0.26337567\n",
      "iteration: 250 / 2400   loss: 0.09591943\n",
      "iteration: 251 / 2400   loss: 0.60832334\n",
      "iteration: 252 / 2400   loss: 0.09971859\n",
      "iteration: 253 / 2400   loss: 0.15400562\n",
      "iteration: 254 / 2400   loss: 0.33497697\n",
      "iteration: 255 / 2400   loss: 0.06873009\n",
      "iteration: 256 / 2400   loss: 0.052834015\n",
      "iteration: 257 / 2400   loss: 0.402677\n",
      "iteration: 258 / 2400   loss: 0.27450278\n",
      "iteration: 259 / 2400   loss: 0.45305556\n",
      "iteration: 260 / 2400   loss: 0.29822767\n",
      "iteration: 261 / 2400   loss: 0.23353562\n",
      "iteration: 262 / 2400   loss: 0.12439108\n",
      "iteration: 263 / 2400   loss: 0.042892512\n",
      "iteration: 264 / 2400   loss: 0.08411759\n",
      "iteration: 265 / 2400   loss: 0.28240374\n",
      "iteration: 266 / 2400   loss: 0.019511452\n",
      "iteration: 267 / 2400   loss: 0.24068034\n",
      "iteration: 268 / 2400   loss: 0.06820276\n",
      "iteration: 269 / 2400   loss: 0.4276387\n",
      "iteration: 270 / 2400   loss: 0.2031312\n",
      "iteration: 271 / 2400   loss: 0.08483529\n",
      "iteration: 272 / 2400   loss: 0.057567015\n",
      "iteration: 273 / 2400   loss: 0.24886733\n",
      "iteration: 274 / 2400   loss: 0.3618247\n",
      "iteration: 275 / 2400   loss: 0.076798975\n",
      "iteration: 276 / 2400   loss: 0.83685285\n",
      "iteration: 277 / 2400   loss: 0.14148651\n",
      "iteration: 278 / 2400   loss: 0.18368332\n",
      "iteration: 279 / 2400   loss: 0.14494246\n",
      "iteration: 280 / 2400   loss: 0.082963675\n",
      "iteration: 281 / 2400   loss: 0.4171629\n",
      "iteration: 282 / 2400   loss: 0.06582949\n",
      "iteration: 283 / 2400   loss: 0.2292122\n",
      "iteration: 284 / 2400   loss: 0.17012703\n",
      "iteration: 285 / 2400   loss: 0.1115628\n",
      "iteration: 286 / 2400   loss: 0.19709943\n",
      "iteration: 287 / 2400   loss: 0.3814919\n",
      "iteration: 288 / 2400   loss: 0.21715465\n",
      "iteration: 289 / 2400   loss: 0.16211525\n",
      "iteration: 290 / 2400   loss: 0.30197245\n",
      "iteration: 291 / 2400   loss: 0.47432527\n",
      "iteration: 292 / 2400   loss: 0.12901673\n",
      "iteration: 293 / 2400   loss: 0.37378514\n",
      "iteration: 294 / 2400   loss: 0.30602527\n",
      "iteration: 295 / 2400   loss: 0.4750613\n",
      "iteration: 296 / 2400   loss: 0.17780033\n",
      "iteration: 297 / 2400   loss: 0.113507785\n",
      "iteration: 298 / 2400   loss: 0.116272084\n",
      "iteration: 299 / 2400   loss: 0.16079412\n",
      "iteration: 300 / 2400   loss: 0.089105986\n",
      "iteration: 301 / 2400   loss: 0.073039815\n",
      "iteration: 302 / 2400   loss: 0.12209575\n",
      "iteration: 303 / 2400   loss: 0.035548095\n",
      "iteration: 304 / 2400   loss: 0.33481827\n",
      "iteration: 305 / 2400   loss: 0.16984528\n",
      "iteration: 306 / 2400   loss: 0.17685011\n",
      "iteration: 307 / 2400   loss: 0.015863743\n",
      "iteration: 308 / 2400   loss: 0.23371315\n",
      "iteration: 309 / 2400   loss: 0.09361947\n",
      "iteration: 310 / 2400   loss: 0.34026295\n",
      "iteration: 311 / 2400   loss: 0.44352493\n",
      "iteration: 312 / 2400   loss: 0.08492409\n",
      "iteration: 313 / 2400   loss: 0.17452028\n",
      "iteration: 314 / 2400   loss: 0.19904435\n",
      "iteration: 315 / 2400   loss: 0.077090666\n",
      "iteration: 316 / 2400   loss: 0.6095799\n",
      "iteration: 317 / 2400   loss: 0.17108521\n",
      "iteration: 318 / 2400   loss: 0.077788465\n",
      "iteration: 319 / 2400   loss: 0.29764956\n",
      "iteration: 320 / 2400   loss: 0.40049493\n",
      "iteration: 321 / 2400   loss: 0.038018893\n",
      "iteration: 322 / 2400   loss: 0.18515486\n",
      "iteration: 323 / 2400   loss: 0.1360779\n",
      "iteration: 324 / 2400   loss: 0.15732564\n",
      "iteration: 325 / 2400   loss: 0.19224173\n",
      "iteration: 326 / 2400   loss: 0.09225919\n",
      "iteration: 327 / 2400   loss: 0.027247243\n",
      "iteration: 328 / 2400   loss: 0.1489935\n",
      "iteration: 329 / 2400   loss: 0.84766984\n",
      "iteration: 330 / 2400   loss: 0.13123326\n",
      "iteration: 331 / 2400   loss: 0.26067498\n",
      "iteration: 332 / 2400   loss: 0.13539113\n",
      "iteration: 333 / 2400   loss: 0.14487039\n",
      "iteration: 334 / 2400   loss: 0.10968027\n",
      "iteration: 335 / 2400   loss: 0.03365667\n",
      "iteration: 336 / 2400   loss: 0.06565023\n",
      "iteration: 337 / 2400   loss: 0.0745576\n",
      "iteration: 338 / 2400   loss: 0.3483087\n",
      "iteration: 339 / 2400   loss: 0.2386555\n",
      "iteration: 340 / 2400   loss: 0.28956404\n",
      "iteration: 341 / 2400   loss: 0.09036094\n",
      "iteration: 342 / 2400   loss: 0.021515809\n",
      "iteration: 343 / 2400   loss: 0.024214316\n",
      "iteration: 344 / 2400   loss: 0.051723737\n",
      "iteration: 345 / 2400   loss: 0.12919153\n",
      "iteration: 346 / 2400   loss: 0.24390501\n",
      "iteration: 347 / 2400   loss: 0.10066721\n",
      "iteration: 348 / 2400   loss: 0.33137\n",
      "iteration: 349 / 2400   loss: 0.44113797\n",
      "iteration: 350 / 2400   loss: 0.36428094\n",
      "iteration: 351 / 2400   loss: 0.4101692\n",
      "iteration: 352 / 2400   loss: 0.1258728\n",
      "iteration: 353 / 2400   loss: 0.09102047\n",
      "iteration: 354 / 2400   loss: 0.074768476\n",
      "iteration: 355 / 2400   loss: 0.8166397\n",
      "iteration: 356 / 2400   loss: 0.38597205\n",
      "iteration: 357 / 2400   loss: 0.36264178\n",
      "iteration: 358 / 2400   loss: 0.15310091\n",
      "iteration: 359 / 2400   loss: 0.1886826\n",
      "iteration: 360 / 2400   loss: 0.07987918\n",
      "iteration: 361 / 2400   loss: 0.08020696\n",
      "iteration: 362 / 2400   loss: 0.09140702\n",
      "iteration: 363 / 2400   loss: 0.049741868\n",
      "iteration: 364 / 2400   loss: 0.1704569\n",
      "iteration: 365 / 2400   loss: 0.3396714\n",
      "iteration: 366 / 2400   loss: 0.1339\n",
      "iteration: 367 / 2400   loss: 0.18444246\n",
      "iteration: 368 / 2400   loss: 0.14554293\n",
      "iteration: 369 / 2400   loss: 0.21247326\n",
      "iteration: 370 / 2400   loss: 0.1533093\n",
      "iteration: 371 / 2400   loss: 0.054908644\n",
      "iteration: 372 / 2400   loss: 0.18510762\n",
      "iteration: 373 / 2400   loss: 0.15104279\n",
      "iteration: 374 / 2400   loss: 0.46083736\n",
      "iteration: 375 / 2400   loss: 0.01875043\n",
      "iteration: 376 / 2400   loss: 0.2444625\n",
      "iteration: 377 / 2400   loss: 0.04126739\n",
      "iteration: 378 / 2400   loss: 0.7548779\n",
      "iteration: 379 / 2400   loss: 0.43398055\n",
      "iteration: 380 / 2400   loss: 0.110130355\n",
      "iteration: 381 / 2400   loss: 0.12920664\n",
      "iteration: 382 / 2400   loss: 0.21723366\n",
      "iteration: 383 / 2400   loss: 0.54635805\n",
      "iteration: 384 / 2400   loss: 0.32431525\n",
      "iteration: 385 / 2400   loss: 0.5201609\n",
      "iteration: 386 / 2400   loss: 0.203915\n",
      "iteration: 387 / 2400   loss: 0.2317468\n",
      "iteration: 388 / 2400   loss: 0.059668656\n",
      "iteration: 389 / 2400   loss: 0.01721384\n",
      "iteration: 390 / 2400   loss: 0.103620015\n",
      "iteration: 391 / 2400   loss: 0.14915694\n",
      "iteration: 392 / 2400   loss: 0.11389042\n",
      "iteration: 393 / 2400   loss: 0.17497687\n",
      "iteration: 394 / 2400   loss: 0.0141893765\n",
      "iteration: 395 / 2400   loss: 0.048043977\n",
      "iteration: 396 / 2400   loss: 0.06757792\n",
      "iteration: 397 / 2400   loss: 0.049563505\n",
      "iteration: 398 / 2400   loss: 0.15854383\n",
      "iteration: 399 / 2400   loss: 0.06747125\n",
      "iteration: 400 / 2400   loss: 0.05330614\n",
      "iteration: 401 / 2400   loss: 0.30323246\n",
      "iteration: 402 / 2400   loss: 0.378164\n",
      "iteration: 403 / 2400   loss: 0.048239402\n",
      "iteration: 404 / 2400   loss: 0.07367214\n",
      "iteration: 405 / 2400   loss: 0.2979796\n",
      "iteration: 406 / 2400   loss: 0.03533636\n",
      "iteration: 407 / 2400   loss: 0.090220585\n",
      "iteration: 408 / 2400   loss: 0.17432588\n",
      "iteration: 409 / 2400   loss: 0.71771264\n",
      "iteration: 410 / 2400   loss: 0.4468998\n",
      "iteration: 411 / 2400   loss: 0.3749012\n",
      "iteration: 412 / 2400   loss: 0.19800472\n",
      "iteration: 413 / 2400   loss: 0.17020886\n",
      "iteration: 414 / 2400   loss: 0.043378524\n",
      "iteration: 415 / 2400   loss: 0.2759177\n",
      "iteration: 416 / 2400   loss: 0.17785023\n",
      "iteration: 417 / 2400   loss: 0.043203257\n",
      "iteration: 418 / 2400   loss: 0.44533557\n",
      "iteration: 419 / 2400   loss: 0.051689405\n",
      "iteration: 420 / 2400   loss: 0.32879317\n",
      "iteration: 421 / 2400   loss: 0.06051444\n",
      "iteration: 422 / 2400   loss: 0.049254805\n",
      "iteration: 423 / 2400   loss: 0.14026609\n",
      "iteration: 424 / 2400   loss: 0.18223585\n",
      "iteration: 425 / 2400   loss: 0.08248496\n",
      "iteration: 426 / 2400   loss: 0.08609667\n",
      "iteration: 427 / 2400   loss: 0.102951005\n",
      "iteration: 428 / 2400   loss: 0.0327518\n",
      "iteration: 429 / 2400   loss: 0.4191046\n",
      "iteration: 430 / 2400   loss: 0.29430264\n",
      "iteration: 431 / 2400   loss: 0.1792877\n",
      "iteration: 432 / 2400   loss: 0.089312896\n",
      "iteration: 433 / 2400   loss: 0.057173986\n",
      "iteration: 434 / 2400   loss: 0.07529081\n",
      "iteration: 435 / 2400   loss: 0.38827115\n",
      "iteration: 436 / 2400   loss: 0.076969154\n",
      "iteration: 437 / 2400   loss: 0.012535934\n",
      "iteration: 438 / 2400   loss: 0.1488247\n",
      "iteration: 439 / 2400   loss: 0.042150754\n",
      "iteration: 440 / 2400   loss: 0.64664847\n",
      "iteration: 441 / 2400   loss: 0.023355465\n",
      "iteration: 442 / 2400   loss: 0.15545267\n",
      "iteration: 443 / 2400   loss: 0.32820433\n",
      "iteration: 444 / 2400   loss: 0.051364373\n",
      "iteration: 445 / 2400   loss: 0.070043735\n",
      "iteration: 446 / 2400   loss: 0.097298235\n",
      "iteration: 447 / 2400   loss: 0.105155714\n",
      "iteration: 448 / 2400   loss: 0.061736755\n",
      "iteration: 449 / 2400   loss: 0.1846235\n",
      "iteration: 450 / 2400   loss: 0.17682633\n",
      "iteration: 451 / 2400   loss: 0.021938553\n",
      "iteration: 452 / 2400   loss: 0.13091289\n",
      "iteration: 453 / 2400   loss: 0.04730178\n",
      "iteration: 454 / 2400   loss: 0.12151732\n",
      "iteration: 455 / 2400   loss: 0.10518814\n",
      "iteration: 456 / 2400   loss: 0.07188801\n",
      "iteration: 457 / 2400   loss: 0.08386183\n",
      "iteration: 458 / 2400   loss: 0.22610556\n",
      "iteration: 459 / 2400   loss: 0.015523625\n",
      "iteration: 460 / 2400   loss: 0.079837896\n",
      "iteration: 461 / 2400   loss: 0.03744974\n",
      "iteration: 462 / 2400   loss: 0.11615923\n",
      "iteration: 463 / 2400   loss: 0.4013209\n",
      "iteration: 464 / 2400   loss: 0.20504235\n",
      "iteration: 465 / 2400   loss: 0.40613008\n",
      "iteration: 466 / 2400   loss: 0.22103734\n",
      "iteration: 467 / 2400   loss: 0.083383754\n",
      "iteration: 468 / 2400   loss: 0.14523202\n",
      "iteration: 469 / 2400   loss: 0.15051468\n",
      "iteration: 470 / 2400   loss: 0.14550604\n",
      "iteration: 471 / 2400   loss: 0.19470516\n",
      "iteration: 472 / 2400   loss: 0.32724893\n",
      "iteration: 473 / 2400   loss: 0.10628795\n",
      "iteration: 474 / 2400   loss: 0.18312487\n",
      "iteration: 475 / 2400   loss: 0.09727864\n",
      "iteration: 476 / 2400   loss: 0.16572541\n",
      "iteration: 477 / 2400   loss: 0.019532824\n",
      "iteration: 478 / 2400   loss: 0.20043762\n",
      "iteration: 479 / 2400   loss: 0.047430202\n",
      "iteration: 480 / 2400   loss: 0.123583905\n",
      "iteration: 481 / 2400   loss: 0.06291655\n",
      "iteration: 482 / 2400   loss: 0.05787237\n",
      "iteration: 483 / 2400   loss: 0.13659976\n",
      "iteration: 484 / 2400   loss: 0.13046536\n",
      "iteration: 485 / 2400   loss: 0.111576945\n",
      "iteration: 486 / 2400   loss: 0.02891346\n",
      "iteration: 487 / 2400   loss: 0.14808986\n",
      "iteration: 488 / 2400   loss: 0.16487923\n",
      "iteration: 489 / 2400   loss: 0.12660418\n",
      "iteration: 490 / 2400   loss: 0.10898141\n",
      "iteration: 491 / 2400   loss: 0.17463924\n",
      "iteration: 492 / 2400   loss: 0.2765333\n",
      "iteration: 493 / 2400   loss: 0.18519968\n",
      "iteration: 494 / 2400   loss: 0.032866374\n",
      "iteration: 495 / 2400   loss: 0.076151565\n",
      "iteration: 496 / 2400   loss: 0.045960132\n",
      "iteration: 497 / 2400   loss: 0.14357913\n",
      "iteration: 498 / 2400   loss: 0.040518694\n",
      "iteration: 499 / 2400   loss: 0.09308982\n",
      "iteration: 500 / 2400   loss: 0.18344775\n",
      "iteration: 501 / 2400   loss: 0.053670768\n",
      "iteration: 502 / 2400   loss: 0.05719159\n",
      "iteration: 503 / 2400   loss: 0.22889966\n",
      "iteration: 504 / 2400   loss: 0.13285911\n",
      "iteration: 505 / 2400   loss: 0.06487721\n",
      "iteration: 506 / 2400   loss: 0.16358185\n",
      "iteration: 507 / 2400   loss: 0.4570282\n",
      "iteration: 508 / 2400   loss: 0.3792591\n",
      "iteration: 509 / 2400   loss: 0.18511014\n",
      "iteration: 510 / 2400   loss: 0.14330813\n",
      "iteration: 511 / 2400   loss: 0.15309343\n",
      "iteration: 512 / 2400   loss: 0.23801975\n",
      "iteration: 513 / 2400   loss: 0.06370415\n",
      "iteration: 514 / 2400   loss: 0.45844537\n",
      "iteration: 515 / 2400   loss: 0.055803765\n",
      "iteration: 516 / 2400   loss: 0.106307104\n",
      "iteration: 517 / 2400   loss: 0.029433452\n",
      "iteration: 518 / 2400   loss: 0.09640827\n",
      "iteration: 519 / 2400   loss: 0.3805102\n",
      "iteration: 520 / 2400   loss: 0.08743675\n",
      "iteration: 521 / 2400   loss: 0.25123858\n",
      "iteration: 522 / 2400   loss: 0.16496013\n",
      "iteration: 523 / 2400   loss: 0.11672121\n",
      "iteration: 524 / 2400   loss: 0.23413068\n",
      "iteration: 525 / 2400   loss: 0.09780904\n",
      "iteration: 526 / 2400   loss: 0.1884304\n",
      "iteration: 527 / 2400   loss: 0.26001525\n",
      "iteration: 528 / 2400   loss: 0.2931065\n",
      "iteration: 529 / 2400   loss: 0.02120882\n",
      "iteration: 530 / 2400   loss: 0.089407004\n",
      "iteration: 531 / 2400   loss: 0.031459283\n",
      "iteration: 532 / 2400   loss: 0.046242543\n",
      "iteration: 533 / 2400   loss: 0.35887206\n",
      "iteration: 534 / 2400   loss: 0.12622973\n",
      "iteration: 535 / 2400   loss: 0.16579701\n",
      "iteration: 536 / 2400   loss: 0.063247375\n",
      "iteration: 537 / 2400   loss: 0.23242855\n",
      "iteration: 538 / 2400   loss: 0.3478835\n",
      "iteration: 539 / 2400   loss: 0.2105365\n",
      "iteration: 540 / 2400   loss: 0.02265873\n",
      "iteration: 541 / 2400   loss: 0.23766327\n",
      "iteration: 542 / 2400   loss: 0.08995478\n",
      "iteration: 543 / 2400   loss: 0.03273204\n",
      "iteration: 544 / 2400   loss: 0.03594737\n",
      "iteration: 545 / 2400   loss: 0.040536575\n",
      "iteration: 546 / 2400   loss: 0.040031556\n",
      "iteration: 547 / 2400   loss: 0.2972744\n",
      "iteration: 548 / 2400   loss: 0.35476944\n",
      "iteration: 549 / 2400   loss: 0.25682786\n",
      "iteration: 550 / 2400   loss: 0.16183022\n",
      "iteration: 551 / 2400   loss: 0.21081153\n",
      "iteration: 552 / 2400   loss: 0.11997273\n",
      "iteration: 553 / 2400   loss: 0.02915243\n",
      "iteration: 554 / 2400   loss: 0.09100938\n",
      "iteration: 555 / 2400   loss: 0.13658883\n",
      "iteration: 556 / 2400   loss: 0.041810054\n",
      "iteration: 557 / 2400   loss: 0.5504407\n",
      "iteration: 558 / 2400   loss: 0.36231664\n",
      "iteration: 559 / 2400   loss: 0.06933021\n",
      "iteration: 560 / 2400   loss: 0.156363\n",
      "iteration: 561 / 2400   loss: 0.3345115\n",
      "iteration: 562 / 2400   loss: 0.055806533\n",
      "iteration: 563 / 2400   loss: 0.07433304\n",
      "iteration: 564 / 2400   loss: 0.26201707\n",
      "iteration: 565 / 2400   loss: 0.07674877\n",
      "iteration: 566 / 2400   loss: 0.13221766\n",
      "iteration: 567 / 2400   loss: 0.33972874\n",
      "iteration: 568 / 2400   loss: 0.08898231\n",
      "iteration: 569 / 2400   loss: 0.2696714\n",
      "iteration: 570 / 2400   loss: 0.0808675\n",
      "iteration: 571 / 2400   loss: 0.24149631\n",
      "iteration: 572 / 2400   loss: 0.21887362\n",
      "iteration: 573 / 2400   loss: 0.04572506\n",
      "iteration: 574 / 2400   loss: 0.2266524\n",
      "iteration: 575 / 2400   loss: 0.46487123\n",
      "iteration: 576 / 2400   loss: 0.2548527\n",
      "iteration: 577 / 2400   loss: 0.13673949\n",
      "iteration: 578 / 2400   loss: 0.05499033\n",
      "iteration: 579 / 2400   loss: 0.06348542\n",
      "iteration: 580 / 2400   loss: 0.07121961\n",
      "iteration: 581 / 2400   loss: 0.15950222\n",
      "iteration: 582 / 2400   loss: 0.19168884\n",
      "iteration: 583 / 2400   loss: 0.06553503\n",
      "iteration: 584 / 2400   loss: 0.47820374\n",
      "iteration: 585 / 2400   loss: 0.17317438\n",
      "iteration: 586 / 2400   loss: 0.17113946\n",
      "iteration: 587 / 2400   loss: 0.21110474\n",
      "iteration: 588 / 2400   loss: 0.17089088\n",
      "iteration: 589 / 2400   loss: 0.19496292\n",
      "iteration: 590 / 2400   loss: 0.057522964\n",
      "iteration: 591 / 2400   loss: 0.5358882\n",
      "iteration: 592 / 2400   loss: 0.36372575\n",
      "iteration: 593 / 2400   loss: 0.17314541\n",
      "iteration: 594 / 2400   loss: 0.10200136\n",
      "iteration: 595 / 2400   loss: 0.23703058\n",
      "iteration: 596 / 2400   loss: 0.25106522\n",
      "iteration: 597 / 2400   loss: 0.08388176\n",
      "iteration: 598 / 2400   loss: 0.038417682\n",
      "iteration: 599 / 2400   loss: 0.18167828\n",
      "iteration: 600 / 2400   loss: 0.05154971\n",
      "iteration: 601 / 2400   loss: 0.07094893\n",
      "iteration: 602 / 2400   loss: 0.013198567\n",
      "iteration: 603 / 2400   loss: 0.16278605\n",
      "iteration: 604 / 2400   loss: 0.04567976\n",
      "iteration: 605 / 2400   loss: 0.34683272\n",
      "iteration: 606 / 2400   loss: 0.08134467\n",
      "iteration: 607 / 2400   loss: 0.23850739\n",
      "iteration: 608 / 2400   loss: 0.18494032\n",
      "iteration: 609 / 2400   loss: 0.020980157\n",
      "iteration: 610 / 2400   loss: 0.12698424\n",
      "iteration: 611 / 2400   loss: 0.34724662\n",
      "iteration: 612 / 2400   loss: 0.13952076\n",
      "iteration: 613 / 2400   loss: 0.065947495\n",
      "iteration: 614 / 2400   loss: 0.08900138\n",
      "iteration: 615 / 2400   loss: 0.18475361\n",
      "iteration: 616 / 2400   loss: 0.045018386\n",
      "iteration: 617 / 2400   loss: 0.5895977\n",
      "iteration: 618 / 2400   loss: 0.03849159\n",
      "iteration: 619 / 2400   loss: 0.25552914\n",
      "iteration: 620 / 2400   loss: 0.23061281\n",
      "iteration: 621 / 2400   loss: 0.22346856\n",
      "iteration: 622 / 2400   loss: 0.042747106\n",
      "iteration: 623 / 2400   loss: 0.050147187\n",
      "iteration: 624 / 2400   loss: 0.07712345\n",
      "iteration: 625 / 2400   loss: 0.19940723\n",
      "iteration: 626 / 2400   loss: 0.08130087\n",
      "iteration: 627 / 2400   loss: 0.045984898\n",
      "iteration: 628 / 2400   loss: 0.103821754\n",
      "iteration: 629 / 2400   loss: 0.18728039\n",
      "iteration: 630 / 2400   loss: 0.30129504\n",
      "iteration: 631 / 2400   loss: 0.35124007\n",
      "iteration: 632 / 2400   loss: 0.190628\n",
      "iteration: 633 / 2400   loss: 0.04087826\n",
      "iteration: 634 / 2400   loss: 0.3840865\n",
      "iteration: 635 / 2400   loss: 0.108450085\n",
      "iteration: 636 / 2400   loss: 0.35520148\n",
      "iteration: 637 / 2400   loss: 0.38475093\n",
      "iteration: 638 / 2400   loss: 0.16258183\n",
      "iteration: 639 / 2400   loss: 0.16353054\n",
      "iteration: 640 / 2400   loss: 0.28271028\n",
      "iteration: 641 / 2400   loss: 0.10574612\n",
      "iteration: 642 / 2400   loss: 0.40931356\n",
      "iteration: 643 / 2400   loss: 0.19506855\n",
      "iteration: 644 / 2400   loss: 0.014149151\n",
      "iteration: 645 / 2400   loss: 0.11412038\n",
      "iteration: 646 / 2400   loss: 0.28827024\n",
      "iteration: 647 / 2400   loss: 0.07231569\n",
      "iteration: 648 / 2400   loss: 0.34291047\n",
      "iteration: 649 / 2400   loss: 0.10956375\n",
      "iteration: 650 / 2400   loss: 0.23709531\n",
      "iteration: 651 / 2400   loss: 0.05758897\n",
      "iteration: 652 / 2400   loss: 0.44764084\n",
      "iteration: 653 / 2400   loss: 0.02253521\n",
      "iteration: 654 / 2400   loss: 0.012793827\n",
      "iteration: 655 / 2400   loss: 0.05536475\n",
      "iteration: 656 / 2400   loss: 0.617785\n",
      "iteration: 657 / 2400   loss: 0.031032057\n",
      "iteration: 658 / 2400   loss: 0.17047462\n",
      "iteration: 659 / 2400   loss: 0.14321098\n",
      "iteration: 660 / 2400   loss: 0.26542827\n",
      "iteration: 661 / 2400   loss: 0.044101905\n",
      "iteration: 662 / 2400   loss: 0.08657061\n",
      "iteration: 663 / 2400   loss: 0.09954394\n",
      "iteration: 664 / 2400   loss: 0.05308034\n",
      "iteration: 665 / 2400   loss: 0.02189744\n",
      "iteration: 666 / 2400   loss: 0.20885216\n",
      "iteration: 667 / 2400   loss: 0.21826626\n",
      "iteration: 668 / 2400   loss: 0.63090265\n",
      "iteration: 669 / 2400   loss: 0.094155215\n",
      "iteration: 670 / 2400   loss: 0.05065795\n",
      "iteration: 671 / 2400   loss: 0.26319277\n",
      "iteration: 672 / 2400   loss: 0.2213635\n",
      "iteration: 673 / 2400   loss: 0.06076949\n",
      "iteration: 674 / 2400   loss: 0.07132392\n",
      "iteration: 675 / 2400   loss: 0.062481526\n",
      "iteration: 676 / 2400   loss: 0.15926094\n",
      "iteration: 677 / 2400   loss: 0.058626223\n",
      "iteration: 678 / 2400   loss: 0.1493887\n",
      "iteration: 679 / 2400   loss: 0.18971525\n",
      "iteration: 680 / 2400   loss: 0.12447123\n",
      "iteration: 681 / 2400   loss: 0.14694616\n",
      "iteration: 682 / 2400   loss: 0.3014816\n",
      "iteration: 683 / 2400   loss: 0.1929614\n",
      "iteration: 684 / 2400   loss: 0.1987161\n",
      "iteration: 685 / 2400   loss: 0.28196576\n",
      "iteration: 686 / 2400   loss: 0.31764966\n",
      "iteration: 687 / 2400   loss: 0.010656586\n",
      "iteration: 688 / 2400   loss: 0.31701282\n",
      "iteration: 689 / 2400   loss: 0.47482896\n",
      "iteration: 690 / 2400   loss: 0.15755056\n",
      "iteration: 691 / 2400   loss: 0.02112606\n",
      "iteration: 692 / 2400   loss: 0.15498443\n",
      "iteration: 693 / 2400   loss: 0.035133783\n",
      "iteration: 694 / 2400   loss: 0.013524285\n",
      "iteration: 695 / 2400   loss: 0.10101755\n",
      "iteration: 696 / 2400   loss: 0.19820954\n",
      "iteration: 697 / 2400   loss: 0.20751178\n",
      "iteration: 698 / 2400   loss: 0.08329866\n",
      "iteration: 699 / 2400   loss: 0.049787186\n",
      "iteration: 700 / 2400   loss: 0.18962677\n",
      "iteration: 701 / 2400   loss: 0.19338493\n",
      "iteration: 702 / 2400   loss: 0.6503735\n",
      "iteration: 703 / 2400   loss: 0.20049463\n",
      "iteration: 704 / 2400   loss: 0.68725723\n",
      "iteration: 705 / 2400   loss: 0.05771226\n",
      "iteration: 706 / 2400   loss: 0.11970295\n",
      "iteration: 707 / 2400   loss: 0.12675186\n",
      "iteration: 708 / 2400   loss: 0.34483948\n",
      "iteration: 709 / 2400   loss: 0.16183335\n",
      "iteration: 710 / 2400   loss: 0.2397324\n",
      "iteration: 711 / 2400   loss: 0.73463637\n",
      "iteration: 712 / 2400   loss: 0.32670543\n",
      "iteration: 713 / 2400   loss: 0.34931755\n",
      "iteration: 714 / 2400   loss: 0.13023818\n",
      "iteration: 715 / 2400   loss: 0.083609104\n",
      "iteration: 716 / 2400   loss: 0.36920333\n",
      "iteration: 717 / 2400   loss: 0.3593721\n",
      "iteration: 718 / 2400   loss: 0.042716686\n",
      "iteration: 719 / 2400   loss: 0.20593731\n",
      "iteration: 720 / 2400   loss: 0.045032006\n",
      "iteration: 721 / 2400   loss: 0.1893449\n",
      "iteration: 722 / 2400   loss: 0.22178644\n",
      "iteration: 723 / 2400   loss: 0.16571605\n",
      "iteration: 724 / 2400   loss: 0.07911257\n",
      "iteration: 725 / 2400   loss: 0.15878886\n",
      "iteration: 726 / 2400   loss: 0.051681947\n",
      "iteration: 727 / 2400   loss: 0.21040052\n",
      "iteration: 728 / 2400   loss: 0.03102767\n",
      "iteration: 729 / 2400   loss: 0.09833519\n",
      "iteration: 730 / 2400   loss: 0.18381505\n",
      "iteration: 731 / 2400   loss: 0.018177776\n",
      "iteration: 732 / 2400   loss: 0.08464023\n",
      "iteration: 733 / 2400   loss: 0.04177868\n",
      "iteration: 734 / 2400   loss: 0.23280233\n",
      "iteration: 735 / 2400   loss: 0.030995674\n",
      "iteration: 736 / 2400   loss: 0.2939106\n",
      "iteration: 737 / 2400   loss: 0.42811787\n",
      "iteration: 738 / 2400   loss: 0.10701234\n",
      "iteration: 739 / 2400   loss: 0.08199503\n",
      "iteration: 740 / 2400   loss: 0.091543816\n",
      "iteration: 741 / 2400   loss: 0.15966293\n",
      "iteration: 742 / 2400   loss: 0.016584206\n",
      "iteration: 743 / 2400   loss: 0.06610413\n",
      "iteration: 744 / 2400   loss: 0.48172694\n",
      "iteration: 745 / 2400   loss: 0.02963852\n",
      "iteration: 746 / 2400   loss: 0.035214216\n",
      "iteration: 747 / 2400   loss: 0.07411981\n",
      "iteration: 748 / 2400   loss: 0.28156763\n",
      "iteration: 749 / 2400   loss: 0.21409827\n",
      "iteration: 750 / 2400   loss: 0.12005086\n",
      "iteration: 751 / 2400   loss: 0.15586491\n",
      "iteration: 752 / 2400   loss: 0.1393458\n",
      "iteration: 753 / 2400   loss: 0.0526895\n",
      "iteration: 754 / 2400   loss: 0.13138588\n",
      "iteration: 755 / 2400   loss: 0.15254575\n",
      "iteration: 756 / 2400   loss: 0.013227081\n",
      "iteration: 757 / 2400   loss: 0.032505084\n",
      "iteration: 758 / 2400   loss: 0.06780371\n",
      "iteration: 759 / 2400   loss: 0.015856838\n",
      "iteration: 760 / 2400   loss: 0.08584252\n",
      "iteration: 761 / 2400   loss: 0.043241274\n",
      "iteration: 762 / 2400   loss: 0.07458992\n",
      "iteration: 763 / 2400   loss: 0.06196371\n",
      "iteration: 764 / 2400   loss: 0.22565162\n",
      "iteration: 765 / 2400   loss: 0.22324428\n",
      "iteration: 766 / 2400   loss: 0.063865386\n",
      "iteration: 767 / 2400   loss: 0.11865896\n",
      "iteration: 768 / 2400   loss: 0.46789432\n",
      "iteration: 769 / 2400   loss: 0.066561535\n",
      "iteration: 770 / 2400   loss: 0.2618243\n",
      "iteration: 771 / 2400   loss: 0.22463574\n",
      "iteration: 772 / 2400   loss: 0.11432525\n",
      "iteration: 773 / 2400   loss: 0.09767578\n",
      "iteration: 774 / 2400   loss: 0.14635526\n",
      "iteration: 775 / 2400   loss: 0.34128746\n",
      "iteration: 776 / 2400   loss: 0.06136799\n",
      "iteration: 777 / 2400   loss: 0.098985195\n",
      "iteration: 778 / 2400   loss: 0.09004897\n",
      "iteration: 779 / 2400   loss: 0.056946345\n",
      "iteration: 780 / 2400   loss: 0.29089016\n",
      "iteration: 781 / 2400   loss: 0.13131091\n",
      "iteration: 782 / 2400   loss: 0.18661089\n",
      "iteration: 783 / 2400   loss: 0.13255131\n",
      "iteration: 784 / 2400   loss: 0.2519732\n",
      "iteration: 785 / 2400   loss: 0.08311976\n",
      "iteration: 786 / 2400   loss: 0.01667576\n",
      "iteration: 787 / 2400   loss: 0.032502424\n",
      "iteration: 788 / 2400   loss: 0.033986036\n",
      "iteration: 789 / 2400   loss: 0.035319794\n",
      "iteration: 790 / 2400   loss: 0.1083843\n",
      "iteration: 791 / 2400   loss: 0.015610027\n",
      "iteration: 792 / 2400   loss: 0.11001148\n",
      "iteration: 793 / 2400   loss: 0.4298333\n",
      "iteration: 794 / 2400   loss: 0.060101174\n",
      "iteration: 795 / 2400   loss: 0.09720071\n",
      "iteration: 796 / 2400   loss: 0.15056302\n",
      "iteration: 797 / 2400   loss: 0.14287205\n",
      "iteration: 798 / 2400   loss: 0.39618596\n",
      "iteration: 799 / 2400   loss: 0.04597701\n",
      "iteration: 800 / 2400   loss: 0.02194725\n",
      "iteration: 801 / 2400   loss: 0.23308407\n",
      "iteration: 802 / 2400   loss: 0.29328775\n",
      "iteration: 803 / 2400   loss: 0.38491952\n",
      "iteration: 804 / 2400   loss: 0.12458992\n",
      "iteration: 805 / 2400   loss: 0.09374754\n",
      "iteration: 806 / 2400   loss: 0.021696834\n",
      "iteration: 807 / 2400   loss: 0.23737061\n",
      "iteration: 808 / 2400   loss: 0.064133234\n",
      "iteration: 809 / 2400   loss: 0.084509544\n",
      "iteration: 810 / 2400   loss: 0.21581739\n",
      "iteration: 811 / 2400   loss: 0.2501523\n",
      "iteration: 812 / 2400   loss: 0.2046172\n",
      "iteration: 813 / 2400   loss: 0.32315195\n",
      "iteration: 814 / 2400   loss: 0.026591796\n",
      "iteration: 815 / 2400   loss: 0.5048515\n",
      "iteration: 816 / 2400   loss: 0.22133413\n",
      "iteration: 817 / 2400   loss: 0.07634772\n",
      "iteration: 818 / 2400   loss: 0.08503292\n",
      "iteration: 819 / 2400   loss: 0.083322614\n",
      "iteration: 820 / 2400   loss: 0.021672811\n",
      "iteration: 821 / 2400   loss: 0.07280516\n",
      "iteration: 822 / 2400   loss: 0.20649844\n",
      "iteration: 823 / 2400   loss: 0.30640107\n",
      "iteration: 824 / 2400   loss: 0.04747881\n",
      "iteration: 825 / 2400   loss: 0.075584956\n",
      "iteration: 826 / 2400   loss: 0.20069546\n",
      "iteration: 827 / 2400   loss: 0.64235055\n",
      "iteration: 828 / 2400   loss: 0.06096305\n",
      "iteration: 829 / 2400   loss: 0.25636968\n",
      "iteration: 830 / 2400   loss: 0.09784062\n",
      "iteration: 831 / 2400   loss: 0.31618145\n",
      "iteration: 832 / 2400   loss: 0.43448326\n",
      "iteration: 833 / 2400   loss: 0.11123333\n",
      "iteration: 834 / 2400   loss: 0.101397075\n",
      "iteration: 835 / 2400   loss: 0.2946837\n",
      "iteration: 836 / 2400   loss: 0.10808577\n",
      "iteration: 837 / 2400   loss: 0.13298577\n",
      "iteration: 838 / 2400   loss: 0.14962621\n",
      "iteration: 839 / 2400   loss: 0.2670938\n",
      "iteration: 840 / 2400   loss: 0.12230409\n",
      "iteration: 841 / 2400   loss: 0.23803815\n",
      "iteration: 842 / 2400   loss: 0.13841842\n",
      "iteration: 843 / 2400   loss: 0.17509468\n",
      "iteration: 844 / 2400   loss: 0.031482086\n",
      "iteration: 845 / 2400   loss: 0.030807046\n",
      "iteration: 846 / 2400   loss: 0.21115929\n",
      "iteration: 847 / 2400   loss: 0.079812914\n",
      "iteration: 848 / 2400   loss: 0.015438118\n",
      "iteration: 849 / 2400   loss: 0.039944723\n",
      "iteration: 850 / 2400   loss: 0.019266795\n",
      "iteration: 851 / 2400   loss: 0.029624652\n",
      "iteration: 852 / 2400   loss: 0.07044297\n",
      "iteration: 853 / 2400   loss: 0.2057914\n",
      "iteration: 854 / 2400   loss: 0.28785497\n",
      "iteration: 855 / 2400   loss: 0.43311524\n",
      "iteration: 856 / 2400   loss: 0.08057631\n",
      "iteration: 857 / 2400   loss: 0.1528983\n",
      "iteration: 858 / 2400   loss: 0.1523802\n",
      "iteration: 859 / 2400   loss: 0.1234089\n",
      "iteration: 860 / 2400   loss: 0.0553831\n",
      "iteration: 861 / 2400   loss: 0.014645156\n",
      "iteration: 862 / 2400   loss: 0.07089122\n",
      "iteration: 863 / 2400   loss: 0.083290614\n",
      "iteration: 864 / 2400   loss: 0.25355422\n",
      "iteration: 865 / 2400   loss: 0.13640814\n",
      "iteration: 866 / 2400   loss: 0.03030325\n",
      "iteration: 867 / 2400   loss: 0.050497007\n",
      "iteration: 868 / 2400   loss: 0.065548345\n",
      "iteration: 869 / 2400   loss: 0.16035968\n",
      "iteration: 870 / 2400   loss: 0.047755547\n",
      "iteration: 871 / 2400   loss: 0.0823389\n",
      "iteration: 872 / 2400   loss: 0.018489175\n",
      "iteration: 873 / 2400   loss: 0.018920727\n",
      "iteration: 874 / 2400   loss: 0.028551636\n",
      "iteration: 875 / 2400   loss: 0.040770873\n",
      "iteration: 876 / 2400   loss: 0.08299225\n",
      "iteration: 877 / 2400   loss: 0.036110714\n",
      "iteration: 878 / 2400   loss: 0.17866112\n",
      "iteration: 879 / 2400   loss: 0.03187146\n",
      "iteration: 880 / 2400   loss: 0.05258501\n",
      "iteration: 881 / 2400   loss: 0.01583538\n",
      "iteration: 882 / 2400   loss: 0.019414416\n",
      "iteration: 883 / 2400   loss: 0.07962275\n",
      "iteration: 884 / 2400   loss: 0.16422035\n",
      "iteration: 885 / 2400   loss: 0.20508419\n",
      "iteration: 886 / 2400   loss: 0.34502074\n",
      "iteration: 887 / 2400   loss: 0.1828417\n",
      "iteration: 888 / 2400   loss: 0.1184363\n",
      "iteration: 889 / 2400   loss: 0.22744994\n",
      "iteration: 890 / 2400   loss: 0.07190441\n",
      "iteration: 891 / 2400   loss: 0.13340367\n",
      "iteration: 892 / 2400   loss: 0.07409148\n",
      "iteration: 893 / 2400   loss: 0.18896575\n",
      "iteration: 894 / 2400   loss: 0.025443267\n",
      "iteration: 895 / 2400   loss: 0.020116653\n",
      "iteration: 896 / 2400   loss: 0.03293354\n",
      "iteration: 897 / 2400   loss: 0.047636107\n",
      "iteration: 898 / 2400   loss: 0.2725191\n",
      "iteration: 899 / 2400   loss: 0.052850734\n",
      "iteration: 900 / 2400   loss: 0.3036379\n",
      "iteration: 901 / 2400   loss: 0.111641824\n",
      "iteration: 902 / 2400   loss: 0.8468312\n",
      "iteration: 903 / 2400   loss: 0.18271858\n",
      "iteration: 904 / 2400   loss: 0.21608616\n",
      "iteration: 905 / 2400   loss: 0.19677211\n",
      "iteration: 906 / 2400   loss: 0.38703537\n",
      "iteration: 907 / 2400   loss: 0.059487\n",
      "iteration: 908 / 2400   loss: 0.10069288\n",
      "iteration: 909 / 2400   loss: 0.3789042\n",
      "iteration: 910 / 2400   loss: 0.3151458\n",
      "iteration: 911 / 2400   loss: 0.1565426\n",
      "iteration: 912 / 2400   loss: 0.2240604\n",
      "iteration: 913 / 2400   loss: 0.2138228\n",
      "iteration: 914 / 2400   loss: 0.26658142\n",
      "iteration: 915 / 2400   loss: 0.03145702\n",
      "iteration: 916 / 2400   loss: 0.2829623\n",
      "iteration: 917 / 2400   loss: 0.053348705\n",
      "iteration: 918 / 2400   loss: 0.11174577\n",
      "iteration: 919 / 2400   loss: 0.023690425\n",
      "iteration: 920 / 2400   loss: 0.10989313\n",
      "iteration: 921 / 2400   loss: 0.12469552\n",
      "iteration: 922 / 2400   loss: 0.25459498\n",
      "iteration: 923 / 2400   loss: 0.11867049\n",
      "iteration: 924 / 2400   loss: 0.20437042\n",
      "iteration: 925 / 2400   loss: 0.23867266\n",
      "iteration: 926 / 2400   loss: 0.06969735\n",
      "iteration: 927 / 2400   loss: 0.033845138\n",
      "iteration: 928 / 2400   loss: 0.21878965\n",
      "iteration: 929 / 2400   loss: 0.17840835\n",
      "iteration: 930 / 2400   loss: 0.016637659\n",
      "iteration: 931 / 2400   loss: 0.34075454\n",
      "iteration: 932 / 2400   loss: 0.015249367\n",
      "iteration: 933 / 2400   loss: 0.0649087\n",
      "iteration: 934 / 2400   loss: 0.06667634\n",
      "iteration: 935 / 2400   loss: 0.0727882\n",
      "iteration: 936 / 2400   loss: 0.13048604\n",
      "iteration: 937 / 2400   loss: 0.17722195\n",
      "iteration: 938 / 2400   loss: 0.07130454\n",
      "iteration: 939 / 2400   loss: 0.17563617\n",
      "iteration: 940 / 2400   loss: 0.030814324\n",
      "iteration: 941 / 2400   loss: 0.076727904\n",
      "iteration: 942 / 2400   loss: 0.08412573\n",
      "iteration: 943 / 2400   loss: 0.022931833\n",
      "iteration: 944 / 2400   loss: 0.07161075\n",
      "iteration: 945 / 2400   loss: 0.090634346\n",
      "iteration: 946 / 2400   loss: 0.04056407\n",
      "iteration: 947 / 2400   loss: 0.05655159\n",
      "iteration: 948 / 2400   loss: 0.051336784\n",
      "iteration: 949 / 2400   loss: 0.1391073\n",
      "iteration: 950 / 2400   loss: 0.6131699\n",
      "iteration: 951 / 2400   loss: 0.059952546\n",
      "iteration: 952 / 2400   loss: 0.07655436\n",
      "iteration: 953 / 2400   loss: 0.30898616\n",
      "iteration: 954 / 2400   loss: 0.051565617\n",
      "iteration: 955 / 2400   loss: 0.20422699\n",
      "iteration: 956 / 2400   loss: 0.06914286\n",
      "iteration: 957 / 2400   loss: 0.15811688\n",
      "iteration: 958 / 2400   loss: 0.052434083\n",
      "iteration: 959 / 2400   loss: 0.11621765\n",
      "iteration: 960 / 2400   loss: 0.11662312\n",
      "iteration: 961 / 2400   loss: 0.08384019\n",
      "iteration: 962 / 2400   loss: 0.27343893\n",
      "iteration: 963 / 2400   loss: 0.28004044\n",
      "iteration: 964 / 2400   loss: 0.13961263\n",
      "iteration: 965 / 2400   loss: 0.07950178\n",
      "iteration: 966 / 2400   loss: 0.118248895\n",
      "iteration: 967 / 2400   loss: 0.22438323\n",
      "iteration: 968 / 2400   loss: 0.12736383\n",
      "iteration: 969 / 2400   loss: 0.1446907\n",
      "iteration: 970 / 2400   loss: 0.11961205\n",
      "iteration: 971 / 2400   loss: 0.17872593\n",
      "iteration: 972 / 2400   loss: 0.05384077\n",
      "iteration: 973 / 2400   loss: 0.17075934\n",
      "iteration: 974 / 2400   loss: 0.027737312\n",
      "iteration: 975 / 2400   loss: 0.071466275\n",
      "iteration: 976 / 2400   loss: 0.01495037\n",
      "iteration: 977 / 2400   loss: 0.1390805\n",
      "iteration: 978 / 2400   loss: 0.028778553\n",
      "iteration: 979 / 2400   loss: 0.1057354\n",
      "iteration: 980 / 2400   loss: 0.10243437\n",
      "iteration: 981 / 2400   loss: 0.23212372\n",
      "iteration: 982 / 2400   loss: 0.13455655\n",
      "iteration: 983 / 2400   loss: 0.09329312\n",
      "iteration: 984 / 2400   loss: 0.6028496\n",
      "iteration: 985 / 2400   loss: 0.20844086\n",
      "iteration: 986 / 2400   loss: 0.36176297\n",
      "iteration: 987 / 2400   loss: 0.16034141\n",
      "iteration: 988 / 2400   loss: 0.192871\n",
      "iteration: 989 / 2400   loss: 0.08496843\n",
      "iteration: 990 / 2400   loss: 0.14714216\n",
      "iteration: 991 / 2400   loss: 0.101045646\n",
      "iteration: 992 / 2400   loss: 0.51115805\n",
      "iteration: 993 / 2400   loss: 0.024883175\n",
      "iteration: 994 / 2400   loss: 0.12991445\n",
      "iteration: 995 / 2400   loss: 0.21776405\n",
      "iteration: 996 / 2400   loss: 0.06850781\n",
      "iteration: 997 / 2400   loss: 0.030631753\n",
      "iteration: 998 / 2400   loss: 0.18315281\n",
      "iteration: 999 / 2400   loss: 0.054315418\n",
      "iteration: 1000 / 2400   loss: 0.0746851\n",
      "iteration: 1001 / 2400   loss: 0.026724204\n",
      "iteration: 1002 / 2400   loss: 0.3146325\n",
      "iteration: 1003 / 2400   loss: 0.103479765\n",
      "iteration: 1004 / 2400   loss: 0.057323966\n",
      "iteration: 1005 / 2400   loss: 0.020262737\n",
      "iteration: 1006 / 2400   loss: 0.06481546\n",
      "iteration: 1007 / 2400   loss: 0.1224274\n",
      "iteration: 1008 / 2400   loss: 0.055693902\n",
      "iteration: 1009 / 2400   loss: 0.363212\n",
      "iteration: 1010 / 2400   loss: 0.22583343\n",
      "iteration: 1011 / 2400   loss: 0.10088502\n",
      "iteration: 1012 / 2400   loss: 0.25426632\n",
      "iteration: 1013 / 2400   loss: 0.280868\n",
      "iteration: 1014 / 2400   loss: 0.06829302\n",
      "iteration: 1015 / 2400   loss: 0.033154268\n",
      "iteration: 1016 / 2400   loss: 0.0078076744\n",
      "iteration: 1017 / 2400   loss: 0.09149833\n",
      "iteration: 1018 / 2400   loss: 0.03992094\n",
      "iteration: 1019 / 2400   loss: 0.0625427\n",
      "iteration: 1020 / 2400   loss: 0.23710307\n",
      "iteration: 1021 / 2400   loss: 0.03966772\n",
      "iteration: 1022 / 2400   loss: 0.2816334\n",
      "iteration: 1023 / 2400   loss: 0.13980827\n",
      "iteration: 1024 / 2400   loss: 0.03607111\n",
      "iteration: 1025 / 2400   loss: 0.084867135\n",
      "iteration: 1026 / 2400   loss: 0.05883355\n",
      "iteration: 1027 / 2400   loss: 0.040959932\n",
      "iteration: 1028 / 2400   loss: 0.28664294\n",
      "iteration: 1029 / 2400   loss: 0.10055977\n",
      "iteration: 1030 / 2400   loss: 0.066947795\n",
      "iteration: 1031 / 2400   loss: 0.05639805\n",
      "iteration: 1032 / 2400   loss: 0.28314933\n",
      "iteration: 1033 / 2400   loss: 0.26224172\n",
      "iteration: 1034 / 2400   loss: 0.101826064\n",
      "iteration: 1035 / 2400   loss: 0.08792632\n",
      "iteration: 1036 / 2400   loss: 0.0873904\n",
      "iteration: 1037 / 2400   loss: 0.30278707\n",
      "iteration: 1038 / 2400   loss: 0.119325064\n",
      "iteration: 1039 / 2400   loss: 0.12876002\n",
      "iteration: 1040 / 2400   loss: 0.090474345\n",
      "iteration: 1041 / 2400   loss: 0.03375397\n",
      "iteration: 1042 / 2400   loss: 0.05589057\n",
      "iteration: 1043 / 2400   loss: 0.089720495\n",
      "iteration: 1044 / 2400   loss: 0.08307884\n",
      "iteration: 1045 / 2400   loss: 0.0337951\n",
      "iteration: 1046 / 2400   loss: 0.052640267\n",
      "iteration: 1047 / 2400   loss: 0.09968513\n",
      "iteration: 1048 / 2400   loss: 0.016144313\n",
      "iteration: 1049 / 2400   loss: 0.27384388\n",
      "iteration: 1050 / 2400   loss: 0.024072485\n",
      "iteration: 1051 / 2400   loss: 0.045331553\n",
      "iteration: 1052 / 2400   loss: 0.10157117\n",
      "iteration: 1053 / 2400   loss: 0.26444092\n",
      "iteration: 1054 / 2400   loss: 0.075257026\n",
      "iteration: 1055 / 2400   loss: 0.25781655\n",
      "iteration: 1056 / 2400   loss: 0.50388914\n",
      "iteration: 1057 / 2400   loss: 0.09715633\n",
      "iteration: 1058 / 2400   loss: 0.11994828\n",
      "iteration: 1059 / 2400   loss: 0.12548634\n",
      "iteration: 1060 / 2400   loss: 0.17466095\n",
      "iteration: 1061 / 2400   loss: 0.46357623\n",
      "iteration: 1062 / 2400   loss: 0.11747239\n",
      "iteration: 1063 / 2400   loss: 0.28449398\n",
      "iteration: 1064 / 2400   loss: 0.10218034\n",
      "iteration: 1065 / 2400   loss: 0.24157366\n",
      "iteration: 1066 / 2400   loss: 0.35083893\n",
      "iteration: 1067 / 2400   loss: 0.045508202\n",
      "iteration: 1068 / 2400   loss: 0.0628823\n",
      "iteration: 1069 / 2400   loss: 0.18779507\n",
      "iteration: 1070 / 2400   loss: 0.2863464\n",
      "iteration: 1071 / 2400   loss: 0.39744836\n",
      "iteration: 1072 / 2400   loss: 0.101799704\n",
      "iteration: 1073 / 2400   loss: 0.063638374\n",
      "iteration: 1074 / 2400   loss: 0.11406348\n",
      "iteration: 1075 / 2400   loss: 0.28932273\n",
      "iteration: 1076 / 2400   loss: 0.033884928\n",
      "iteration: 1077 / 2400   loss: 0.030357562\n",
      "iteration: 1078 / 2400   loss: 0.116367444\n",
      "iteration: 1079 / 2400   loss: 0.04593484\n",
      "iteration: 1080 / 2400   loss: 0.039287556\n",
      "iteration: 1081 / 2400   loss: 0.14997521\n",
      "iteration: 1082 / 2400   loss: 0.084200755\n",
      "iteration: 1083 / 2400   loss: 0.12301965\n",
      "iteration: 1084 / 2400   loss: 0.21375072\n",
      "iteration: 1085 / 2400   loss: 0.14780918\n",
      "iteration: 1086 / 2400   loss: 0.037902955\n",
      "iteration: 1087 / 2400   loss: 0.19285484\n",
      "iteration: 1088 / 2400   loss: 0.7349124\n",
      "iteration: 1089 / 2400   loss: 0.38659447\n",
      "iteration: 1090 / 2400   loss: 0.20803732\n",
      "iteration: 1091 / 2400   loss: 0.1439978\n",
      "iteration: 1092 / 2400   loss: 0.21594755\n",
      "iteration: 1093 / 2400   loss: 0.09168038\n",
      "iteration: 1094 / 2400   loss: 0.09183393\n",
      "iteration: 1095 / 2400   loss: 0.37051606\n",
      "iteration: 1096 / 2400   loss: 0.031986102\n",
      "iteration: 1097 / 2400   loss: 0.080842026\n",
      "iteration: 1098 / 2400   loss: 0.14243329\n",
      "iteration: 1099 / 2400   loss: 0.51114726\n",
      "iteration: 1100 / 2400   loss: 0.0928708\n",
      "iteration: 1101 / 2400   loss: 0.7011356\n",
      "iteration: 1102 / 2400   loss: 0.13535938\n",
      "iteration: 1103 / 2400   loss: 0.059714362\n",
      "iteration: 1104 / 2400   loss: 0.44765297\n",
      "iteration: 1105 / 2400   loss: 0.20834133\n",
      "iteration: 1106 / 2400   loss: 0.08272061\n",
      "iteration: 1107 / 2400   loss: 0.16344486\n",
      "iteration: 1108 / 2400   loss: 0.2580117\n",
      "iteration: 1109 / 2400   loss: 0.13394794\n",
      "iteration: 1110 / 2400   loss: 0.28015378\n",
      "iteration: 1111 / 2400   loss: 0.14593409\n",
      "iteration: 1112 / 2400   loss: 0.13816902\n",
      "iteration: 1113 / 2400   loss: 0.23088932\n",
      "iteration: 1114 / 2400   loss: 0.22904159\n",
      "iteration: 1115 / 2400   loss: 0.052020244\n",
      "iteration: 1116 / 2400   loss: 0.115182206\n",
      "iteration: 1117 / 2400   loss: 0.10576499\n",
      "iteration: 1118 / 2400   loss: 0.078172415\n",
      "iteration: 1119 / 2400   loss: 0.15656742\n",
      "iteration: 1120 / 2400   loss: 0.11559875\n",
      "iteration: 1121 / 2400   loss: 0.38262147\n",
      "iteration: 1122 / 2400   loss: 0.015334549\n",
      "iteration: 1123 / 2400   loss: 0.039164208\n",
      "iteration: 1124 / 2400   loss: 0.07532658\n",
      "iteration: 1125 / 2400   loss: 0.0516158\n",
      "iteration: 1126 / 2400   loss: 0.08087911\n",
      "iteration: 1127 / 2400   loss: 0.24754629\n",
      "iteration: 1128 / 2400   loss: 0.19801739\n",
      "iteration: 1129 / 2400   loss: 0.027025748\n",
      "iteration: 1130 / 2400   loss: 0.055545654\n",
      "iteration: 1131 / 2400   loss: 0.053362057\n",
      "iteration: 1132 / 2400   loss: 0.069669984\n",
      "iteration: 1133 / 2400   loss: 0.12837675\n",
      "iteration: 1134 / 2400   loss: 0.05318858\n",
      "iteration: 1135 / 2400   loss: 0.4023995\n",
      "iteration: 1136 / 2400   loss: 0.3352672\n",
      "iteration: 1137 / 2400   loss: 0.44124907\n",
      "iteration: 1138 / 2400   loss: 0.012848244\n",
      "iteration: 1139 / 2400   loss: 0.15666361\n",
      "iteration: 1140 / 2400   loss: 0.055731878\n",
      "iteration: 1141 / 2400   loss: 0.23201999\n",
      "iteration: 1142 / 2400   loss: 0.13582984\n",
      "iteration: 1143 / 2400   loss: 0.11236494\n",
      "iteration: 1144 / 2400   loss: 0.12086166\n",
      "iteration: 1145 / 2400   loss: 0.05406475\n",
      "iteration: 1146 / 2400   loss: 0.17685746\n",
      "iteration: 1147 / 2400   loss: 0.43079895\n",
      "iteration: 1148 / 2400   loss: 0.06794605\n",
      "iteration: 1149 / 2400   loss: 0.3032789\n",
      "iteration: 1150 / 2400   loss: 0.040100336\n",
      "iteration: 1151 / 2400   loss: 0.02815466\n",
      "iteration: 1152 / 2400   loss: 0.18045597\n",
      "iteration: 1153 / 2400   loss: 0.08713209\n",
      "iteration: 1154 / 2400   loss: 0.19821712\n",
      "iteration: 1155 / 2400   loss: 0.03907922\n",
      "iteration: 1156 / 2400   loss: 0.0992352\n",
      "iteration: 1157 / 2400   loss: 0.014118414\n",
      "iteration: 1158 / 2400   loss: 0.05954943\n",
      "iteration: 1159 / 2400   loss: 0.085877955\n",
      "iteration: 1160 / 2400   loss: 0.2894001\n",
      "iteration: 1161 / 2400   loss: 0.15136774\n",
      "iteration: 1162 / 2400   loss: 0.17163986\n",
      "iteration: 1163 / 2400   loss: 0.35367295\n",
      "iteration: 1164 / 2400   loss: 0.07189164\n",
      "iteration: 1165 / 2400   loss: 0.38019615\n",
      "iteration: 1166 / 2400   loss: 0.059131745\n",
      "iteration: 1167 / 2400   loss: 0.052657414\n",
      "iteration: 1168 / 2400   loss: 0.2893225\n",
      "iteration: 1169 / 2400   loss: 0.14051308\n",
      "iteration: 1170 / 2400   loss: 0.1105\n",
      "iteration: 1171 / 2400   loss: 0.026521102\n",
      "iteration: 1172 / 2400   loss: 0.30117923\n",
      "iteration: 1173 / 2400   loss: 0.2286548\n",
      "iteration: 1174 / 2400   loss: 0.12591644\n",
      "iteration: 1175 / 2400   loss: 0.10644159\n",
      "iteration: 1176 / 2400   loss: 0.12605757\n",
      "iteration: 1177 / 2400   loss: 0.11367324\n",
      "iteration: 1178 / 2400   loss: 0.4490148\n",
      "iteration: 1179 / 2400   loss: 0.25120607\n",
      "iteration: 1180 / 2400   loss: 0.10597815\n",
      "iteration: 1181 / 2400   loss: 0.12098958\n",
      "iteration: 1182 / 2400   loss: 0.092920095\n",
      "iteration: 1183 / 2400   loss: 0.20692837\n",
      "iteration: 1184 / 2400   loss: 0.2659065\n",
      "iteration: 1185 / 2400   loss: 0.122474134\n",
      "iteration: 1186 / 2400   loss: 0.12704833\n",
      "iteration: 1187 / 2400   loss: 0.2569782\n",
      "iteration: 1188 / 2400   loss: 0.027735958\n",
      "iteration: 1189 / 2400   loss: 0.5064156\n",
      "iteration: 1190 / 2400   loss: 0.2151651\n",
      "iteration: 1191 / 2400   loss: 0.2844792\n",
      "iteration: 1192 / 2400   loss: 0.057588004\n",
      "iteration: 1193 / 2400   loss: 0.06411373\n",
      "iteration: 1194 / 2400   loss: 0.34390563\n",
      "iteration: 1195 / 2400   loss: 0.19155689\n",
      "iteration: 1196 / 2400   loss: 0.27939633\n",
      "iteration: 1197 / 2400   loss: 0.37433538\n",
      "iteration: 1198 / 2400   loss: 0.14465311\n",
      "iteration: 1199 / 2400   loss: 0.14337324\n",
      "iteration: 1200 / 2400   loss: 0.06946373\n",
      "iteration: 1201 / 2400   loss: 0.13853706\n",
      "iteration: 1202 / 2400   loss: 0.2375115\n",
      "iteration: 1203 / 2400   loss: 0.12744656\n",
      "iteration: 1204 / 2400   loss: 0.16309308\n",
      "iteration: 1205 / 2400   loss: 0.2065111\n",
      "iteration: 1206 / 2400   loss: 0.22369786\n",
      "iteration: 1207 / 2400   loss: 0.11466926\n",
      "iteration: 1208 / 2400   loss: 0.3083115\n",
      "iteration: 1209 / 2400   loss: 0.0673356\n",
      "iteration: 1210 / 2400   loss: 0.046890955\n",
      "iteration: 1211 / 2400   loss: 0.051159363\n",
      "iteration: 1212 / 2400   loss: 0.054972142\n",
      "iteration: 1213 / 2400   loss: 0.13026571\n",
      "iteration: 1214 / 2400   loss: 0.28526416\n",
      "iteration: 1215 / 2400   loss: 0.16905068\n",
      "iteration: 1216 / 2400   loss: 0.013216934\n",
      "iteration: 1217 / 2400   loss: 0.025712261\n",
      "iteration: 1218 / 2400   loss: 0.14365052\n",
      "iteration: 1219 / 2400   loss: 0.43521476\n",
      "iteration: 1220 / 2400   loss: 0.13316041\n",
      "iteration: 1221 / 2400   loss: 0.19131012\n",
      "iteration: 1222 / 2400   loss: 0.061286718\n",
      "iteration: 1223 / 2400   loss: 0.28552878\n",
      "iteration: 1224 / 2400   loss: 0.11326656\n",
      "iteration: 1225 / 2400   loss: 0.23296174\n",
      "iteration: 1226 / 2400   loss: 0.11865569\n",
      "iteration: 1227 / 2400   loss: 0.2599625\n",
      "iteration: 1228 / 2400   loss: 0.15337273\n",
      "iteration: 1229 / 2400   loss: 0.056461364\n",
      "iteration: 1230 / 2400   loss: 0.060185824\n",
      "iteration: 1231 / 2400   loss: 0.16611378\n",
      "iteration: 1232 / 2400   loss: 0.20217489\n",
      "iteration: 1233 / 2400   loss: 0.11471516\n",
      "iteration: 1234 / 2400   loss: 0.14573018\n",
      "iteration: 1235 / 2400   loss: 0.19407165\n",
      "iteration: 1236 / 2400   loss: 0.27975774\n",
      "iteration: 1237 / 2400   loss: 0.14011069\n",
      "iteration: 1238 / 2400   loss: 0.056454953\n",
      "iteration: 1239 / 2400   loss: 0.27092034\n",
      "iteration: 1240 / 2400   loss: 0.0704999\n",
      "iteration: 1241 / 2400   loss: 0.10092259\n",
      "iteration: 1242 / 2400   loss: 0.062175035\n",
      "iteration: 1243 / 2400   loss: 0.17596689\n",
      "iteration: 1244 / 2400   loss: 0.028566752\n",
      "iteration: 1245 / 2400   loss: 0.036539286\n",
      "iteration: 1246 / 2400   loss: 0.27745193\n",
      "iteration: 1247 / 2400   loss: 0.14274575\n",
      "iteration: 1248 / 2400   loss: 0.5730146\n",
      "iteration: 1249 / 2400   loss: 0.13044748\n",
      "iteration: 1250 / 2400   loss: 0.06275951\n",
      "iteration: 1251 / 2400   loss: 0.38330755\n",
      "iteration: 1252 / 2400   loss: 0.082223214\n",
      "iteration: 1253 / 2400   loss: 0.3471685\n",
      "iteration: 1254 / 2400   loss: 0.43547475\n",
      "iteration: 1255 / 2400   loss: 0.107875325\n",
      "iteration: 1256 / 2400   loss: 0.22517878\n",
      "iteration: 1257 / 2400   loss: 0.076798\n",
      "iteration: 1258 / 2400   loss: 0.19298731\n",
      "iteration: 1259 / 2400   loss: 0.061870173\n",
      "iteration: 1260 / 2400   loss: 0.024757996\n",
      "iteration: 1261 / 2400   loss: 0.20168115\n",
      "iteration: 1262 / 2400   loss: 0.019318476\n",
      "iteration: 1263 / 2400   loss: 0.12379705\n",
      "iteration: 1264 / 2400   loss: 0.4205484\n",
      "iteration: 1265 / 2400   loss: 0.30055943\n",
      "iteration: 1266 / 2400   loss: 0.05690544\n",
      "iteration: 1267 / 2400   loss: 0.3647663\n",
      "iteration: 1268 / 2400   loss: 0.08005426\n",
      "iteration: 1269 / 2400   loss: 0.5605106\n",
      "iteration: 1270 / 2400   loss: 0.22301193\n",
      "iteration: 1271 / 2400   loss: 0.09455496\n",
      "iteration: 1272 / 2400   loss: 0.1662651\n",
      "iteration: 1273 / 2400   loss: 0.02997981\n",
      "iteration: 1274 / 2400   loss: 0.030935477\n",
      "iteration: 1275 / 2400   loss: 0.1369537\n",
      "iteration: 1276 / 2400   loss: 0.136097\n",
      "iteration: 1277 / 2400   loss: 0.052554455\n",
      "iteration: 1278 / 2400   loss: 0.1753693\n",
      "iteration: 1279 / 2400   loss: 0.44734037\n",
      "iteration: 1280 / 2400   loss: 0.025576573\n",
      "iteration: 1281 / 2400   loss: 0.18020357\n",
      "iteration: 1282 / 2400   loss: 0.023223752\n",
      "iteration: 1283 / 2400   loss: 0.22161354\n",
      "iteration: 1284 / 2400   loss: 0.01823862\n",
      "iteration: 1285 / 2400   loss: 0.2544798\n",
      "iteration: 1286 / 2400   loss: 0.086785324\n",
      "iteration: 1287 / 2400   loss: 0.1314891\n",
      "iteration: 1288 / 2400   loss: 0.20413508\n",
      "iteration: 1289 / 2400   loss: 0.15996282\n",
      "iteration: 1290 / 2400   loss: 0.14584737\n",
      "iteration: 1291 / 2400   loss: 0.48006877\n",
      "iteration: 1292 / 2400   loss: 0.29750085\n",
      "iteration: 1293 / 2400   loss: 0.11235295\n",
      "iteration: 1294 / 2400   loss: 0.24762622\n",
      "iteration: 1295 / 2400   loss: 0.15555643\n",
      "iteration: 1296 / 2400   loss: 0.08715969\n",
      "iteration: 1297 / 2400   loss: 0.3854825\n",
      "iteration: 1298 / 2400   loss: 0.27981323\n",
      "iteration: 1299 / 2400   loss: 0.1713567\n",
      "iteration: 1300 / 2400   loss: 0.2471214\n",
      "iteration: 1301 / 2400   loss: 0.15768388\n",
      "iteration: 1302 / 2400   loss: 0.057055034\n",
      "iteration: 1303 / 2400   loss: 0.349188\n",
      "iteration: 1304 / 2400   loss: 0.03009489\n",
      "iteration: 1305 / 2400   loss: 0.053304043\n",
      "iteration: 1306 / 2400   loss: 0.19552176\n",
      "iteration: 1307 / 2400   loss: 0.058645267\n",
      "iteration: 1308 / 2400   loss: 0.027263641\n",
      "iteration: 1309 / 2400   loss: 0.17414097\n",
      "iteration: 1310 / 2400   loss: 0.4421088\n",
      "iteration: 1311 / 2400   loss: 0.059078313\n",
      "iteration: 1312 / 2400   loss: 0.103631504\n",
      "iteration: 1313 / 2400   loss: 0.26683208\n",
      "iteration: 1314 / 2400   loss: 0.10154599\n",
      "iteration: 1315 / 2400   loss: 0.039670393\n",
      "iteration: 1316 / 2400   loss: 0.22680305\n",
      "iteration: 1317 / 2400   loss: 0.29387414\n",
      "iteration: 1318 / 2400   loss: 0.027513837\n",
      "iteration: 1319 / 2400   loss: 0.049536325\n",
      "iteration: 1320 / 2400   loss: 0.014924736\n",
      "iteration: 1321 / 2400   loss: 0.029499207\n",
      "iteration: 1322 / 2400   loss: 0.029610328\n",
      "iteration: 1323 / 2400   loss: 0.111395165\n",
      "iteration: 1324 / 2400   loss: 0.061748963\n",
      "iteration: 1325 / 2400   loss: 0.050350264\n",
      "iteration: 1326 / 2400   loss: 0.24261695\n",
      "iteration: 1327 / 2400   loss: 0.27136093\n",
      "iteration: 1328 / 2400   loss: 0.22561292\n",
      "iteration: 1329 / 2400   loss: 0.10970703\n",
      "iteration: 1330 / 2400   loss: 0.22092685\n",
      "iteration: 1331 / 2400   loss: 0.034425944\n",
      "iteration: 1332 / 2400   loss: 0.0377417\n",
      "iteration: 1333 / 2400   loss: 0.30634257\n",
      "iteration: 1334 / 2400   loss: 0.16255997\n",
      "iteration: 1335 / 2400   loss: 0.09752321\n",
      "iteration: 1336 / 2400   loss: 0.33911607\n",
      "iteration: 1337 / 2400   loss: 0.1269679\n",
      "iteration: 1338 / 2400   loss: 0.046785787\n",
      "iteration: 1339 / 2400   loss: 0.03670702\n",
      "iteration: 1340 / 2400   loss: 0.06234038\n",
      "iteration: 1341 / 2400   loss: 0.17565735\n",
      "iteration: 1342 / 2400   loss: 0.01633524\n",
      "iteration: 1343 / 2400   loss: 0.21033545\n",
      "iteration: 1344 / 2400   loss: 0.087319784\n",
      "iteration: 1345 / 2400   loss: 0.05087526\n",
      "iteration: 1346 / 2400   loss: 0.13233422\n",
      "iteration: 1347 / 2400   loss: 0.0455581\n",
      "iteration: 1348 / 2400   loss: 0.048129372\n",
      "iteration: 1349 / 2400   loss: 0.00922493\n",
      "iteration: 1350 / 2400   loss: 0.03499631\n",
      "iteration: 1351 / 2400   loss: 0.2296279\n",
      "iteration: 1352 / 2400   loss: 0.091761485\n",
      "iteration: 1353 / 2400   loss: 0.06081006\n",
      "iteration: 1354 / 2400   loss: 0.043510858\n",
      "iteration: 1355 / 2400   loss: 0.03147476\n",
      "iteration: 1356 / 2400   loss: 0.030630369\n",
      "iteration: 1357 / 2400   loss: 0.08808695\n",
      "iteration: 1358 / 2400   loss: 0.055466622\n",
      "iteration: 1359 / 2400   loss: 0.054740313\n",
      "iteration: 1360 / 2400   loss: 0.17634481\n",
      "iteration: 1361 / 2400   loss: 0.12083352\n",
      "iteration: 1362 / 2400   loss: 0.07699785\n",
      "iteration: 1363 / 2400   loss: 0.10598714\n",
      "iteration: 1364 / 2400   loss: 0.047134694\n",
      "iteration: 1365 / 2400   loss: 0.3263272\n",
      "iteration: 1366 / 2400   loss: 0.047481738\n",
      "iteration: 1367 / 2400   loss: 0.027010327\n",
      "iteration: 1368 / 2400   loss: 0.12188548\n",
      "iteration: 1369 / 2400   loss: 0.18889081\n",
      "iteration: 1370 / 2400   loss: 0.08289872\n",
      "iteration: 1371 / 2400   loss: 0.03740474\n",
      "iteration: 1372 / 2400   loss: 0.011612587\n",
      "iteration: 1373 / 2400   loss: 0.13337626\n",
      "iteration: 1374 / 2400   loss: 0.20328023\n",
      "iteration: 1375 / 2400   loss: 0.14017387\n",
      "iteration: 1376 / 2400   loss: 0.035333578\n",
      "iteration: 1377 / 2400   loss: 0.27840948\n",
      "iteration: 1378 / 2400   loss: 0.11546261\n",
      "iteration: 1379 / 2400   loss: 0.021437587\n",
      "iteration: 1380 / 2400   loss: 0.14867005\n",
      "iteration: 1381 / 2400   loss: 0.07043385\n",
      "iteration: 1382 / 2400   loss: 0.13853237\n",
      "iteration: 1383 / 2400   loss: 0.21473645\n",
      "iteration: 1384 / 2400   loss: 0.11480012\n",
      "iteration: 1385 / 2400   loss: 0.05078616\n",
      "iteration: 1386 / 2400   loss: 0.16898276\n",
      "iteration: 1387 / 2400   loss: 0.49293533\n",
      "iteration: 1388 / 2400   loss: 0.14766063\n",
      "iteration: 1389 / 2400   loss: 0.29025185\n",
      "iteration: 1390 / 2400   loss: 0.11011715\n",
      "iteration: 1391 / 2400   loss: 0.29598424\n",
      "iteration: 1392 / 2400   loss: 0.06687399\n",
      "iteration: 1393 / 2400   loss: 0.38280404\n",
      "iteration: 1394 / 2400   loss: 0.2994041\n",
      "iteration: 1395 / 2400   loss: 0.57777196\n",
      "iteration: 1396 / 2400   loss: 0.11224863\n",
      "iteration: 1397 / 2400   loss: 0.31437027\n",
      "iteration: 1398 / 2400   loss: 0.054736994\n",
      "iteration: 1399 / 2400   loss: 0.14640425\n",
      "iteration: 1400 / 2400   loss: 0.044429768\n",
      "iteration: 1401 / 2400   loss: 0.110224836\n",
      "iteration: 1402 / 2400   loss: 0.025350476\n",
      "iteration: 1403 / 2400   loss: 0.27669698\n",
      "iteration: 1404 / 2400   loss: 0.11240783\n",
      "iteration: 1405 / 2400   loss: 0.038111266\n",
      "iteration: 1406 / 2400   loss: 0.056664582\n",
      "iteration: 1407 / 2400   loss: 0.056642294\n",
      "iteration: 1408 / 2400   loss: 0.14991814\n",
      "iteration: 1409 / 2400   loss: 0.07163886\n",
      "iteration: 1410 / 2400   loss: 0.58700424\n",
      "iteration: 1411 / 2400   loss: 0.028561726\n",
      "iteration: 1412 / 2400   loss: 0.07628116\n",
      "iteration: 1413 / 2400   loss: 0.20370802\n",
      "iteration: 1414 / 2400   loss: 0.071806334\n",
      "iteration: 1415 / 2400   loss: 0.072713085\n",
      "iteration: 1416 / 2400   loss: 0.046218585\n",
      "iteration: 1417 / 2400   loss: 0.21264137\n",
      "iteration: 1418 / 2400   loss: 0.037189122\n",
      "iteration: 1419 / 2400   loss: 0.18948315\n",
      "iteration: 1420 / 2400   loss: 0.091011375\n",
      "iteration: 1421 / 2400   loss: 0.073975466\n",
      "iteration: 1422 / 2400   loss: 0.031394664\n",
      "iteration: 1423 / 2400   loss: 0.021832561\n",
      "iteration: 1424 / 2400   loss: 0.022146378\n",
      "iteration: 1425 / 2400   loss: 0.30571365\n",
      "iteration: 1426 / 2400   loss: 0.18076943\n",
      "iteration: 1427 / 2400   loss: 0.052406806\n",
      "iteration: 1428 / 2400   loss: 0.26616687\n",
      "iteration: 1429 / 2400   loss: 0.05447217\n",
      "iteration: 1430 / 2400   loss: 0.107777685\n",
      "iteration: 1431 / 2400   loss: 0.115754336\n",
      "iteration: 1432 / 2400   loss: 0.12850532\n",
      "iteration: 1433 / 2400   loss: 0.027440634\n",
      "iteration: 1434 / 2400   loss: 0.050600015\n",
      "iteration: 1435 / 2400   loss: 0.060132798\n",
      "iteration: 1436 / 2400   loss: 0.08298407\n",
      "iteration: 1437 / 2400   loss: 0.16114563\n",
      "iteration: 1438 / 2400   loss: 0.23436977\n",
      "iteration: 1439 / 2400   loss: 0.5464903\n",
      "iteration: 1440 / 2400   loss: 0.119433805\n",
      "iteration: 1441 / 2400   loss: 0.17464392\n",
      "iteration: 1442 / 2400   loss: 0.19627155\n",
      "iteration: 1443 / 2400   loss: 0.25188634\n",
      "iteration: 1444 / 2400   loss: 0.1934687\n",
      "iteration: 1445 / 2400   loss: 0.37904635\n",
      "iteration: 1446 / 2400   loss: 0.068440124\n",
      "iteration: 1447 / 2400   loss: 0.09501393\n",
      "iteration: 1448 / 2400   loss: 0.029918384\n",
      "iteration: 1449 / 2400   loss: 0.3066493\n",
      "iteration: 1450 / 2400   loss: 0.4239065\n",
      "iteration: 1451 / 2400   loss: 0.07807833\n",
      "iteration: 1452 / 2400   loss: 0.15255837\n",
      "iteration: 1453 / 2400   loss: 0.111863375\n",
      "iteration: 1454 / 2400   loss: 0.23292679\n",
      "iteration: 1455 / 2400   loss: 0.20297317\n",
      "iteration: 1456 / 2400   loss: 0.08877348\n",
      "iteration: 1457 / 2400   loss: 0.17085105\n",
      "iteration: 1458 / 2400   loss: 0.20438434\n",
      "iteration: 1459 / 2400   loss: 0.031292543\n",
      "iteration: 1460 / 2400   loss: 0.28714418\n",
      "iteration: 1461 / 2400   loss: 0.06620117\n",
      "iteration: 1462 / 2400   loss: 0.09991134\n",
      "iteration: 1463 / 2400   loss: 0.024272118\n",
      "iteration: 1464 / 2400   loss: 0.22144535\n",
      "iteration: 1465 / 2400   loss: 0.06842374\n",
      "iteration: 1466 / 2400   loss: 0.02996313\n",
      "iteration: 1467 / 2400   loss: 0.08561819\n",
      "iteration: 1468 / 2400   loss: 0.12740147\n",
      "iteration: 1469 / 2400   loss: 0.08614662\n",
      "iteration: 1470 / 2400   loss: 0.5974821\n",
      "iteration: 1471 / 2400   loss: 0.23850344\n",
      "iteration: 1472 / 2400   loss: 0.06490366\n",
      "iteration: 1473 / 2400   loss: 0.13805456\n",
      "iteration: 1474 / 2400   loss: 0.4561401\n",
      "iteration: 1475 / 2400   loss: 0.2436892\n",
      "iteration: 1476 / 2400   loss: 0.05296295\n",
      "iteration: 1477 / 2400   loss: 0.02617221\n",
      "iteration: 1478 / 2400   loss: 0.25200546\n",
      "iteration: 1479 / 2400   loss: 0.05946946\n",
      "iteration: 1480 / 2400   loss: 0.22495364\n",
      "iteration: 1481 / 2400   loss: 0.05420617\n",
      "iteration: 1482 / 2400   loss: 0.48207647\n",
      "iteration: 1483 / 2400   loss: 0.4425515\n",
      "iteration: 1484 / 2400   loss: 0.09783772\n",
      "iteration: 1485 / 2400   loss: 0.061929755\n",
      "iteration: 1486 / 2400   loss: 0.07311253\n",
      "iteration: 1487 / 2400   loss: 0.12747283\n",
      "iteration: 1488 / 2400   loss: 0.037091676\n",
      "iteration: 1489 / 2400   loss: 0.08459995\n",
      "iteration: 1490 / 2400   loss: 0.041951943\n",
      "iteration: 1491 / 2400   loss: 0.24427925\n",
      "iteration: 1492 / 2400   loss: 0.35852134\n",
      "iteration: 1493 / 2400   loss: 0.0937339\n",
      "iteration: 1494 / 2400   loss: 0.20542961\n",
      "iteration: 1495 / 2400   loss: 0.10348446\n",
      "iteration: 1496 / 2400   loss: 0.09194503\n",
      "iteration: 1497 / 2400   loss: 0.10626598\n",
      "iteration: 1498 / 2400   loss: 0.19098757\n",
      "iteration: 1499 / 2400   loss: 0.18049115\n",
      "iteration: 1500 / 2400   loss: 0.3025237\n",
      "iteration: 1501 / 2400   loss: 0.07048211\n",
      "iteration: 1502 / 2400   loss: 0.17002025\n",
      "iteration: 1503 / 2400   loss: 0.14337562\n",
      "iteration: 1504 / 2400   loss: 0.09851578\n",
      "iteration: 1505 / 2400   loss: 0.20825945\n",
      "iteration: 1506 / 2400   loss: 0.023365794\n",
      "iteration: 1507 / 2400   loss: 0.044539813\n",
      "iteration: 1508 / 2400   loss: 0.1742624\n",
      "iteration: 1509 / 2400   loss: 0.28348026\n",
      "iteration: 1510 / 2400   loss: 0.1246352\n",
      "iteration: 1511 / 2400   loss: 0.17347582\n",
      "iteration: 1512 / 2400   loss: 0.03438057\n",
      "iteration: 1513 / 2400   loss: 0.21875906\n",
      "iteration: 1514 / 2400   loss: 0.1799973\n",
      "iteration: 1515 / 2400   loss: 0.03917862\n",
      "iteration: 1516 / 2400   loss: 0.07631925\n",
      "iteration: 1517 / 2400   loss: 0.34206754\n",
      "iteration: 1518 / 2400   loss: 0.13275892\n",
      "iteration: 1519 / 2400   loss: 0.08833514\n",
      "iteration: 1520 / 2400   loss: 0.034955252\n",
      "iteration: 1521 / 2400   loss: 0.19051456\n",
      "iteration: 1522 / 2400   loss: 0.058854155\n",
      "iteration: 1523 / 2400   loss: 0.08489294\n",
      "iteration: 1524 / 2400   loss: 0.1263539\n",
      "iteration: 1525 / 2400   loss: 0.014556179\n",
      "iteration: 1526 / 2400   loss: 0.17079681\n",
      "iteration: 1527 / 2400   loss: 0.2845682\n",
      "iteration: 1528 / 2400   loss: 0.15101467\n",
      "iteration: 1529 / 2400   loss: 0.03427395\n",
      "iteration: 1530 / 2400   loss: 0.037721623\n",
      "iteration: 1531 / 2400   loss: 0.100822516\n",
      "iteration: 1532 / 2400   loss: 0.066378325\n",
      "iteration: 1533 / 2400   loss: 0.12944657\n",
      "iteration: 1534 / 2400   loss: 0.058891878\n",
      "iteration: 1535 / 2400   loss: 0.2289965\n",
      "iteration: 1536 / 2400   loss: 0.536927\n",
      "iteration: 1537 / 2400   loss: 0.10438753\n",
      "iteration: 1538 / 2400   loss: 0.053768672\n",
      "iteration: 1539 / 2400   loss: 0.21289518\n",
      "iteration: 1540 / 2400   loss: 0.07766155\n",
      "iteration: 1541 / 2400   loss: 0.4089637\n",
      "iteration: 1542 / 2400   loss: 0.15487394\n",
      "iteration: 1543 / 2400   loss: 0.06267881\n",
      "iteration: 1544 / 2400   loss: 0.15161388\n",
      "iteration: 1545 / 2400   loss: 0.17398643\n",
      "iteration: 1546 / 2400   loss: 0.061907653\n",
      "iteration: 1547 / 2400   loss: 0.05862766\n",
      "iteration: 1548 / 2400   loss: 0.057281915\n",
      "iteration: 1549 / 2400   loss: 0.14446914\n",
      "iteration: 1550 / 2400   loss: 0.07843158\n",
      "iteration: 1551 / 2400   loss: 0.5872966\n",
      "iteration: 1552 / 2400   loss: 0.102574736\n",
      "iteration: 1553 / 2400   loss: 0.043234468\n",
      "iteration: 1554 / 2400   loss: 0.051104918\n",
      "iteration: 1555 / 2400   loss: 0.016434899\n",
      "iteration: 1556 / 2400   loss: 0.09056285\n",
      "iteration: 1557 / 2400   loss: 0.3673127\n",
      "iteration: 1558 / 2400   loss: 0.01586504\n",
      "iteration: 1559 / 2400   loss: 0.07489568\n",
      "iteration: 1560 / 2400   loss: 0.12389177\n",
      "iteration: 1561 / 2400   loss: 0.03951659\n",
      "iteration: 1562 / 2400   loss: 0.0752902\n",
      "iteration: 1563 / 2400   loss: 0.052191105\n",
      "iteration: 1564 / 2400   loss: 0.038211297\n",
      "iteration: 1565 / 2400   loss: 0.025001144\n",
      "iteration: 1566 / 2400   loss: 0.06429586\n",
      "iteration: 1567 / 2400   loss: 0.05369066\n",
      "iteration: 1568 / 2400   loss: 0.2702643\n",
      "iteration: 1569 / 2400   loss: 0.10032815\n",
      "iteration: 1570 / 2400   loss: 0.06825117\n",
      "iteration: 1571 / 2400   loss: 0.17523998\n",
      "iteration: 1572 / 2400   loss: 0.062054854\n",
      "iteration: 1573 / 2400   loss: 0.14707977\n",
      "iteration: 1574 / 2400   loss: 0.12529153\n",
      "iteration: 1575 / 2400   loss: 0.26838148\n",
      "iteration: 1576 / 2400   loss: 0.30455709\n",
      "iteration: 1577 / 2400   loss: 0.30779418\n",
      "iteration: 1578 / 2400   loss: 0.21716301\n",
      "iteration: 1579 / 2400   loss: 0.24057063\n",
      "iteration: 1580 / 2400   loss: 0.22851585\n",
      "iteration: 1581 / 2400   loss: 0.049004957\n",
      "iteration: 1582 / 2400   loss: 0.18115193\n",
      "iteration: 1583 / 2400   loss: 0.016213704\n",
      "iteration: 1584 / 2400   loss: 0.033766232\n",
      "iteration: 1585 / 2400   loss: 0.26891255\n",
      "iteration: 1586 / 2400   loss: 0.18122835\n",
      "iteration: 1587 / 2400   loss: 0.2916855\n",
      "iteration: 1588 / 2400   loss: 0.06414384\n",
      "iteration: 1589 / 2400   loss: 0.18963738\n",
      "iteration: 1590 / 2400   loss: 0.15951891\n",
      "iteration: 1591 / 2400   loss: 0.1506891\n",
      "iteration: 1592 / 2400   loss: 0.15064403\n",
      "iteration: 1593 / 2400   loss: 0.14888763\n",
      "iteration: 1594 / 2400   loss: 0.115987584\n",
      "iteration: 1595 / 2400   loss: 0.15302864\n",
      "iteration: 1596 / 2400   loss: 0.20688707\n",
      "iteration: 1597 / 2400   loss: 0.061726302\n",
      "iteration: 1598 / 2400   loss: 0.21659414\n",
      "iteration: 1599 / 2400   loss: 0.0512589\n",
      "iteration: 1600 / 2400   loss: 0.12165127\n",
      "iteration: 1601 / 2400   loss: 0.5553979\n",
      "iteration: 1602 / 2400   loss: 0.037291814\n",
      "iteration: 1603 / 2400   loss: 0.06738461\n",
      "iteration: 1604 / 2400   loss: 0.13812914\n",
      "iteration: 1605 / 2400   loss: 0.06152878\n",
      "iteration: 1606 / 2400   loss: 0.33785665\n",
      "iteration: 1607 / 2400   loss: 0.09393057\n",
      "iteration: 1608 / 2400   loss: 0.0269378\n",
      "iteration: 1609 / 2400   loss: 0.05787035\n",
      "iteration: 1610 / 2400   loss: 0.07008843\n",
      "iteration: 1611 / 2400   loss: 0.2234779\n",
      "iteration: 1612 / 2400   loss: 0.26911277\n",
      "iteration: 1613 / 2400   loss: 0.035276767\n",
      "iteration: 1614 / 2400   loss: 0.02009386\n",
      "iteration: 1615 / 2400   loss: 0.07920156\n",
      "iteration: 1616 / 2400   loss: 0.37535897\n",
      "iteration: 1617 / 2400   loss: 0.0076828958\n",
      "iteration: 1618 / 2400   loss: 0.124929756\n",
      "iteration: 1619 / 2400   loss: 0.09399104\n",
      "iteration: 1620 / 2400   loss: 0.19710311\n",
      "iteration: 1621 / 2400   loss: 0.015831433\n",
      "iteration: 1622 / 2400   loss: 0.37208158\n",
      "iteration: 1623 / 2400   loss: 0.06272949\n",
      "iteration: 1624 / 2400   loss: 0.055645846\n",
      "iteration: 1625 / 2400   loss: 0.051945\n",
      "iteration: 1626 / 2400   loss: 0.24009018\n",
      "iteration: 1627 / 2400   loss: 0.32064793\n",
      "iteration: 1628 / 2400   loss: 0.029569617\n",
      "iteration: 1629 / 2400   loss: 0.519204\n",
      "iteration: 1630 / 2400   loss: 0.01486392\n",
      "iteration: 1631 / 2400   loss: 0.11746857\n",
      "iteration: 1632 / 2400   loss: 0.018349858\n",
      "iteration: 1633 / 2400   loss: 0.04584316\n",
      "iteration: 1634 / 2400   loss: 0.13695359\n",
      "iteration: 1635 / 2400   loss: 0.06903341\n",
      "iteration: 1636 / 2400   loss: 0.1574285\n",
      "iteration: 1637 / 2400   loss: 0.04865532\n",
      "iteration: 1638 / 2400   loss: 0.08816473\n",
      "iteration: 1639 / 2400   loss: 0.13709165\n",
      "iteration: 1640 / 2400   loss: 0.23070225\n",
      "iteration: 1641 / 2400   loss: 0.08092352\n",
      "iteration: 1642 / 2400   loss: 0.019125376\n",
      "iteration: 1643 / 2400   loss: 0.3148038\n",
      "iteration: 1644 / 2400   loss: 0.19869423\n",
      "iteration: 1645 / 2400   loss: 0.24544129\n",
      "iteration: 1646 / 2400   loss: 0.07525253\n",
      "iteration: 1647 / 2400   loss: 0.0143543435\n",
      "iteration: 1648 / 2400   loss: 0.14570902\n",
      "iteration: 1649 / 2400   loss: 0.1986746\n",
      "iteration: 1650 / 2400   loss: 0.27209422\n",
      "iteration: 1651 / 2400   loss: 0.26545963\n",
      "iteration: 1652 / 2400   loss: 0.40059066\n",
      "iteration: 1653 / 2400   loss: 0.09718817\n",
      "iteration: 1654 / 2400   loss: 0.22546352\n",
      "iteration: 1655 / 2400   loss: 0.15262365\n",
      "iteration: 1656 / 2400   loss: 0.3809793\n",
      "iteration: 1657 / 2400   loss: 0.09288598\n",
      "iteration: 1658 / 2400   loss: 0.35435134\n",
      "iteration: 1659 / 2400   loss: 0.45548934\n",
      "iteration: 1660 / 2400   loss: 0.31592494\n",
      "iteration: 1661 / 2400   loss: 0.25377455\n",
      "iteration: 1662 / 2400   loss: 0.5306096\n",
      "iteration: 1663 / 2400   loss: 0.122158624\n",
      "iteration: 1664 / 2400   loss: 0.066078566\n",
      "iteration: 1665 / 2400   loss: 0.43341464\n",
      "iteration: 1666 / 2400   loss: 0.14460286\n",
      "iteration: 1667 / 2400   loss: 0.046361998\n",
      "iteration: 1668 / 2400   loss: 0.04880375\n",
      "iteration: 1669 / 2400   loss: 0.08579878\n",
      "iteration: 1670 / 2400   loss: 0.04148591\n",
      "iteration: 1671 / 2400   loss: 0.17736007\n",
      "iteration: 1672 / 2400   loss: 0.068032466\n",
      "iteration: 1673 / 2400   loss: 0.14749947\n",
      "iteration: 1674 / 2400   loss: 0.17109665\n",
      "iteration: 1675 / 2400   loss: 0.04197836\n",
      "iteration: 1676 / 2400   loss: 0.06428677\n",
      "iteration: 1677 / 2400   loss: 0.3217306\n",
      "iteration: 1678 / 2400   loss: 0.023272716\n",
      "iteration: 1679 / 2400   loss: 0.15100718\n",
      "iteration: 1680 / 2400   loss: 0.23227929\n",
      "iteration: 1681 / 2400   loss: 0.19753556\n",
      "iteration: 1682 / 2400   loss: 0.38318658\n",
      "iteration: 1683 / 2400   loss: 0.1310208\n",
      "iteration: 1684 / 2400   loss: 0.21369705\n",
      "iteration: 1685 / 2400   loss: 0.18451907\n",
      "iteration: 1686 / 2400   loss: 0.322888\n",
      "iteration: 1687 / 2400   loss: 0.033487912\n",
      "iteration: 1688 / 2400   loss: 0.03161042\n",
      "iteration: 1689 / 2400   loss: 0.1092437\n",
      "iteration: 1690 / 2400   loss: 0.038856335\n",
      "iteration: 1691 / 2400   loss: 0.11857245\n",
      "iteration: 1692 / 2400   loss: 0.059463825\n",
      "iteration: 1693 / 2400   loss: 0.36717927\n",
      "iteration: 1694 / 2400   loss: 0.059705857\n",
      "iteration: 1695 / 2400   loss: 0.1407755\n",
      "iteration: 1696 / 2400   loss: 0.02631214\n",
      "iteration: 1697 / 2400   loss: 0.05456252\n",
      "iteration: 1698 / 2400   loss: 0.48908845\n",
      "iteration: 1699 / 2400   loss: 0.23672092\n",
      "iteration: 1700 / 2400   loss: 0.24032165\n",
      "iteration: 1701 / 2400   loss: 0.1440291\n",
      "iteration: 1702 / 2400   loss: 0.13014798\n",
      "iteration: 1703 / 2400   loss: 0.17375366\n",
      "iteration: 1704 / 2400   loss: 0.018190173\n",
      "iteration: 1705 / 2400   loss: 0.040430497\n",
      "iteration: 1706 / 2400   loss: 0.09522436\n",
      "iteration: 1707 / 2400   loss: 0.06923138\n",
      "iteration: 1708 / 2400   loss: 0.2682666\n",
      "iteration: 1709 / 2400   loss: 0.19682705\n",
      "iteration: 1710 / 2400   loss: 0.09341809\n",
      "iteration: 1711 / 2400   loss: 0.21692406\n",
      "iteration: 1712 / 2400   loss: 0.08131881\n",
      "iteration: 1713 / 2400   loss: 0.036192074\n",
      "iteration: 1714 / 2400   loss: 0.15621799\n",
      "iteration: 1715 / 2400   loss: 0.29986644\n",
      "iteration: 1716 / 2400   loss: 0.076304376\n",
      "iteration: 1717 / 2400   loss: 0.1496974\n",
      "iteration: 1718 / 2400   loss: 0.33739412\n",
      "iteration: 1719 / 2400   loss: 0.09521593\n",
      "iteration: 1720 / 2400   loss: 0.20324631\n",
      "iteration: 1721 / 2400   loss: 0.105301954\n",
      "iteration: 1722 / 2400   loss: 0.16518368\n",
      "iteration: 1723 / 2400   loss: 0.27121413\n",
      "iteration: 1724 / 2400   loss: 0.06687402\n",
      "iteration: 1725 / 2400   loss: 0.37381336\n",
      "iteration: 1726 / 2400   loss: 0.01463129\n",
      "iteration: 1727 / 2400   loss: 0.046542764\n",
      "iteration: 1728 / 2400   loss: 0.09280735\n",
      "iteration: 1729 / 2400   loss: 0.27533787\n",
      "iteration: 1730 / 2400   loss: 0.3764835\n",
      "iteration: 1731 / 2400   loss: 0.021149864\n",
      "iteration: 1732 / 2400   loss: 0.07189434\n",
      "iteration: 1733 / 2400   loss: 0.056471262\n",
      "iteration: 1734 / 2400   loss: 0.08737495\n",
      "iteration: 1735 / 2400   loss: 0.01841362\n",
      "iteration: 1736 / 2400   loss: 0.10749554\n",
      "iteration: 1737 / 2400   loss: 0.19595636\n",
      "iteration: 1738 / 2400   loss: 0.083565995\n",
      "iteration: 1739 / 2400   loss: 0.25706378\n",
      "iteration: 1740 / 2400   loss: 0.03218569\n",
      "iteration: 1741 / 2400   loss: 0.043363884\n",
      "iteration: 1742 / 2400   loss: 0.6159622\n",
      "iteration: 1743 / 2400   loss: 0.2730106\n",
      "iteration: 1744 / 2400   loss: 0.2359917\n",
      "iteration: 1745 / 2400   loss: 0.034465246\n",
      "iteration: 1746 / 2400   loss: 0.01587326\n",
      "iteration: 1747 / 2400   loss: 0.35348862\n",
      "iteration: 1748 / 2400   loss: 0.049725723\n",
      "iteration: 1749 / 2400   loss: 0.023348484\n",
      "iteration: 1750 / 2400   loss: 0.13726535\n",
      "iteration: 1751 / 2400   loss: 0.10310897\n",
      "iteration: 1752 / 2400   loss: 0.22159721\n",
      "iteration: 1753 / 2400   loss: 0.032860298\n",
      "iteration: 1754 / 2400   loss: 0.056816407\n",
      "iteration: 1755 / 2400   loss: 0.33990538\n",
      "iteration: 1756 / 2400   loss: 0.016898861\n",
      "iteration: 1757 / 2400   loss: 0.033943817\n",
      "iteration: 1758 / 2400   loss: 0.03876689\n",
      "iteration: 1759 / 2400   loss: 0.27177617\n",
      "iteration: 1760 / 2400   loss: 0.07948741\n",
      "iteration: 1761 / 2400   loss: 0.10162545\n",
      "iteration: 1762 / 2400   loss: 0.17683892\n",
      "iteration: 1763 / 2400   loss: 0.019276123\n",
      "iteration: 1764 / 2400   loss: 0.17076045\n",
      "iteration: 1765 / 2400   loss: 0.095690794\n",
      "iteration: 1766 / 2400   loss: 0.16645092\n",
      "iteration: 1767 / 2400   loss: 0.22821407\n",
      "iteration: 1768 / 2400   loss: 0.12047191\n",
      "iteration: 1769 / 2400   loss: 0.06402689\n",
      "iteration: 1770 / 2400   loss: 0.1191732\n",
      "iteration: 1771 / 2400   loss: 0.20299569\n",
      "iteration: 1772 / 2400   loss: 0.10583932\n",
      "iteration: 1773 / 2400   loss: 0.027270164\n",
      "iteration: 1774 / 2400   loss: 0.22217648\n",
      "iteration: 1775 / 2400   loss: 0.08881636\n",
      "iteration: 1776 / 2400   loss: 0.12087595\n",
      "iteration: 1777 / 2400   loss: 0.18057087\n",
      "iteration: 1778 / 2400   loss: 0.047755834\n",
      "iteration: 1779 / 2400   loss: 0.008127651\n",
      "iteration: 1780 / 2400   loss: 0.12125662\n",
      "iteration: 1781 / 2400   loss: 0.07497642\n",
      "iteration: 1782 / 2400   loss: 0.08358745\n",
      "iteration: 1783 / 2400   loss: 0.024109134\n",
      "iteration: 1784 / 2400   loss: 0.072144344\n",
      "iteration: 1785 / 2400   loss: 0.05634485\n",
      "iteration: 1786 / 2400   loss: 0.074406736\n",
      "iteration: 1787 / 2400   loss: 0.035256844\n",
      "iteration: 1788 / 2400   loss: 0.019520989\n",
      "iteration: 1789 / 2400   loss: 0.08418474\n",
      "iteration: 1790 / 2400   loss: 0.4823286\n",
      "iteration: 1791 / 2400   loss: 0.07675457\n",
      "iteration: 1792 / 2400   loss: 0.06564095\n",
      "iteration: 1793 / 2400   loss: 0.09107749\n",
      "iteration: 1794 / 2400   loss: 0.14122485\n",
      "iteration: 1795 / 2400   loss: 0.17912224\n",
      "iteration: 1796 / 2400   loss: 0.25892752\n",
      "iteration: 1797 / 2400   loss: 0.038529433\n",
      "iteration: 1798 / 2400   loss: 0.16135779\n",
      "iteration: 1799 / 2400   loss: 0.0753322\n",
      "iteration: 1800 / 2400   loss: 0.038047228\n",
      "iteration: 1801 / 2400   loss: 0.015885582\n",
      "iteration: 1802 / 2400   loss: 0.5585373\n",
      "iteration: 1803 / 2400   loss: 0.06974512\n",
      "iteration: 1804 / 2400   loss: 0.07275887\n",
      "iteration: 1805 / 2400   loss: 0.03277981\n",
      "iteration: 1806 / 2400   loss: 0.142908\n",
      "iteration: 1807 / 2400   loss: 0.031320896\n",
      "iteration: 1808 / 2400   loss: 0.18114015\n",
      "iteration: 1809 / 2400   loss: 0.009460621\n",
      "iteration: 1810 / 2400   loss: 0.053473245\n",
      "iteration: 1811 / 2400   loss: 0.088658065\n",
      "iteration: 1812 / 2400   loss: 0.026943656\n",
      "iteration: 1813 / 2400   loss: 0.024806451\n",
      "iteration: 1814 / 2400   loss: 0.088786356\n",
      "iteration: 1815 / 2400   loss: 0.21269085\n",
      "iteration: 1816 / 2400   loss: 0.030586585\n",
      "iteration: 1817 / 2400   loss: 0.16284393\n",
      "iteration: 1818 / 2400   loss: 0.29646122\n",
      "iteration: 1819 / 2400   loss: 0.38785386\n",
      "iteration: 1820 / 2400   loss: 0.042508394\n",
      "iteration: 1821 / 2400   loss: 0.2739786\n",
      "iteration: 1822 / 2400   loss: 0.24371602\n",
      "iteration: 1823 / 2400   loss: 0.07828024\n",
      "iteration: 1824 / 2400   loss: 0.1542486\n",
      "iteration: 1825 / 2400   loss: 0.21258946\n",
      "iteration: 1826 / 2400   loss: 0.0056770705\n",
      "iteration: 1827 / 2400   loss: 0.18303777\n",
      "iteration: 1828 / 2400   loss: 0.045173015\n",
      "iteration: 1829 / 2400   loss: 0.05354646\n",
      "iteration: 1830 / 2400   loss: 0.2442325\n",
      "iteration: 1831 / 2400   loss: 0.35308915\n",
      "iteration: 1832 / 2400   loss: 0.3269651\n",
      "iteration: 1833 / 2400   loss: 0.29716584\n",
      "iteration: 1834 / 2400   loss: 0.044135693\n",
      "iteration: 1835 / 2400   loss: 0.3672859\n",
      "iteration: 1836 / 2400   loss: 0.096799806\n",
      "iteration: 1837 / 2400   loss: 0.556669\n",
      "iteration: 1838 / 2400   loss: 0.111235306\n",
      "iteration: 1839 / 2400   loss: 0.20892927\n",
      "iteration: 1840 / 2400   loss: 0.09320042\n",
      "iteration: 1841 / 2400   loss: 0.3377468\n",
      "iteration: 1842 / 2400   loss: 0.16106361\n",
      "iteration: 1843 / 2400   loss: 0.25389102\n",
      "iteration: 1844 / 2400   loss: 0.1772713\n",
      "iteration: 1845 / 2400   loss: 0.34379402\n",
      "iteration: 1846 / 2400   loss: 0.17368269\n",
      "iteration: 1847 / 2400   loss: 0.10760498\n",
      "iteration: 1848 / 2400   loss: 0.3626353\n",
      "iteration: 1849 / 2400   loss: 0.02123723\n",
      "iteration: 1850 / 2400   loss: 0.19749431\n",
      "iteration: 1851 / 2400   loss: 0.21703786\n",
      "iteration: 1852 / 2400   loss: 0.27121472\n",
      "iteration: 1853 / 2400   loss: 0.36668587\n",
      "iteration: 1854 / 2400   loss: 0.026213694\n",
      "iteration: 1855 / 2400   loss: 0.39648202\n",
      "iteration: 1856 / 2400   loss: 0.25573343\n",
      "iteration: 1857 / 2400   loss: 0.4816696\n",
      "iteration: 1858 / 2400   loss: 0.29179725\n",
      "iteration: 1859 / 2400   loss: 0.18903874\n",
      "iteration: 1860 / 2400   loss: 0.11381029\n",
      "iteration: 1861 / 2400   loss: 0.10052654\n",
      "iteration: 1862 / 2400   loss: 0.022468423\n",
      "iteration: 1863 / 2400   loss: 0.040556528\n",
      "iteration: 1864 / 2400   loss: 0.08334413\n",
      "iteration: 1865 / 2400   loss: 0.028907318\n",
      "iteration: 1866 / 2400   loss: 0.012161655\n",
      "iteration: 1867 / 2400   loss: 0.01202568\n",
      "iteration: 1868 / 2400   loss: 0.11492218\n",
      "iteration: 1869 / 2400   loss: 0.3003259\n",
      "iteration: 1870 / 2400   loss: 0.30993274\n",
      "iteration: 1871 / 2400   loss: 0.07457469\n",
      "iteration: 1872 / 2400   loss: 0.1817072\n",
      "iteration: 1873 / 2400   loss: 0.108115666\n",
      "iteration: 1874 / 2400   loss: 0.14454615\n",
      "iteration: 1875 / 2400   loss: 0.21208316\n",
      "iteration: 1876 / 2400   loss: 0.10849084\n",
      "iteration: 1877 / 2400   loss: 0.06773598\n",
      "iteration: 1878 / 2400   loss: 0.02688488\n",
      "iteration: 1879 / 2400   loss: 0.10753883\n",
      "iteration: 1880 / 2400   loss: 0.062088918\n",
      "iteration: 1881 / 2400   loss: 0.15807122\n",
      "iteration: 1882 / 2400   loss: 0.3524175\n",
      "iteration: 1883 / 2400   loss: 0.041286573\n",
      "iteration: 1884 / 2400   loss: 0.11048265\n",
      "iteration: 1885 / 2400   loss: 0.013535595\n",
      "iteration: 1886 / 2400   loss: 0.041470986\n",
      "iteration: 1887 / 2400   loss: 0.14495763\n",
      "iteration: 1888 / 2400   loss: 0.013498001\n",
      "iteration: 1889 / 2400   loss: 0.12653826\n",
      "iteration: 1890 / 2400   loss: 0.5436304\n",
      "iteration: 1891 / 2400   loss: 0.39587775\n",
      "iteration: 1892 / 2400   loss: 0.2606495\n",
      "iteration: 1893 / 2400   loss: 0.089571305\n",
      "iteration: 1894 / 2400   loss: 0.16958974\n",
      "iteration: 1895 / 2400   loss: 0.07158122\n",
      "iteration: 1896 / 2400   loss: 0.15383606\n",
      "iteration: 1897 / 2400   loss: 0.19815445\n",
      "iteration: 1898 / 2400   loss: 0.2693975\n",
      "iteration: 1899 / 2400   loss: 0.24587502\n",
      "iteration: 1900 / 2400   loss: 0.31577155\n",
      "iteration: 1901 / 2400   loss: 0.2548751\n",
      "iteration: 1902 / 2400   loss: 0.041062996\n",
      "iteration: 1903 / 2400   loss: 0.19969742\n",
      "iteration: 1904 / 2400   loss: 0.37196326\n",
      "iteration: 1905 / 2400   loss: 0.33362728\n",
      "iteration: 1906 / 2400   loss: 0.10846953\n",
      "iteration: 1907 / 2400   loss: 0.12018748\n",
      "iteration: 1908 / 2400   loss: 0.37180173\n",
      "iteration: 1909 / 2400   loss: 0.10217067\n",
      "iteration: 1910 / 2400   loss: 0.09352214\n",
      "iteration: 1911 / 2400   loss: 0.3501801\n",
      "iteration: 1912 / 2400   loss: 0.09661984\n",
      "iteration: 1913 / 2400   loss: 0.03660548\n",
      "iteration: 1914 / 2400   loss: 0.16357143\n",
      "iteration: 1915 / 2400   loss: 0.0783492\n",
      "iteration: 1916 / 2400   loss: 0.05517556\n",
      "iteration: 1917 / 2400   loss: 0.37438846\n",
      "iteration: 1918 / 2400   loss: 0.47593617\n",
      "iteration: 1919 / 2400   loss: 0.2568818\n",
      "iteration: 1920 / 2400   loss: 0.04876258\n",
      "iteration: 1921 / 2400   loss: 0.13967738\n",
      "iteration: 1922 / 2400   loss: 0.05544899\n",
      "iteration: 1923 / 2400   loss: 0.16157722\n",
      "iteration: 1924 / 2400   loss: 0.1025328\n",
      "iteration: 1925 / 2400   loss: 0.062121972\n",
      "iteration: 1926 / 2400   loss: 0.020225773\n",
      "iteration: 1927 / 2400   loss: 0.20071541\n",
      "iteration: 1928 / 2400   loss: 0.036036834\n",
      "iteration: 1929 / 2400   loss: 0.095878534\n",
      "iteration: 1930 / 2400   loss: 0.07549352\n",
      "iteration: 1931 / 2400   loss: 0.5097808\n",
      "iteration: 1932 / 2400   loss: 0.11168322\n",
      "iteration: 1933 / 2400   loss: 0.18456069\n",
      "iteration: 1934 / 2400   loss: 0.17407493\n",
      "iteration: 1935 / 2400   loss: 0.114213794\n",
      "iteration: 1936 / 2400   loss: 0.28179228\n",
      "iteration: 1937 / 2400   loss: 0.040386513\n",
      "iteration: 1938 / 2400   loss: 0.044384077\n",
      "iteration: 1939 / 2400   loss: 0.21353036\n",
      "iteration: 1940 / 2400   loss: 0.05485465\n",
      "iteration: 1941 / 2400   loss: 0.29016653\n",
      "iteration: 1942 / 2400   loss: 0.12810658\n",
      "iteration: 1943 / 2400   loss: 0.07795054\n",
      "iteration: 1944 / 2400   loss: 0.110306054\n",
      "iteration: 1945 / 2400   loss: 0.055075243\n",
      "iteration: 1946 / 2400   loss: 0.056017667\n",
      "iteration: 1947 / 2400   loss: 0.01942338\n",
      "iteration: 1948 / 2400   loss: 0.44137338\n",
      "iteration: 1949 / 2400   loss: 0.15714322\n",
      "iteration: 1950 / 2400   loss: 0.0068800733\n",
      "iteration: 1951 / 2400   loss: 0.020051822\n",
      "iteration: 1952 / 2400   loss: 0.01750188\n",
      "iteration: 1953 / 2400   loss: 0.21253414\n",
      "iteration: 1954 / 2400   loss: 0.0770066\n",
      "iteration: 1955 / 2400   loss: 0.11714068\n",
      "iteration: 1956 / 2400   loss: 0.11142369\n",
      "iteration: 1957 / 2400   loss: 0.07021787\n",
      "iteration: 1958 / 2400   loss: 0.2427407\n",
      "iteration: 1959 / 2400   loss: 0.2605009\n",
      "iteration: 1960 / 2400   loss: 0.28352556\n",
      "iteration: 1961 / 2400   loss: 0.16086154\n",
      "iteration: 1962 / 2400   loss: 0.0917683\n",
      "iteration: 1963 / 2400   loss: 0.4820388\n",
      "iteration: 1964 / 2400   loss: 0.10234053\n",
      "iteration: 1965 / 2400   loss: 0.17309003\n",
      "iteration: 1966 / 2400   loss: 0.46548575\n",
      "iteration: 1967 / 2400   loss: 0.3404796\n",
      "iteration: 1968 / 2400   loss: 0.16318806\n",
      "iteration: 1969 / 2400   loss: 0.39103958\n",
      "iteration: 1970 / 2400   loss: 0.25250456\n",
      "iteration: 1971 / 2400   loss: 0.066344835\n",
      "iteration: 1972 / 2400   loss: 0.22449028\n",
      "iteration: 1973 / 2400   loss: 0.030909061\n",
      "iteration: 1974 / 2400   loss: 0.016592674\n",
      "iteration: 1975 / 2400   loss: 0.054986067\n",
      "iteration: 1976 / 2400   loss: 0.02888629\n",
      "iteration: 1977 / 2400   loss: 0.14910324\n",
      "iteration: 1978 / 2400   loss: 0.074681\n",
      "iteration: 1979 / 2400   loss: 0.2190028\n",
      "iteration: 1980 / 2400   loss: 0.6365119\n",
      "iteration: 1981 / 2400   loss: 0.38118103\n",
      "iteration: 1982 / 2400   loss: 0.57108366\n",
      "iteration: 1983 / 2400   loss: 0.20860374\n",
      "iteration: 1984 / 2400   loss: 0.13608481\n",
      "iteration: 1985 / 2400   loss: 0.015615711\n",
      "iteration: 1986 / 2400   loss: 0.06417571\n",
      "iteration: 1987 / 2400   loss: 0.20525709\n",
      "iteration: 1988 / 2400   loss: 0.17983578\n",
      "iteration: 1989 / 2400   loss: 0.038339976\n",
      "iteration: 1990 / 2400   loss: 0.0797219\n",
      "iteration: 1991 / 2400   loss: 0.075166285\n",
      "iteration: 1992 / 2400   loss: 0.06806242\n",
      "iteration: 1993 / 2400   loss: 0.029209834\n",
      "iteration: 1994 / 2400   loss: 0.16830418\n",
      "iteration: 1995 / 2400   loss: 0.12674867\n",
      "iteration: 1996 / 2400   loss: 0.4747888\n",
      "iteration: 1997 / 2400   loss: 0.41586006\n",
      "iteration: 1998 / 2400   loss: 0.08676792\n",
      "iteration: 1999 / 2400   loss: 0.17049919\n",
      "iteration: 2000 / 2400   loss: 0.202831\n",
      "iteration: 2001 / 2400   loss: 0.032029923\n",
      "iteration: 2002 / 2400   loss: 0.26365212\n",
      "iteration: 2003 / 2400   loss: 0.04374219\n",
      "iteration: 2004 / 2400   loss: 0.4995866\n",
      "iteration: 2005 / 2400   loss: 0.2022258\n",
      "iteration: 2006 / 2400   loss: 0.24170792\n",
      "iteration: 2007 / 2400   loss: 0.033441715\n",
      "iteration: 2008 / 2400   loss: 0.07793766\n",
      "iteration: 2009 / 2400   loss: 0.54885536\n",
      "iteration: 2010 / 2400   loss: 0.31549853\n",
      "iteration: 2011 / 2400   loss: 0.11295485\n",
      "iteration: 2012 / 2400   loss: 0.08104206\n",
      "iteration: 2013 / 2400   loss: 0.16851233\n",
      "iteration: 2014 / 2400   loss: 0.06608744\n",
      "iteration: 2015 / 2400   loss: 0.25994128\n",
      "iteration: 2016 / 2400   loss: 0.51494604\n",
      "iteration: 2017 / 2400   loss: 0.31483445\n",
      "iteration: 2018 / 2400   loss: 0.22878386\n",
      "iteration: 2019 / 2400   loss: 0.26300943\n",
      "iteration: 2020 / 2400   loss: 0.063055746\n",
      "iteration: 2021 / 2400   loss: 0.33889297\n",
      "iteration: 2022 / 2400   loss: 0.04830536\n",
      "iteration: 2023 / 2400   loss: 0.37090996\n",
      "iteration: 2024 / 2400   loss: 0.046270236\n",
      "iteration: 2025 / 2400   loss: 0.0650304\n",
      "iteration: 2026 / 2400   loss: 0.48255676\n",
      "iteration: 2027 / 2400   loss: 0.03974371\n",
      "iteration: 2028 / 2400   loss: 0.18832909\n",
      "iteration: 2029 / 2400   loss: 0.13268557\n",
      "iteration: 2030 / 2400   loss: 0.10099909\n",
      "iteration: 2031 / 2400   loss: 0.02544405\n",
      "iteration: 2032 / 2400   loss: 0.078758314\n",
      "iteration: 2033 / 2400   loss: 0.14398824\n",
      "iteration: 2034 / 2400   loss: 0.22880886\n",
      "iteration: 2035 / 2400   loss: 0.0042099\n",
      "iteration: 2036 / 2400   loss: 0.33852348\n",
      "iteration: 2037 / 2400   loss: 0.11385414\n",
      "iteration: 2038 / 2400   loss: 0.07706598\n",
      "iteration: 2039 / 2400   loss: 0.074031666\n",
      "iteration: 2040 / 2400   loss: 0.09672776\n",
      "iteration: 2041 / 2400   loss: 0.034443293\n",
      "iteration: 2042 / 2400   loss: 0.020341815\n",
      "iteration: 2043 / 2400   loss: 0.090647794\n",
      "iteration: 2044 / 2400   loss: 0.022619382\n",
      "iteration: 2045 / 2400   loss: 0.082262866\n",
      "iteration: 2046 / 2400   loss: 0.09808346\n",
      "iteration: 2047 / 2400   loss: 0.13535157\n",
      "iteration: 2048 / 2400   loss: 0.063014224\n",
      "iteration: 2049 / 2400   loss: 0.04825819\n",
      "iteration: 2050 / 2400   loss: 0.8610356\n",
      "iteration: 2051 / 2400   loss: 0.12806319\n",
      "iteration: 2052 / 2400   loss: 0.15569101\n",
      "iteration: 2053 / 2400   loss: 0.15026447\n",
      "iteration: 2054 / 2400   loss: 0.112604216\n",
      "iteration: 2055 / 2400   loss: 0.09828502\n",
      "iteration: 2056 / 2400   loss: 0.04108547\n",
      "iteration: 2057 / 2400   loss: 0.040373307\n",
      "iteration: 2058 / 2400   loss: 0.16498913\n",
      "iteration: 2059 / 2400   loss: 0.14481044\n",
      "iteration: 2060 / 2400   loss: 0.11489656\n",
      "iteration: 2061 / 2400   loss: 0.08470478\n",
      "iteration: 2062 / 2400   loss: 0.677078\n",
      "iteration: 2063 / 2400   loss: 0.014035778\n",
      "iteration: 2064 / 2400   loss: 0.025416564\n",
      "iteration: 2065 / 2400   loss: 0.36371133\n",
      "iteration: 2066 / 2400   loss: 0.041087076\n",
      "iteration: 2067 / 2400   loss: 0.015271368\n",
      "iteration: 2068 / 2400   loss: 0.01343708\n",
      "iteration: 2069 / 2400   loss: 0.03817587\n",
      "iteration: 2070 / 2400   loss: 0.09771671\n",
      "iteration: 2071 / 2400   loss: 0.09480323\n",
      "iteration: 2072 / 2400   loss: 0.08452501\n",
      "iteration: 2073 / 2400   loss: 0.023605179\n",
      "iteration: 2074 / 2400   loss: 0.017243814\n",
      "iteration: 2075 / 2400   loss: 0.109502114\n",
      "iteration: 2076 / 2400   loss: 0.009718247\n",
      "iteration: 2077 / 2400   loss: 0.041763887\n",
      "iteration: 2078 / 2400   loss: 0.16945721\n",
      "iteration: 2079 / 2400   loss: 0.11090552\n",
      "iteration: 2080 / 2400   loss: 0.14193752\n",
      "iteration: 2081 / 2400   loss: 0.11176246\n",
      "iteration: 2082 / 2400   loss: 0.051076986\n",
      "iteration: 2083 / 2400   loss: 0.09780651\n",
      "iteration: 2084 / 2400   loss: 0.2732073\n",
      "iteration: 2085 / 2400   loss: 0.1936613\n",
      "iteration: 2086 / 2400   loss: 0.27555564\n",
      "iteration: 2087 / 2400   loss: 0.33022597\n",
      "iteration: 2088 / 2400   loss: 0.03712549\n",
      "iteration: 2089 / 2400   loss: 0.21375366\n",
      "iteration: 2090 / 2400   loss: 0.11068285\n",
      "iteration: 2091 / 2400   loss: 0.06518689\n",
      "iteration: 2092 / 2400   loss: 0.078182764\n",
      "iteration: 2093 / 2400   loss: 0.18723732\n",
      "iteration: 2094 / 2400   loss: 0.059831478\n",
      "iteration: 2095 / 2400   loss: 0.04189243\n",
      "iteration: 2096 / 2400   loss: 0.040930767\n",
      "iteration: 2097 / 2400   loss: 0.13460912\n",
      "iteration: 2098 / 2400   loss: 0.16236149\n",
      "iteration: 2099 / 2400   loss: 0.17633805\n",
      "iteration: 2100 / 2400   loss: 0.021730117\n",
      "iteration: 2101 / 2400   loss: 0.097833194\n",
      "iteration: 2102 / 2400   loss: 0.013292522\n",
      "iteration: 2103 / 2400   loss: 0.020415058\n",
      "iteration: 2104 / 2400   loss: 0.11780418\n",
      "iteration: 2105 / 2400   loss: 0.07205416\n",
      "iteration: 2106 / 2400   loss: 0.10774683\n",
      "iteration: 2107 / 2400   loss: 0.13711035\n",
      "iteration: 2108 / 2400   loss: 0.18289918\n",
      "iteration: 2109 / 2400   loss: 0.22723332\n",
      "iteration: 2110 / 2400   loss: 0.50956136\n",
      "iteration: 2111 / 2400   loss: 0.04915065\n",
      "iteration: 2112 / 2400   loss: 0.011136179\n",
      "iteration: 2113 / 2400   loss: 0.6208643\n",
      "iteration: 2114 / 2400   loss: 0.20214362\n",
      "iteration: 2115 / 2400   loss: 0.07339543\n",
      "iteration: 2116 / 2400   loss: 0.19233343\n",
      "iteration: 2117 / 2400   loss: 0.16771255\n",
      "iteration: 2118 / 2400   loss: 0.43880185\n",
      "iteration: 2119 / 2400   loss: 0.26935762\n",
      "iteration: 2120 / 2400   loss: 0.18416512\n",
      "iteration: 2121 / 2400   loss: 0.18012194\n",
      "iteration: 2122 / 2400   loss: 0.10815429\n",
      "iteration: 2123 / 2400   loss: 0.13254578\n",
      "iteration: 2124 / 2400   loss: 0.07844017\n",
      "iteration: 2125 / 2400   loss: 0.1664152\n",
      "iteration: 2126 / 2400   loss: 0.12914976\n",
      "iteration: 2127 / 2400   loss: 0.32395685\n",
      "iteration: 2128 / 2400   loss: 0.1419423\n",
      "iteration: 2129 / 2400   loss: 0.20136164\n",
      "iteration: 2130 / 2400   loss: 0.112973034\n",
      "iteration: 2131 / 2400   loss: 0.058580894\n",
      "iteration: 2132 / 2400   loss: 0.08088412\n",
      "iteration: 2133 / 2400   loss: 0.032122973\n",
      "iteration: 2134 / 2400   loss: 0.01043272\n",
      "iteration: 2135 / 2400   loss: 0.098912895\n",
      "iteration: 2136 / 2400   loss: 0.07152214\n",
      "iteration: 2137 / 2400   loss: 0.076757096\n",
      "iteration: 2138 / 2400   loss: 0.03121458\n",
      "iteration: 2139 / 2400   loss: 0.11118105\n",
      "iteration: 2140 / 2400   loss: 0.166018\n",
      "iteration: 2141 / 2400   loss: 0.060189247\n",
      "iteration: 2142 / 2400   loss: 0.020117264\n",
      "iteration: 2143 / 2400   loss: 0.51851344\n",
      "iteration: 2144 / 2400   loss: 0.06591324\n",
      "iteration: 2145 / 2400   loss: 0.086852856\n",
      "iteration: 2146 / 2400   loss: 0.55561197\n",
      "iteration: 2147 / 2400   loss: 0.09816967\n",
      "iteration: 2148 / 2400   loss: 0.1708551\n",
      "iteration: 2149 / 2400   loss: 0.086377256\n",
      "iteration: 2150 / 2400   loss: 0.22054298\n",
      "iteration: 2151 / 2400   loss: 0.0915135\n",
      "iteration: 2152 / 2400   loss: 0.06649309\n",
      "iteration: 2153 / 2400   loss: 0.3542434\n",
      "iteration: 2154 / 2400   loss: 0.050989475\n",
      "iteration: 2155 / 2400   loss: 0.3996658\n",
      "iteration: 2156 / 2400   loss: 0.080788895\n",
      "iteration: 2157 / 2400   loss: 0.2537433\n",
      "iteration: 2158 / 2400   loss: 0.10455056\n",
      "iteration: 2159 / 2400   loss: 0.07473718\n",
      "iteration: 2160 / 2400   loss: 0.3099315\n",
      "iteration: 2161 / 2400   loss: 0.029720068\n",
      "iteration: 2162 / 2400   loss: 0.24739979\n",
      "iteration: 2163 / 2400   loss: 0.13903707\n",
      "iteration: 2164 / 2400   loss: 0.12585163\n",
      "iteration: 2165 / 2400   loss: 0.046910763\n",
      "iteration: 2166 / 2400   loss: 0.014042787\n",
      "iteration: 2167 / 2400   loss: 0.06877014\n",
      "iteration: 2168 / 2400   loss: 0.055201244\n",
      "iteration: 2169 / 2400   loss: 0.015508328\n",
      "iteration: 2170 / 2400   loss: 0.04441553\n",
      "iteration: 2171 / 2400   loss: 0.17225365\n",
      "iteration: 2172 / 2400   loss: 0.22628044\n",
      "iteration: 2173 / 2400   loss: 0.037400667\n",
      "iteration: 2174 / 2400   loss: 0.007808933\n",
      "iteration: 2175 / 2400   loss: 0.031930313\n",
      "iteration: 2176 / 2400   loss: 0.05639719\n",
      "iteration: 2177 / 2400   loss: 0.019210644\n",
      "iteration: 2178 / 2400   loss: 0.09751991\n",
      "iteration: 2179 / 2400   loss: 0.05698287\n",
      "iteration: 2180 / 2400   loss: 0.0323736\n",
      "iteration: 2181 / 2400   loss: 0.6423928\n",
      "iteration: 2182 / 2400   loss: 0.19499192\n",
      "iteration: 2183 / 2400   loss: 0.121368945\n",
      "iteration: 2184 / 2400   loss: 0.13908665\n",
      "iteration: 2185 / 2400   loss: 0.02535016\n",
      "iteration: 2186 / 2400   loss: 0.026762152\n",
      "iteration: 2187 / 2400   loss: 0.10610384\n",
      "iteration: 2188 / 2400   loss: 0.041816875\n",
      "iteration: 2189 / 2400   loss: 0.038678933\n",
      "iteration: 2190 / 2400   loss: 0.091371976\n",
      "iteration: 2191 / 2400   loss: 0.13338336\n",
      "iteration: 2192 / 2400   loss: 0.36115074\n",
      "iteration: 2193 / 2400   loss: 0.037967082\n",
      "iteration: 2194 / 2400   loss: 0.089488484\n",
      "iteration: 2195 / 2400   loss: 0.41793343\n",
      "iteration: 2196 / 2400   loss: 0.24335471\n",
      "iteration: 2197 / 2400   loss: 0.08193969\n",
      "iteration: 2198 / 2400   loss: 0.3429849\n",
      "iteration: 2199 / 2400   loss: 0.525275\n",
      "iteration: 2200 / 2400   loss: 0.05201188\n",
      "iteration: 2201 / 2400   loss: 0.09897313\n",
      "iteration: 2202 / 2400   loss: 0.264638\n",
      "iteration: 2203 / 2400   loss: 0.17143735\n",
      "iteration: 2204 / 2400   loss: 0.10029101\n",
      "iteration: 2205 / 2400   loss: 0.056014366\n",
      "iteration: 2206 / 2400   loss: 0.015217428\n",
      "iteration: 2207 / 2400   loss: 0.17120968\n",
      "iteration: 2208 / 2400   loss: 0.06518745\n",
      "iteration: 2209 / 2400   loss: 0.017624846\n",
      "iteration: 2210 / 2400   loss: 0.10988326\n",
      "iteration: 2211 / 2400   loss: 0.052481346\n",
      "iteration: 2212 / 2400   loss: 0.24582665\n",
      "iteration: 2213 / 2400   loss: 0.06709183\n",
      "iteration: 2214 / 2400   loss: 0.15969974\n",
      "iteration: 2215 / 2400   loss: 0.10767221\n",
      "iteration: 2216 / 2400   loss: 0.040101778\n",
      "iteration: 2217 / 2400   loss: 0.07162826\n",
      "iteration: 2218 / 2400   loss: 0.17597178\n",
      "iteration: 2219 / 2400   loss: 0.024755325\n",
      "iteration: 2220 / 2400   loss: 0.27691576\n",
      "iteration: 2221 / 2400   loss: 0.34242302\n",
      "iteration: 2222 / 2400   loss: 0.22188836\n",
      "iteration: 2223 / 2400   loss: 0.017065411\n",
      "iteration: 2224 / 2400   loss: 0.025678521\n",
      "iteration: 2225 / 2400   loss: 0.21791007\n",
      "iteration: 2226 / 2400   loss: 0.10949581\n",
      "iteration: 2227 / 2400   loss: 0.02292677\n",
      "iteration: 2228 / 2400   loss: 0.065074444\n",
      "iteration: 2229 / 2400   loss: 0.1274934\n",
      "iteration: 2230 / 2400   loss: 0.083990686\n",
      "iteration: 2231 / 2400   loss: 0.010720329\n",
      "iteration: 2232 / 2400   loss: 0.084870785\n",
      "iteration: 2233 / 2400   loss: 0.04243399\n",
      "iteration: 2234 / 2400   loss: 0.04872215\n",
      "iteration: 2235 / 2400   loss: 0.13152206\n",
      "iteration: 2236 / 2400   loss: 0.13531885\n",
      "iteration: 2237 / 2400   loss: 0.24146794\n",
      "iteration: 2238 / 2400   loss: 0.03453453\n",
      "iteration: 2239 / 2400   loss: 0.08559156\n",
      "iteration: 2240 / 2400   loss: 0.05174424\n",
      "iteration: 2241 / 2400   loss: 0.08716519\n",
      "iteration: 2242 / 2400   loss: 0.034542866\n",
      "iteration: 2243 / 2400   loss: 0.10022804\n",
      "iteration: 2244 / 2400   loss: 0.21172121\n",
      "iteration: 2245 / 2400   loss: 0.03119978\n",
      "iteration: 2246 / 2400   loss: 0.08282148\n",
      "iteration: 2247 / 2400   loss: 0.1892733\n",
      "iteration: 2248 / 2400   loss: 0.09793207\n",
      "iteration: 2249 / 2400   loss: 0.23581806\n",
      "iteration: 2250 / 2400   loss: 0.09927814\n",
      "iteration: 2251 / 2400   loss: 0.061197788\n",
      "iteration: 2252 / 2400   loss: 0.102978066\n",
      "iteration: 2253 / 2400   loss: 0.052179586\n",
      "iteration: 2254 / 2400   loss: 0.07443385\n",
      "iteration: 2255 / 2400   loss: 0.22400108\n",
      "iteration: 2256 / 2400   loss: 0.2626518\n",
      "iteration: 2257 / 2400   loss: 0.027937831\n",
      "iteration: 2258 / 2400   loss: 0.060445365\n",
      "iteration: 2259 / 2400   loss: 0.18827757\n",
      "iteration: 2260 / 2400   loss: 0.17014202\n",
      "iteration: 2261 / 2400   loss: 0.034465503\n",
      "iteration: 2262 / 2400   loss: 0.013586903\n",
      "iteration: 2263 / 2400   loss: 0.04045819\n",
      "iteration: 2264 / 2400   loss: 0.18349874\n",
      "iteration: 2265 / 2400   loss: 0.12929462\n",
      "iteration: 2266 / 2400   loss: 0.122874334\n",
      "iteration: 2267 / 2400   loss: 0.070360154\n",
      "iteration: 2268 / 2400   loss: 0.013119927\n",
      "iteration: 2269 / 2400   loss: 0.3026285\n",
      "iteration: 2270 / 2400   loss: 0.13873944\n",
      "iteration: 2271 / 2400   loss: 0.08098436\n",
      "iteration: 2272 / 2400   loss: 0.0033378983\n",
      "iteration: 2273 / 2400   loss: 0.03252206\n",
      "iteration: 2274 / 2400   loss: 0.19593054\n",
      "iteration: 2275 / 2400   loss: 0.21155441\n",
      "iteration: 2276 / 2400   loss: 0.11534648\n",
      "iteration: 2277 / 2400   loss: 0.07338763\n",
      "iteration: 2278 / 2400   loss: 0.007405586\n",
      "iteration: 2279 / 2400   loss: 0.014867935\n",
      "iteration: 2280 / 2400   loss: 0.091961935\n",
      "iteration: 2281 / 2400   loss: 0.17681883\n",
      "iteration: 2282 / 2400   loss: 0.09196003\n",
      "iteration: 2283 / 2400   loss: 0.16361855\n",
      "iteration: 2284 / 2400   loss: 0.2583617\n",
      "iteration: 2285 / 2400   loss: 0.11114468\n",
      "iteration: 2286 / 2400   loss: 0.023787804\n",
      "iteration: 2287 / 2400   loss: 0.11430634\n",
      "iteration: 2288 / 2400   loss: 0.075622655\n",
      "iteration: 2289 / 2400   loss: 0.06417483\n",
      "iteration: 2290 / 2400   loss: 0.25927138\n",
      "iteration: 2291 / 2400   loss: 0.24203095\n",
      "iteration: 2292 / 2400   loss: 0.035182916\n",
      "iteration: 2293 / 2400   loss: 0.28626537\n",
      "iteration: 2294 / 2400   loss: 0.06535698\n",
      "iteration: 2295 / 2400   loss: 0.030978432\n",
      "iteration: 2296 / 2400   loss: 0.16230506\n",
      "iteration: 2297 / 2400   loss: 0.1427123\n",
      "iteration: 2298 / 2400   loss: 0.089234546\n",
      "iteration: 2299 / 2400   loss: 0.2710601\n",
      "iteration: 2300 / 2400   loss: 0.24329574\n",
      "iteration: 2301 / 2400   loss: 0.105359085\n",
      "iteration: 2302 / 2400   loss: 0.17860495\n",
      "iteration: 2303 / 2400   loss: 0.021825504\n",
      "iteration: 2304 / 2400   loss: 0.042486686\n",
      "iteration: 2305 / 2400   loss: 0.13518849\n",
      "iteration: 2306 / 2400   loss: 0.036910478\n",
      "iteration: 2307 / 2400   loss: 0.61314774\n",
      "iteration: 2308 / 2400   loss: 0.082310654\n",
      "iteration: 2309 / 2400   loss: 0.2514564\n",
      "iteration: 2310 / 2400   loss: 0.15854716\n",
      "iteration: 2311 / 2400   loss: 0.3592749\n",
      "iteration: 2312 / 2400   loss: 0.034262255\n",
      "iteration: 2313 / 2400   loss: 0.27810684\n",
      "iteration: 2314 / 2400   loss: 0.06603792\n",
      "iteration: 2315 / 2400   loss: 0.111774065\n",
      "iteration: 2316 / 2400   loss: 0.06650862\n",
      "iteration: 2317 / 2400   loss: 0.060456734\n",
      "iteration: 2318 / 2400   loss: 0.113997385\n",
      "iteration: 2319 / 2400   loss: 0.4843212\n",
      "iteration: 2320 / 2400   loss: 0.0633706\n",
      "iteration: 2321 / 2400   loss: 0.07463778\n",
      "iteration: 2322 / 2400   loss: 0.029315548\n",
      "iteration: 2323 / 2400   loss: 0.26401365\n",
      "iteration: 2324 / 2400   loss: 0.06673227\n",
      "iteration: 2325 / 2400   loss: 0.09245705\n",
      "iteration: 2326 / 2400   loss: 0.08820769\n",
      "iteration: 2327 / 2400   loss: 0.029165149\n",
      "iteration: 2328 / 2400   loss: 0.11276326\n",
      "iteration: 2329 / 2400   loss: 0.0077833747\n",
      "iteration: 2330 / 2400   loss: 0.19135508\n",
      "iteration: 2331 / 2400   loss: 0.08488199\n",
      "iteration: 2332 / 2400   loss: 0.005600338\n",
      "iteration: 2333 / 2400   loss: 0.16337653\n",
      "iteration: 2334 / 2400   loss: 0.053164028\n",
      "iteration: 2335 / 2400   loss: 0.008916473\n",
      "iteration: 2336 / 2400   loss: 0.048561744\n",
      "iteration: 2337 / 2400   loss: 0.11392838\n",
      "iteration: 2338 / 2400   loss: 0.06502517\n",
      "iteration: 2339 / 2400   loss: 0.3279309\n",
      "iteration: 2340 / 2400   loss: 0.073669136\n",
      "iteration: 2341 / 2400   loss: 0.05992035\n",
      "iteration: 2342 / 2400   loss: 0.14713284\n",
      "iteration: 2343 / 2400   loss: 0.23127684\n",
      "iteration: 2344 / 2400   loss: 0.037696704\n",
      "iteration: 2345 / 2400   loss: 0.01385561\n",
      "iteration: 2346 / 2400   loss: 0.083649404\n",
      "iteration: 2347 / 2400   loss: 0.14758082\n",
      "iteration: 2348 / 2400   loss: 0.039327335\n",
      "iteration: 2349 / 2400   loss: 0.06516808\n",
      "iteration: 2350 / 2400   loss: 0.023455106\n",
      "iteration: 2351 / 2400   loss: 0.046349324\n",
      "iteration: 2352 / 2400   loss: 0.1950788\n",
      "iteration: 2353 / 2400   loss: 0.9376679\n",
      "iteration: 2354 / 2400   loss: 0.28971687\n",
      "iteration: 2355 / 2400   loss: 0.21816225\n",
      "iteration: 2356 / 2400   loss: 0.044307355\n",
      "iteration: 2357 / 2400   loss: 0.010610466\n",
      "iteration: 2358 / 2400   loss: 0.0070475675\n",
      "iteration: 2359 / 2400   loss: 0.06547428\n",
      "iteration: 2360 / 2400   loss: 0.0063038445\n",
      "iteration: 2361 / 2400   loss: 0.022463016\n",
      "iteration: 2362 / 2400   loss: 0.02907812\n",
      "iteration: 2363 / 2400   loss: 0.079862155\n",
      "iteration: 2364 / 2400   loss: 0.04067893\n",
      "iteration: 2365 / 2400   loss: 0.03841816\n",
      "iteration: 2366 / 2400   loss: 0.0957429\n",
      "iteration: 2367 / 2400   loss: 0.03760871\n",
      "iteration: 2368 / 2400   loss: 0.052578993\n",
      "iteration: 2369 / 2400   loss: 0.070639804\n",
      "iteration: 2370 / 2400   loss: 0.022325087\n",
      "iteration: 2371 / 2400   loss: 0.028149605\n",
      "iteration: 2372 / 2400   loss: 0.48905525\n",
      "iteration: 2373 / 2400   loss: 0.19580856\n",
      "iteration: 2374 / 2400   loss: 0.30702308\n",
      "iteration: 2375 / 2400   loss: 0.3330774\n",
      "iteration: 2376 / 2400   loss: 0.26441616\n",
      "iteration: 2377 / 2400   loss: 0.5149508\n",
      "iteration: 2378 / 2400   loss: 0.041873626\n",
      "iteration: 2379 / 2400   loss: 0.16633959\n",
      "iteration: 2380 / 2400   loss: 0.08199976\n",
      "iteration: 2381 / 2400   loss: 0.036327057\n",
      "iteration: 2382 / 2400   loss: 0.06271657\n",
      "iteration: 2383 / 2400   loss: 0.13648349\n",
      "iteration: 2384 / 2400   loss: 0.21964462\n",
      "iteration: 2385 / 2400   loss: 0.039386004\n",
      "iteration: 2386 / 2400   loss: 0.14027098\n",
      "iteration: 2387 / 2400   loss: 0.12523647\n",
      "iteration: 2388 / 2400   loss: 0.08349304\n",
      "iteration: 2389 / 2400   loss: 1.2646363\n",
      "iteration: 2390 / 2400   loss: 1.1563455\n",
      "iteration: 2391 / 2400   loss: 0.29266876\n",
      "iteration: 2392 / 2400   loss: 0.17620596\n",
      "iteration: 2393 / 2400   loss: 0.026909351\n",
      "iteration: 2394 / 2400   loss: 0.021444358\n",
      "iteration: 2395 / 2400   loss: 0.0047896574\n",
      "iteration: 2396 / 2400   loss: 0.0058618356\n",
      "iteration: 2397 / 2400   loss: 0.7544183\n",
      "iteration: 2398 / 2400   loss: 0.33267215\n",
      "iteration: 2399 / 2400   loss: 0.11194071\n",
      "iteration: 2400 / 2400   loss: 0.17458138\n",
      "epoch: 3    test_acc: 0.9413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Any[1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  7191, 7192, 7193, 7194, 7195, 7196, 7197, 7198, 7199, 7200], Any[2.2992837f0, 2.304644f0, 2.2968507f0, 2.296804f0, 2.2750115f0, 2.3167093f0, 2.2964826f0, 2.30612f0, 2.2734694f0, 2.2977426f0  …  0.29266876f0, 0.17620596f0, 0.026909351f0, 0.021444358f0, 0.0047896574f0, 0.0058618356f0, 0.7544183f0, 0.33267215f0, 0.11194071f0, 0.17458138f0], Any[2.2992837f0, 2.3019638f0, 2.3002594f0, 2.2993956f0, 2.2945187f0, 2.298217f0, 2.2979693f0, 2.298988f0, 2.2961528f0, 2.2963119f0  …  0.3194391f0, 0.3194192f0, 0.31937853f0, 0.31933713f0, 0.3192934f0, 0.31924987f0, 0.31931034f0, 0.31931219f0, 0.3192834f0, 0.3192633f0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind, currloss, avgloss = train(\"mnist_stride.jld2\", dtrn, dtst, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([avgloss], ylim=(0.0 , 3.0 ),\n",
    "     labels=[:trn_avgloss], xlabel = \"Iterations\", ylabel = \"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
