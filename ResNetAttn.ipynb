{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "for p in (\"Knet\",\"Images\",\"ImageMagick\", \"MAT\")\n",
    "    haskey(Pkg.installed(),p) || Pkg.add(p)\n",
    "end\n",
    "using Knet, MAT, Images, Random\n",
    "using Base.Iterators: flatten, cycle, take\n",
    "using IterTools\n",
    "using Statistics: mean\n",
    "using Plots; default(fmt=:png,ls=:auto)\n",
    "include(Knet.dir(\"data\",\"imagenet.jl\"))\n",
    "import Base: length, size, iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading imagenet-resnet-101-dag.mat...\n",
      "└ @ Main /home/cankucuksozen/.julia/packages/Knet/vxHRi/data/imagenet.jl:14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{String,Any} with 4 entries:\n",
       "  \"meta\"   => Dict{String,Any}(\"classes\"=>Dict{String,Any}(\"name\"=>Any[\"n014407…\n",
       "  \"params\" => Dict{String,Any}(\"name\"=>Any[\"conv1_filter\" \"bn_conv1_mult\" … \"fc…\n",
       "  \"vars\"   => Dict{String,Any}(\"name\"=>Any[\"data\" \"conv1\" … \"fc1000\" \"prob\"],\"p…\n",
       "  \"layers\" => Dict{String,Any}(\"name\"=>Any[\"conv1\" \"bn_conv1\" … \"fc1000\" \"prob\"…"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = matconvnet(\"imagenet-resnet-101-dag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preprocess_img (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function preprocess_img(img, average_img)\n",
    "    \n",
    "    new_size = ntuple(i->div(size(img,i)*224,minimum(size(img))),2)\n",
    "    a1 = Images.imresize(img, new_size)\n",
    "    i1 = div(size(a1,1)-224,2)\n",
    "    j1 = div(size(a1,2)-224,2)\n",
    "    b1 = a1[i1+1:i1+224,j1+1:j1+224]\n",
    "    c1 = channelview(b1)\n",
    "    if ndims(c1) != 3 # image is grayscale\n",
    "        c1 = reshape(c1,(1,224,224))\n",
    "        temp = cat(c1, c1, dims =1)\n",
    "        c1 = cat(temp, c1, dims =1)\n",
    "    end\n",
    "    d1 = convert(Array{Float32}, c1)\n",
    "    e1 = reshape(d1, (224,224,3,1))\n",
    "    f1 = (255 * e1 .- average_img)\n",
    "    g1 = permutedims(f1, [2,1,3,4])\n",
    "    \n",
    "    return g1\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct imagenet_minibatch\n",
    "    classes\n",
    "    class_to_label\n",
    "    class_descriptions\n",
    "    data_path\n",
    "    samples_list\n",
    "    batchsize\n",
    "    shuffle\n",
    "    mode\n",
    "    ninstances \n",
    "    avgimg\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "imagenet_minibatch"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function imagenet_minibatch(model; batchsize = 4, shuffle = true, mode = \"train\")\n",
    "    \n",
    "    root_path = \"/media/cankucuksozen/CAN HD/Can/image_net/imagenet-object-localization-challenge/ILSVRC/\"\n",
    "    samples_path = joinpath(root_path, \"ImageSets/CLS-LOC/\")\n",
    "    data_path = joinpath(root_path, \"Data/CLS-LOC/train\")\n",
    "    \n",
    "    classes = model[\"meta\"][\"classes\"][\"name\"]\n",
    "    labels = reshape(collect(1:length(classes)),(1,:))\n",
    "    class_to_label = Dict(classes .=> labels)\n",
    "    \n",
    "    class_descriptions = model[\"meta\"][\"classes\"][\"description\"]\n",
    "    \n",
    "    avgimg = model[\"meta\"][\"normalization\"][\"averageImage\"]\n",
    "    avgimg = convert(Array{Float32}, avgimg) \n",
    "    \n",
    "    if mode == \"train\"\n",
    "        trn_smp_txt_path = joinpath(samples_path, \"train_cls_mini.txt\")\n",
    "        trn_samples_file = open(trn_smp_txt_path)\n",
    "        trn_samples = readlines(trn_samples_file)\n",
    "        trn_ninstances = length(trn_samples)\n",
    "    end \n",
    "    \n",
    "    return imagenet_minibatch(classes, class_to_label, class_descriptions, data_path, trn_samples, \n",
    "                                batchsize, shuffle, mode, trn_ninstances, avgimg)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "length (generic function with 206 methods)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function length(d::imagenet_minibatch)\n",
    "    batch_count, remains = divrem(d.ninstances, d.batchsize)\n",
    "    return (remains == 0 ? batch_count : batch_count + 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iterate (generic function with 407 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function iterate(d::imagenet_minibatch, \n",
    "    state=ifelse( d.shuffle, randperm(d.ninstances), collect(1:d.ninstances)))\n",
    "\n",
    "    inds = state; \n",
    "    inds_len = length(inds) \n",
    "    max_ind = 0\n",
    "    \n",
    "    if inds_len == 0 \n",
    "        return nothing\n",
    "    else\n",
    "        batch_img = []\n",
    "        batch_label = []\n",
    "        max_ind = min(inds_len, d.batchsize)\n",
    "        \n",
    "        for i in range(1, stop = max_ind)\n",
    "            samp = d.samples_list[inds[i]]\n",
    "            class = split(samp, \"/\")[1]\n",
    "            label = d.class_to_label[class]\n",
    "            img_ext = (split(samp, \" \")[1]) * \".JPEG\"\n",
    "            img_path = joinpath(d.data_path, img_ext)\n",
    "            img = Images.load(img_path)\n",
    "            img = preprocess_img(img, d.avgimg)\n",
    "            img = KnetArray(img)\n",
    "            push!(batch_img, img)\n",
    "            push!(batch_label, label)\n",
    "        end\n",
    "        \n",
    "        img = cat(batch_img..., dims = 4)\n",
    "        labels = hcat(batch_label...)\n",
    "\n",
    "        deleteat!(inds, 1:max_ind)\n",
    "\n",
    "        return ((img,labels), inds)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Conv\n",
    "    w\n",
    "    stride\n",
    "    padding\n",
    "end\n",
    "\n",
    "function Conv(w1::Int, w2::Int, cx::Int, cy::Int; stride = 1, padding = 0)\n",
    "    w = param(w1, w2, cx, cy)\n",
    "    return Conv(w, stride, padding)\n",
    "end\n",
    "\n",
    "function (c::Conv)(x)\n",
    "    return conv4(c.w, x ; padding = c.padding, stride = c.stride)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Conv1x1\n",
    "    w\n",
    "    stride\n",
    "    padding\n",
    "end\n",
    "\n",
    "function Conv1x1(cx::Int, cy::Int; stride = 1, padding = 0)\n",
    "    w = param(1, 1, cx, cy)\n",
    "    return Conv1x1(w, stride, padding)\n",
    "end\n",
    "\n",
    "function (c::Conv1x1)(x)\n",
    "    return conv4(c.w, x ; padding = c.padding, stride = c.stride)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Conv3x3\n",
    "    w\n",
    "    stride\n",
    "    padding\n",
    "end\n",
    "\n",
    "function Conv3x3(cx::Int, cy::Int; stride = 1, padding = 1)\n",
    "    w = param(3, 3, cx, cy)\n",
    "    return Conv3x3(w, stride, padding)\n",
    "end\n",
    "\n",
    "function (c::Conv3x3)(x)\n",
    "    return conv4(c.w, x ; padding = c.padding, stride = c.stride)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Dense\n",
    "    w\n",
    "    b\n",
    "end\n",
    "function Dense(i::Int,o::Int)     \n",
    "    w = param(o,i)\n",
    "    b = param0(o)\n",
    "    return Dense(w,b)\n",
    "end\n",
    "\n",
    "function (d::Dense)(x)\n",
    "    return d.w * mat(x) .+ d.b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Chain\n",
    "    layers\n",
    "    Chain(layers...) = new(layers)\n",
    "end\n",
    "(c::Chain)(x) = (for l in c.layers; x = l(x); end; x)\n",
    "#(c::Chain)(x,y) = nll(c(x),y, average = true)\n",
    "#(c::Chain)(d::Data) = mean(c(x,y) for (x,y) in d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Batchnorm\n",
    "    w\n",
    "    m\n",
    "end \n",
    "\n",
    "function Batchnorm(w1::Int)\n",
    "    w = KnetArray(bnparams(Float32, w1))\n",
    "    m = bnmoments()\n",
    "    return Batchnorm(w,m)\n",
    "end\n",
    "\n",
    "function (b::Batchnorm)(x)\n",
    "    return batchnorm(x, b.m, b.w)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RelEmbed"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct RelEmbed\n",
    "    weight\n",
    "end\n",
    "\n",
    "function RelEmbed(num_relative::Int, embed_channels::Int) \n",
    "    w = Param(KnetArray{Float32}(gaussian(embed_channels,num_relative; mean = 0.0 , std = (embed_channels^-0.5))))\n",
    "    return RelEmbed(w)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flatten_hw (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function flatten_hw(input, d)\n",
    "    h, w, _, Nh, b = size(input)\n",
    "    return reshape(input, (h*w, d, Nh, :))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split_heads_2d (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function split_heads_2d(inputs, Nh)\n",
    "    h, w, d, b = size(inputs)\n",
    "    ret_shape = (h, w, floor(Int,d/Nh), Nh, b)\n",
    "    out = reshape(inputs, ret_shape)\n",
    "    #out = permutedims(split, [1,4,2,3,5])\n",
    "    return out\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "combine_heads_2d (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function combine_heads_2d(inputs)\n",
    "    h, w, dh, Nh, b  = size(inputs)\n",
    "    ret_shape = (h,w, dh*Nh, b)\n",
    "    return reshape(inputs, ret_shape)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rel_to_abs (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function rel_to_abs(x)\n",
    "    b, Nh, l, _ = size(x)\n",
    "    col_pad = KnetArray{Float32}(zeros((b, Nh, l, 1)))\n",
    "    x = cat(x, col_pad; dims = 4)\n",
    "    flat_x = reshape(x, (b, Nh, l*2*l))\n",
    "    flat_pad = KnetArray{Float32}(zeros((b, Nh, l-1)))\n",
    "    flat_x_padded = cat(flat_x, flat_pad; dims = 3)\n",
    "    final_x = reshape(flat_x_padded, (b, Nh, l+1, 2*l-1))\n",
    "    final_x = final_x[:, :, 1:l, l:end]\n",
    "    return final_x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relative_logits_1d (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function relative_logits_1d(q, rel_k, h, w, Nh, transpose_mask)\n",
    "    \n",
    "    _, _, d, _, b = size(q)\n",
    "    flat_q = flatten_hw(q, d)\n",
    "    \n",
    "    rel_k = reshape(rel_k, (d, :, 1, 1))\n",
    "    rel_k_rep = copy(rel_k)\n",
    "    for _ in range(1, stop = Nh-1)\n",
    "        rel_k_rep = cat(rel_k_rep, rel_k, dims = 3)\n",
    "    end\n",
    "    rel_k = copy(rel_k_rep)\n",
    "    for _ in range(1, stop = b-1)\n",
    "        rel_k = cat(rel_k, rel_k_rep, dims = 4)\n",
    "    end\n",
    "    \n",
    "    rel_logits = bmm(flat_q, rel_k)\n",
    "        \n",
    "    #println(size(rel_logits))\n",
    "    \n",
    "    \n",
    "    #=rel_logits = KnetArray{Float32}(zeros((h*w, 2*w-1, Nh, b))); #display(size(rel_logits))\n",
    "    for i in range(1, stop = b)\n",
    "        for j in range(1, stop = Nh)\n",
    "            temp = flat_q[:,:,j,i] * rel_k\n",
    "            rel_logits[:,:,j,i] = temp\n",
    "        end\n",
    "    end\n",
    "    =#\n",
    "    \n",
    "    rel_logits = reshape(rel_logits, (h,w,:,Nh,b))\n",
    "    rel_logits = permutedims(rel_logits, [5,4,2,1,3])\n",
    "    rel_logits = reshape(rel_logits, (:, Nh*h, w, 2*w-1))\n",
    "    rel_logits = rel_to_abs(rel_logits)\n",
    "    rel_logits = reshape(rel_logits, (:, Nh, h, 1, w, w))\n",
    "    rel_logits_rep = copy(rel_logits)\n",
    "    for i in range(1, stop = h-1) #NOT SURE IF THIS SUPPORTS AUTOGRAD\n",
    "        rel_logits_rep = cat(rel_logits_rep, rel_logits, dims = 4)\n",
    "    end\n",
    "    rel_logits = permutedims(rel_logits_rep, transpose_mask)\n",
    "    rel_logits = reshape(rel_logits, (:, Nh, h*w, h*w))\n",
    "    return rel_logits\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pad_zeros (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function pad_zeros(img, kernel_size)\n",
    "    pad = Int(floor(kernel_size/2))\n",
    "    h,w,c,b = size(img)\n",
    "    c_pad = KnetArray{Float32}(zeros((h,pad,c,b))); #display(size(c_pad))\n",
    "    r_pad = KnetArray{Float32}(zeros((pad,w+2*pad,c,b))); #display(size(r_pad))\n",
    "    img_temp_p = cat(c_pad,img, dims = 2); #display(size(img_temp_p))\n",
    "    img_temp_p = cat(img_temp_p,c_pad, dims = 2); #display(size(img_temp_p))\n",
    "    img_temp_p = cat(r_pad,img_temp_p, dims = 1); #display(size(img_temp_p))\n",
    "    img_temp_p = cat(img_temp_p,r_pad, dims = 1); #display(size(img_temp_p))\n",
    "    return img_temp_p\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function odims(input, kernel_size, dv)\n",
    "    #assume stride = 1\n",
    "    inh,inw,inc,b = size(input)\n",
    "    pad = Int(floor(kernel_size/2))\n",
    "    out_dims_h = ((inh-kernel_size) + 2*pad) + 1\n",
    "    out_dims = (out_dims_h, out_dims_h, dv, b)\n",
    "    return out_dims\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odims (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function odims(input, kernel_size, dv)\n",
    "    #assume stride = 1\n",
    "    inh,inw,inc,b = size(input)\n",
    "    pad = Int(floor(kernel_size/2))\n",
    "    out_dims_h = ((inh-kernel_size) + 2*pad) + 1\n",
    "    out_dims_flat = (1, dv, b)\n",
    "    out_dims_orig = (out_dims_h, out_dims_h, dv, b)\n",
    "    return out_dims_flat, out_dims_orig\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "im2col (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function im2col(x, kernel_size)\n",
    "    #Assumes stride = 1 and necessary padding for input_h = output_h\n",
    "    out_h = size(x,1)\n",
    "    x = pad_zeros(x, kernel_size)\n",
    "    imh, imw, imc, b = size(x)\n",
    "    x = reshape(x, (imh*imw, imc, b))\n",
    "    \n",
    "    out = reshape(x[1:kernel_size,:,:],(kernel_size,1, imc,b))\n",
    "    for i in range(2, stop = kernel_size)\n",
    "        temp = reshape(x[(i-1)*imh+1: (i-1)*imh+1 + kernel_size-1, :, :], (kernel_size, 1, imc,b))\n",
    "        out = cat(out, temp, dims = 1)\n",
    "    end\n",
    "    \n",
    "    for i in range(2, stop = out_h*out_h)\n",
    "        next_out = reshape(x[i: i+kernel_size-1, :, :], (kernel_size,1,imc,b))\n",
    "        for j in range(2, stop = kernel_size)\n",
    "            temp = reshape(x[(j-1)*imh+i: (j-1)*imh+i + kernel_size-1, :, :], (kernel_size,1, imc,b))\n",
    "            next_out = cat(next_out, temp, dims = 1 )\n",
    "        end\n",
    "        out = cat(out,next_out, dims = 2)\n",
    "    end\n",
    "    \n",
    "    return out\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct self_attention_2d\n",
    "    conv_q\n",
    "    conv_k\n",
    "    conv_v\n",
    "    conv_attn\n",
    "    rel_embed_h\n",
    "    rel_embed_w\n",
    "    kernel_size\n",
    "    dk\n",
    "    dv\n",
    "    Nh\n",
    "    dkh\n",
    "    dvh\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "self_attention_2d"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function self_attention_2d(input_dims, kernel_size, Nh, dk, dv)\n",
    "    conv_q = Conv(1,1, input_dims, dk)\n",
    "    conv_k = Conv(1,1, input_dims, dk)\n",
    "    conv_v = Conv(1,1, input_dims, dv)\n",
    "    conv_attn = Conv(1,1, dv, dv)\n",
    "    \n",
    "    dkh = floor(Int, dk/Nh)\n",
    "    dvh = floor(Int, dv/Nh)\n",
    "    \n",
    "    rel_embed_h = RelEmbed(2*kernel_size-1, dkh)\n",
    "    rel_embed_w = RelEmbed(2*kernel_size-1, dkh)\n",
    "    \n",
    "    return self_attention_2d(conv_q, conv_k, conv_v, conv_attn, rel_embed_h,\n",
    "                                rel_embed_w, kernel_size, dk, dv, Nh, dkh, dvh)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s::self_attention_2d)(x)\n",
    "    \n",
    "    _, _, _, b = size(x)\n",
    "    q = s.conv_q(x)\n",
    "    k = s.conv_k(x)\n",
    "    v = s.conv_v(x)\n",
    "    \n",
    "    q = q .* (s.dkh ^ -0.5)\n",
    "    \n",
    "    q = split_heads_2d(q,s.Nh)\n",
    "    k = split_heads_2d(k,s.Nh)\n",
    "    v = split_heads_2d(v,s.Nh)\n",
    "    \n",
    "    flat_q = flatten_hw(q, s.dkh)\n",
    "    flat_k = flatten_hw(k, s.dkh)\n",
    "    flat_v = flatten_hw(v, s.dvh)\n",
    "    logits = bmm(flat_q, flat_k, transB = true)\n",
    "    \n",
    "    rel_h = s.rel_embed_h.weight\n",
    "    rel_w = s.rel_embed_w.weight\n",
    "    \n",
    "    q_in_w = copy(q)\n",
    "    q_in_w = permutedims(q_in_w, [2,1,3,4,5])\n",
    "    \n",
    "    rel_logits_h = relative_logits_1d(q, rel_h, s.kernel_size, s.kernel_size, s.Nh, [1, 2, 3, 5, 4, 6])\n",
    "    rel_logits_w = relative_logits_1d(q_in_w, rel_w, s.kernel_size, s.kernel_size, s.Nh, [1, 2, 5, 3, 6, 4])\n",
    "    \n",
    "    logits = permutedims(logits, [4,3, 1,2])\n",
    "    \n",
    "    logits += rel_logits_h\n",
    "    logits += rel_logits_w\n",
    "    \n",
    "    weights = softmax(logits; dims = 4)\n",
    "    weights = permutedims(weights, [3,4,2,1])\n",
    "\n",
    "    attn_out = bmm(weights, flat_v)\n",
    "    attn_out = reshape(attn_out, (s.kernel_size, s.kernel_size, s.dvh, s.Nh, :))\n",
    "    attn_out = combine_heads_2d(attn_out) \n",
    "\n",
    "    attn_out = s.conv_attn(attn_out)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function (s::self_attention_2d)(x)\n",
    "    \n",
    "    out_dims = odims(x, s.kernel_size, s.dv)\n",
    "    out = KnetArray{Float32}(zeros(out_dims))\n",
    "    x = pad_zeros(x, s.kernel_size)\n",
    "    imh, imw, imc, b = size(x)\n",
    "    \n",
    "    #assumes square input spattially, needs reimplementing for efficiency issues.\n",
    "    #Most probably going to reshape input tensor like im2col. then modify inner calculations\n",
    "    #according to new dimensions?\n",
    "    \n",
    "    ##Also, implemented 2d relative logits from Augmented Attention paper, which\n",
    "    ##stores tensors in another format compared to knet ops. need to get rid\n",
    "    ##of some permutedims operations\n",
    "    for i in range(1, stop = imh - s.kernel_size + 1)\n",
    "        for j in range(1, stop = imw - s.kernel_size + 1)\n",
    "\n",
    "            x_patch = x[i:i+s.kernel_size-1, j:j+s.kernel_size-1, :, :]\n",
    "            q = s.conv_q(x_patch)\n",
    "            k = s.conv_k(x_patch)\n",
    "            v = s.conv_v(x_patch)\n",
    "\n",
    "            q = q .* (s.dkh ^ -0.5)\n",
    "\n",
    "            q = split_heads_2d(q,s.Nh)\n",
    "            k = split_heads_2d(k,s.Nh)\n",
    "            v = split_heads_2d(v,s.Nh)\n",
    "\n",
    "            flat_q = flatten_hw(q, s.dkh)\n",
    "            flat_k = flatten_hw(k, s.dkh)\n",
    "            flat_v = flatten_hw(v, s.dvh)\n",
    "            logits = bmm(flat_q, flat_k, transB = true)\n",
    "\n",
    "            rel_h = s.rel_embed_h.weight\n",
    "            rel_w = s.rel_embed_w.weight\n",
    "\n",
    "            q_in_w = copy(q)\n",
    "            q_in_w = permutedims(q_in_w, [2,1,3,4,5])\n",
    "\n",
    "            rel_logits_h = relative_logits_1d(q, rel_h, s.kernel_size, s.kernel_size, s.Nh, [1, 2, 3, 5, 4, 6])\n",
    "            rel_logits_w = relative_logits_1d(q_in_w, rel_w, s.kernel_size, s.kernel_size, s.Nh, [1, 2, 5, 3, 6, 4])\n",
    "\n",
    "            logits = permutedims(logits, [4,3, 1,2])\n",
    "\n",
    "            logits += rel_logits_h\n",
    "            logits += rel_logits_w\n",
    "\n",
    "            weights = softmax(logits; dims = 4)\n",
    "            weights = permutedims(weights, [3,4,2,1])\n",
    "\n",
    "            attn_out = bmm(weights, flat_v)\n",
    "            attn_out = reshape(attn_out, (s.kernel_size, s.kernel_size, s.dvh, s.Nh, :))\n",
    "            attn_out = combine_heads_2d(attn_out) \n",
    "\n",
    "            attn_out = s.conv_attn(attn_out)\n",
    "            \n",
    "            attn_out = reshape(attn_out, (s.kernel_size*s.kernel_size, s.dv, :))\n",
    "            attn_out = sum(attn_out, dims = 1)\n",
    "            \n",
    "            out[i,j] = attn_out\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return out\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Bottleneck_Conv\n",
    "    expansion\n",
    "    conv1\n",
    "    bn1\n",
    "    conv2\n",
    "    bn2\n",
    "    conv3\n",
    "    bn3\n",
    "    activation\n",
    "    downsample\n",
    "    stride\n",
    "end\n",
    "\n",
    "function Bottleneck_Conv(inplanes, planes, expansion; stride = 1, is_downsample = false, groups = 1, \n",
    "                        base_width = 64, dilation = 1)\n",
    "    \n",
    "    width = Int(planes * (base_width / 64.)) * groups\n",
    "    \n",
    "    conv1 = Conv1x1(inplanes,width)\n",
    "    bn1 = Batchnorm(width)\n",
    "    conv2 = Conv3x3(width,width, stride = stride, padding = dilation)\n",
    "    bn2 = Batchnorm(width)\n",
    "    conv3 = Conv1x1(width, planes*expansion)\n",
    "    bn3 = Batchnorm(planes*expansion)\n",
    "    activation = relu\n",
    "    \n",
    "    if is_downsample\n",
    "        downsample = Chain(Conv1x1(inplanes, planes*expansion, stride = stride),\n",
    "                            Batchnorm(planes*expansion))\n",
    "    else\n",
    "        downsample = nothing\n",
    "    end\n",
    "    \n",
    "    return Bottleneck_Conv(expansion, conv1, bn1, conv2, bn2, conv3, bn3, activation, downsample, stride)\n",
    "end\n",
    "\n",
    "function (b::Bottleneck_Conv)(x)\n",
    "    identity = x\n",
    "    \n",
    "    out = b.conv1(x)\n",
    "    out = b.bn1(out)\n",
    "    out = b.activation.(out)\n",
    "    \n",
    "    out = b.conv2(out)\n",
    "    out = b.bn2(out)\n",
    "    out = b.activation.(out)\n",
    "    \n",
    "    out = b.conv3(out)\n",
    "    out = b.bn3(out)\n",
    "    \n",
    "    if b.downsample != nothing\n",
    "        identity = b.downsample(x)\n",
    "    end\n",
    "    \n",
    "    out += identity\n",
    "    out = b.activation.(out)\n",
    "    \n",
    "    return out\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Bottleneck_Attn\n",
    "    expansion\n",
    "    conv1\n",
    "    bn1\n",
    "    self_attn\n",
    "    bn2\n",
    "    conv3\n",
    "    bn3\n",
    "    activation\n",
    "    downsample\n",
    "    stride\n",
    "end\n",
    "\n",
    "function Bottleneck_Attn(inplanes, kernel_size, planes, expansion; stride = 1, is_downsample = false, groups = 1, \n",
    "                        base_width = 64, dilation = 1)\n",
    "    \n",
    "    width = Int(planes * (base_width / 64.)) * groups\n",
    "    \n",
    "    conv1 = Conv1x1(inplanes,width)\n",
    "    bn1 = Batchnorm(width)\n",
    "    self_attn = self_attention_2d(width, kernel_size, 8, width, width)\n",
    "    bn2 = Batchnorm(width)\n",
    "    conv3 = Conv1x1(width, planes*expansion)\n",
    "    bn3 = Batchnorm(planes*expansion)\n",
    "    activation = relu\n",
    "    \n",
    "    if is_downsample\n",
    "        downsample = Chain(Conv1x1(inplanes, planes*expansion, stride = stride),\n",
    "                            Batchnorm(planes*expansion))\n",
    "    else\n",
    "        downsample = nothing\n",
    "    end\n",
    "    \n",
    "    return Bottleneck_Attn(expansion, conv1, bn1, self_attn, bn2, conv3, bn3, activation, downsample, stride)\n",
    "end\n",
    "\n",
    "function (b::Bottleneck_Attn)(x)\n",
    "    \n",
    "    identity = x\n",
    "    \n",
    "    out = b.conv1(x)\n",
    "    out = b.bn1(out)\n",
    "    out = b.activation.(out)\n",
    "    \n",
    "    out = b.self_attn(out)\n",
    "    if b.stride != 1\n",
    "        out = pool(out)\n",
    "    end\n",
    "    out = b.bn2(out)\n",
    "    out = b.activation.(out)\n",
    "    \n",
    "    out = b.conv3(out)\n",
    "    out = b.bn3(out)\n",
    "    \n",
    "    if b.downsample != nothing\n",
    "        identity = b.downsample(x)\n",
    "    end\n",
    "    \n",
    "    out += identity\n",
    "    out = b.activation.(out)\n",
    "    \n",
    "    return out\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_make_conv_layer (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function _make_conv_layer(self_inplanes, block, block_expansion, planes, blocks; stride = 1) #dilate = False\n",
    "    is_downsample = false\n",
    "    if stride != 1 || self_inplanes != planes * block_expansion\n",
    "        is_downsample = true\n",
    "    end\n",
    "    layers = []\n",
    "    push!(layers, block(self_inplanes, planes, block_expansion; stride = stride, is_downsample = is_downsample))\n",
    "    self_inplanes = planes * block_expansion\n",
    "    for _ in range(1, stop = blocks)\n",
    "        push!(layers, block(self_inplanes, planes, block_expansion))\n",
    "    end\n",
    "    return Chain(layers...), self_inplanes\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_make_attn_layer (generic function with 1 method)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function _make_attn_layer(self_inplanes, kernel_size, block, block_expansion, planes, blocks; stride = 1) #dilate = False\n",
    "    is_downsample = false\n",
    "    if stride != 1 || self_inplanes != planes * block_expansion\n",
    "        is_downsample = true\n",
    "    end\n",
    "    layers = []\n",
    "    push!(layers, block(self_inplanes, kernel_size, planes, block_expansion; stride = stride, is_downsample = is_downsample))\n",
    "    self_inplanes = planes * block_expansion\n",
    "  \n",
    "    kernel_size = Int(kernel_size / 2)\n",
    "    for _ in range(1, stop = blocks)\n",
    "        push!(layers, block(self_inplanes, kernel_size, planes, block_expansion))\n",
    "    end\n",
    "    return Chain(layers...), self_inplanes\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct ResNetAttn\n",
    "    base_width\n",
    "    conv_i\n",
    "    bn_i\n",
    "    activation\n",
    "    max_pool\n",
    "    bottleneck_conv_1\n",
    "    bottleneck_conv_2\n",
    "    bottleneck_attn_1\n",
    "    bottleneck_attn_2\n",
    "    avg_pool\n",
    "    fc\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetAttn"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ResNetAttn(layers, num_classes = 1000)\n",
    "    \n",
    "    inplanes = 64\n",
    "    base_width = 64\n",
    "    conv_i = Conv(7,7,3,64; stride = 2, padding = 3)\n",
    "    bn_i = Batchnorm(64)\n",
    "    activation = relu\n",
    "    max_pool = pool\n",
    "    \n",
    "    layer_1, inplanes = _make_conv_layer(inplanes, Bottleneck_Conv, 2, 64, layers[1])\n",
    "    layer_2, inplanes = _make_conv_layer(inplanes, Bottleneck_Conv, 2, 64, layers[2]; stride = 2)\n",
    "    layer_3, inplanes = _make_attn_layer(inplanes, 28, Bottleneck_Attn, 2, 128, layers[3]; stride = 2)\n",
    "    layer_4, inplanes = _make_attn_layer(inplanes, 14, Bottleneck_Attn, 2, 256, layers[4]; stride = 2)\n",
    "    \n",
    "    avg_pool = pool\n",
    "    fc = Dense(256 * 2, num_classes)\n",
    "    \n",
    "    return ResNetAttn(base_width, conv_i, bn_i, activation, max_pool, layer_1, layer_2, layer_3,\n",
    "                    layer_4, avg_pool, fc)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (r::ResNetAttn)(x)\n",
    "    x = r.conv_i(x)\n",
    "    x = r.bn_i(x)\n",
    "    x = r.activation.(x)\n",
    "    x = r.max_pool(x; window = 3, stride = 2, padding = 1)\n",
    "    x = r.bottleneck_conv_1(x)\n",
    "    x = r.bottleneck_conv_2(x)\n",
    "    x = r.bottleneck_attn_1(x)\n",
    "    x = r.bottleneck_attn_2(x)\n",
    "    x = r.avg_pool(x; window = 7, mode = 2)\n",
    "    x = mat(x)\n",
    "    x = r.fc(x)\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (r::ResNetAttn)(x,y)\n",
    "    scores = r(x)\n",
    "    loss = nll(scores, y)\n",
    "    return loss\n",
    "end\n",
    "\n",
    "function (r::ResNetAttn)(d::imagenet_minibatch)\n",
    "    mean_loss = mean(r(x,y) for (x,y) in d)\n",
    "    return mean_loss\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = imagenet_minibatch(model; batchsize = 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resnet_maker (generic function with 1 method)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_maker() = ResNetAttn([2, 3, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(file,maker, dtrn, epochs; lr = 0.001)\n",
    "    res = maker()\n",
    "    r = []\n",
    "    avg = []\n",
    "    sum = 0\n",
    "    for (i,v) in enumerate(adam(res, dataset))\n",
    "        push!(r,(i,v))\n",
    "        sum += v\n",
    "        push!(avg, sum/i)\n",
    "        if i%100 == 0\n",
    "            println(string(\"iteration:  \", i, \"    average_loss:  \", (sum/i)))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    Knet.save(file,\"res\", res)\n",
    "    return r,avg\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  100    average_loss:  8.771205\n",
      "iteration:  200    average_loss:  9.348103\n",
      "iteration:  300    average_loss:  9.537666\n",
      "iteration:  400    average_loss:  9.665838\n",
      "iteration:  500    average_loss:  9.673134\n",
      "iteration:  600    average_loss:  9.607274\n",
      "iteration:  700    average_loss:  9.645316\n",
      "iteration:  800    average_loss:  9.690813\n",
      "iteration:  900    average_loss:  9.621347\n",
      "iteration:  1000    average_loss:  9.649214\n",
      "iteration:  1100    average_loss:  9.562136\n",
      "iteration:  1200    average_loss:  9.504649\n",
      "iteration:  1300    average_loss:  9.426281\n",
      "iteration:  1400    average_loss:  9.377642\n",
      "iteration:  1500    average_loss:  9.430183\n",
      "iteration:  1600    average_loss:  9.405974\n",
      "iteration:  1700    average_loss:  9.392669\n",
      "iteration:  1800    average_loss:  9.424133\n",
      "iteration:  1900    average_loss:  9.404661\n",
      "iteration:  2000    average_loss:  9.385399\n",
      "iteration:  2100    average_loss:  9.362661\n",
      "iteration:  2200    average_loss:  9.3393345\n",
      "iteration:  2300    average_loss:  9.301739\n",
      "iteration:  2400    average_loss:  9.281497\n",
      "iteration:  2500    average_loss:  9.257969\n",
      "iteration:  2600    average_loss:  9.212488\n",
      "iteration:  2700    average_loss:  9.186699\n",
      "iteration:  2800    average_loss:  9.163101\n",
      "iteration:  2900    average_loss:  9.12751\n",
      "iteration:  3000    average_loss:  9.097659\n",
      "iteration:  3100    average_loss:  9.074568\n",
      "iteration:  3200    average_loss:  9.050538\n",
      "iteration:  3300    average_loss:  9.0305\n",
      "iteration:  3400    average_loss:  9.012674\n",
      "iteration:  3500    average_loss:  8.977497\n",
      "iteration:  3600    average_loss:  8.947851\n",
      "iteration:  3700    average_loss:  8.935021\n",
      "iteration:  3800    average_loss:  8.919745\n",
      "iteration:  3900    average_loss:  8.898012\n",
      "iteration:  4000    average_loss:  8.877322\n",
      "iteration:  4100    average_loss:  8.854909\n",
      "iteration:  4200    average_loss:  8.831663\n",
      "iteration:  4300    average_loss:  8.808704\n",
      "iteration:  4400    average_loss:  8.788118\n",
      "iteration:  4500    average_loss:  8.768279\n",
      "iteration:  4600    average_loss:  8.758303\n",
      "iteration:  4700    average_loss:  8.747695\n",
      "iteration:  4800    average_loss:  8.74042\n",
      "iteration:  4900    average_loss:  8.727707\n",
      "iteration:  5000    average_loss:  8.710725\n",
      "iteration:  5100    average_loss:  8.702033\n",
      "iteration:  5200    average_loss:  8.696864\n",
      "iteration:  5300    average_loss:  8.692423\n",
      "iteration:  5400    average_loss:  8.690461\n",
      "iteration:  5500    average_loss:  8.681064\n",
      "iteration:  5600    average_loss:  8.674979\n",
      "iteration:  5700    average_loss:  8.666863\n",
      "iteration:  5800    average_loss:  8.654934\n",
      "iteration:  5900    average_loss:  8.641624\n",
      "iteration:  6000    average_loss:  8.628561\n",
      "iteration:  6100    average_loss:  8.618671\n",
      "iteration:  6200    average_loss:  8.60997\n",
      "iteration:  6300    average_loss:  8.599527\n",
      "iteration:  6400    average_loss:  8.589063\n",
      "iteration:  6500    average_loss:  8.586549\n",
      "iteration:  6600    average_loss:  8.583053\n",
      "iteration:  6700    average_loss:  8.579644\n",
      "iteration:  6800    average_loss:  8.580301\n",
      "iteration:  6900    average_loss:  8.57438\n",
      "iteration:  7000    average_loss:  8.567672\n",
      "iteration:  7100    average_loss:  8.561351\n",
      "iteration:  7200    average_loss:  8.555663\n",
      "iteration:  7300    average_loss:  8.5496645\n",
      "iteration:  7400    average_loss:  8.54284\n",
      "iteration:  7500    average_loss:  8.538405\n",
      "iteration:  7600    average_loss:  8.5347\n",
      "iteration:  7700    average_loss:  8.531318\n",
      "iteration:  7800    average_loss:  8.541533\n",
      "iteration:  7900    average_loss:  8.542903\n",
      "iteration:  8000    average_loss:  8.538317\n",
      "iteration:  8100    average_loss:  8.532623\n",
      "iteration:  8200    average_loss:  8.530594\n",
      "iteration:  8300    average_loss:  8.528179\n",
      "iteration:  8400    average_loss:  8.526079\n",
      "iteration:  8500    average_loss:  8.527277\n",
      "iteration:  8600    average_loss:  8.526838\n",
      "iteration:  8700    average_loss:  8.5237465\n",
      "iteration:  8800    average_loss:  8.521077\n",
      "iteration:  8900    average_loss:  8.519782\n",
      "iteration:  9000    average_loss:  8.517208\n",
      "iteration:  9100    average_loss:  8.511984\n",
      "iteration:  9200    average_loss:  8.510975\n",
      "iteration:  9300    average_loss:  8.509261\n",
      "iteration:  9400    average_loss:  8.504467\n",
      "iteration:  9500    average_loss:  8.500294\n",
      "iteration:  9600    average_loss:  8.495561\n",
      "iteration:  9700    average_loss:  8.492938\n",
      "iteration:  9800    average_loss:  8.491175\n",
      "iteration:  9900    average_loss:  8.489984\n",
      "iteration:  10000    average_loss:  8.486759\n",
      "iteration:  10100    average_loss:  8.485383\n",
      "iteration:  10200    average_loss:  8.483268\n",
      "iteration:  10300    average_loss:  8.482061\n",
      "iteration:  10400    average_loss:  8.480597\n",
      "iteration:  10500    average_loss:  8.477231\n",
      "iteration:  10600    average_loss:  8.474153\n",
      "iteration:  10700    average_loss:  8.47184\n",
      "iteration:  10800    average_loss:  8.467965\n",
      "iteration:  10900    average_loss:  8.464559\n",
      "iteration:  11000    average_loss:  8.460091\n",
      "iteration:  11100    average_loss:  8.454218\n",
      "iteration:  11200    average_loss:  8.44959\n",
      "iteration:  11300    average_loss:  8.447456\n",
      "iteration:  11400    average_loss:  8.442081\n",
      "iteration:  11500    average_loss:  8.439887\n",
      "iteration:  11600    average_loss:  8.437235\n",
      "iteration:  11700    average_loss:  8.433163\n",
      "iteration:  11800    average_loss:  8.429463\n",
      "iteration:  11900    average_loss:  8.426695\n",
      "iteration:  12000    average_loss:  8.426022\n",
      "iteration:  12100    average_loss:  8.424617\n",
      "iteration:  12200    average_loss:  8.423708\n",
      "iteration:  12300    average_loss:  8.420665\n",
      "iteration:  12400    average_loss:  8.420495\n",
      "iteration:  12500    average_loss:  8.418026\n",
      "iteration:  12600    average_loss:  8.41639\n",
      "iteration:  12700    average_loss:  8.412279\n",
      "iteration:  12800    average_loss:  8.409858\n",
      "iteration:  12900    average_loss:  8.408581\n",
      "iteration:  13000    average_loss:  8.407991\n",
      "iteration:  13100    average_loss:  8.406297\n",
      "iteration:  13200    average_loss:  8.403083\n",
      "iteration:  13300    average_loss:  8.4026165\n",
      "iteration:  13400    average_loss:  8.401554\n",
      "iteration:  13500    average_loss:  8.399148\n",
      "iteration:  13600    average_loss:  8.401783\n",
      "iteration:  13700    average_loss:  8.401336\n",
      "iteration:  13800    average_loss:  8.402549\n",
      "iteration:  13900    average_loss:  8.400874\n",
      "iteration:  14000    average_loss:  8.400267\n",
      "iteration:  14100    average_loss:  8.398255\n",
      "iteration:  14200    average_loss:  8.396504\n",
      "iteration:  14300    average_loss:  8.394061\n",
      "iteration:  14400    average_loss:  8.391753\n",
      "iteration:  14500    average_loss:  8.388583\n",
      "iteration:  14600    average_loss:  8.386258\n",
      "iteration:  14700    average_loss:  8.383956\n",
      "iteration:  14800    average_loss:  8.384064\n",
      "iteration:  14900    average_loss:  8.383746\n",
      "iteration:  15000    average_loss:  8.383483\n",
      "iteration:  15100    average_loss:  8.381905\n",
      "iteration:  15200    average_loss:  8.379737\n",
      "iteration:  15300    average_loss:  8.378956\n",
      "iteration:  15400    average_loss:  8.3778715\n",
      "iteration:  15500    average_loss:  8.377284\n",
      "iteration:  15600    average_loss:  8.37595\n",
      "iteration:  15700    average_loss:  8.373956\n",
      "iteration:  15800    average_loss:  8.373565\n",
      "iteration:  15900    average_loss:  8.37319\n",
      "iteration:  16000    average_loss:  8.37096\n",
      "iteration:  16100    average_loss:  8.368751\n",
      "iteration:  16200    average_loss:  8.365976\n",
      "iteration:  16300    average_loss:  8.364845\n",
      "iteration:  16400    average_loss:  8.364216\n",
      "iteration:  16500    average_loss:  8.362928\n",
      "iteration:  16600    average_loss:  8.3606\n",
      "iteration:  16700    average_loss:  8.3586855\n",
      "iteration:  16800    average_loss:  8.358438\n",
      "iteration:  16900    average_loss:  8.357543\n",
      "iteration:  17000    average_loss:  8.356051\n",
      "iteration:  17100    average_loss:  8.35377\n",
      "iteration:  17200    average_loss:  8.352846\n",
      "iteration:  17300    average_loss:  8.35074\n",
      "iteration:  17400    average_loss:  8.348905\n",
      "iteration:  17500    average_loss:  8.347229\n",
      "iteration:  17600    average_loss:  8.345769\n",
      "iteration:  17700    average_loss:  8.345671\n",
      "iteration:  17800    average_loss:  8.344939\n",
      "iteration:  17900    average_loss:  8.345192\n",
      "iteration:  18000    average_loss:  8.344512\n",
      "iteration:  18100    average_loss:  8.343185\n",
      "iteration:  18200    average_loss:  8.34318\n",
      "iteration:  18300    average_loss:  8.341238\n",
      "iteration:  18400    average_loss:  8.340555\n",
      "iteration:  18500    average_loss:  8.340309\n",
      "iteration:  18600    average_loss:  8.338356\n",
      "iteration:  18700    average_loss:  8.337316\n",
      "iteration:  18800    average_loss:  8.334309\n",
      "iteration:  18900    average_loss:  8.33265\n",
      "iteration:  19000    average_loss:  8.331826\n",
      "iteration:  19100    average_loss:  8.330468\n",
      "iteration:  19200    average_loss:  8.329265\n",
      "iteration:  19300    average_loss:  8.327611\n",
      "iteration:  19400    average_loss:  8.325336\n",
      "iteration:  19500    average_loss:  8.323493\n",
      "iteration:  19600    average_loss:  8.322821\n",
      "iteration:  19700    average_loss:  8.320824\n",
      "iteration:  19800    average_loss:  8.320594\n",
      "iteration:  19900    average_loss:  8.319423\n",
      "iteration:  20000    average_loss:  8.318677\n",
      "iteration:  20100    average_loss:  8.317328\n",
      "iteration:  20200    average_loss:  8.315957\n",
      "iteration:  20300    average_loss:  8.315189\n",
      "iteration:  20400    average_loss:  8.312422\n",
      "iteration:  20500    average_loss:  8.31096\n",
      "iteration:  20600    average_loss:  8.310345\n",
      "iteration:  20700    average_loss:  8.310463\n",
      "iteration:  20800    average_loss:  8.310495\n",
      "iteration:  20900    average_loss:  8.31086\n",
      "iteration:  21000    average_loss:  8.311449\n",
      "iteration:  21100    average_loss:  8.310451\n",
      "iteration:  21200    average_loss:  8.308909\n",
      "iteration:  21300    average_loss:  8.308808\n",
      "iteration:  21400    average_loss:  8.307238\n",
      "iteration:  21500    average_loss:  8.30809\n",
      "iteration:  21600    average_loss:  8.30824\n",
      "iteration:  21700    average_loss:  8.308297\n",
      "iteration:  21800    average_loss:  8.307727\n",
      "iteration:  21900    average_loss:  8.3066635\n",
      "iteration:  22000    average_loss:  8.306133\n",
      "iteration:  22100    average_loss:  8.305839\n",
      "iteration:  22200    average_loss:  8.3043585\n",
      "iteration:  22300    average_loss:  8.303477\n",
      "iteration:  22400    average_loss:  8.303591\n",
      "iteration:  22500    average_loss:  8.302128\n",
      "iteration:  22600    average_loss:  8.301055\n",
      "iteration:  22700    average_loss:  8.300696\n",
      "iteration:  22800    average_loss:  8.301192\n",
      "iteration:  22900    average_loss:  8.302814\n",
      "iteration:  23000    average_loss:  8.302611\n",
      "iteration:  23100    average_loss:  8.302804\n",
      "iteration:  23200    average_loss:  8.303269\n",
      "iteration:  23300    average_loss:  8.302744\n",
      "iteration:  23400    average_loss:  8.302085\n",
      "iteration:  23500    average_loss:  8.301659\n",
      "iteration:  23600    average_loss:  8.300868\n",
      "iteration:  23700    average_loss:  8.3001\n",
      "iteration:  23800    average_loss:  8.300038\n",
      "iteration:  23900    average_loss:  8.299664\n",
      "iteration:  24000    average_loss:  8.29935\n",
      "iteration:  24100    average_loss:  8.29945\n",
      "iteration:  24200    average_loss:  8.298342\n",
      "iteration:  24300    average_loss:  8.297693\n",
      "iteration:  24400    average_loss:  8.297483\n",
      "iteration:  24500    average_loss:  8.296632\n",
      "iteration:  24600    average_loss:  8.29679\n",
      "iteration:  24700    average_loss:  8.297008\n",
      "iteration:  24800    average_loss:  8.29562\n",
      "iteration:  24900    average_loss:  8.29541\n",
      "iteration:  25000    average_loss:  8.294707\n",
      "iteration:  25100    average_loss:  8.294436\n",
      "iteration:  25200    average_loss:  8.29441\n",
      "iteration:  25300    average_loss:  8.294456\n",
      "iteration:  25400    average_loss:  8.294709\n",
      "iteration:  25500    average_loss:  8.294089\n",
      "iteration:  25600    average_loss:  8.293669\n",
      "iteration:  25700    average_loss:  8.29295\n",
      "iteration:  25800    average_loss:  8.292472\n",
      "iteration:  25900    average_loss:  8.291992\n",
      "iteration:  26000    average_loss:  8.290865\n",
      "iteration:  26100    average_loss:  8.29017\n",
      "iteration:  26200    average_loss:  8.289274\n",
      "iteration:  26300    average_loss:  8.288988\n",
      "iteration:  26400    average_loss:  8.288586\n",
      "iteration:  26500    average_loss:  8.286984\n",
      "iteration:  26600    average_loss:  8.286331\n",
      "iteration:  26700    average_loss:  8.286865\n",
      "iteration:  26800    average_loss:  8.287155\n",
      "iteration:  26900    average_loss:  8.288228\n",
      "iteration:  27000    average_loss:  8.288379\n",
      "iteration:  27100    average_loss:  8.287249\n",
      "iteration:  27200    average_loss:  8.286344\n",
      "iteration:  27300    average_loss:  8.285779\n",
      "iteration:  27400    average_loss:  8.285331\n",
      "iteration:  27500    average_loss:  8.284935\n",
      "iteration:  27600    average_loss:  8.283913\n",
      "iteration:  27700    average_loss:  8.283142\n",
      "iteration:  27800    average_loss:  8.28242\n",
      "iteration:  27900    average_loss:  8.282182\n",
      "iteration:  28000    average_loss:  8.282009\n",
      "iteration:  28100    average_loss:  8.281122\n",
      "iteration:  28200    average_loss:  8.27948\n",
      "iteration:  28300    average_loss:  8.278266\n",
      "iteration:  28400    average_loss:  8.276918\n",
      "iteration:  28500    average_loss:  8.275393\n",
      "iteration:  28600    average_loss:  8.274677\n",
      "iteration:  28700    average_loss:  8.273192\n",
      "iteration:  28800    average_loss:  8.272476\n",
      "iteration:  28900    average_loss:  8.27273\n",
      "iteration:  29000    average_loss:  8.272879\n",
      "iteration:  29100    average_loss:  8.272243\n",
      "iteration:  29200    average_loss:  8.271835\n",
      "iteration:  29300    average_loss:  8.271916\n",
      "iteration:  29400    average_loss:  8.271233\n",
      "iteration:  29500    average_loss:  8.270742\n",
      "iteration:  29600    average_loss:  8.270828\n",
      "iteration:  29700    average_loss:  8.270359\n",
      "iteration:  29800    average_loss:  8.269682\n",
      "iteration:  29900    average_loss:  8.269723\n",
      "iteration:  30000    average_loss:  8.26972\n",
      "iteration:  30100    average_loss:  8.269809\n",
      "iteration:  30200    average_loss:  8.268684\n",
      "iteration:  30300    average_loss:  8.267961\n",
      "iteration:  30400    average_loss:  8.26688\n",
      "iteration:  30500    average_loss:  8.266323\n",
      "iteration:  30600    average_loss:  8.26518\n",
      "iteration:  30700    average_loss:  8.264403\n",
      "iteration:  30800    average_loss:  8.263601\n",
      "iteration:  30900    average_loss:  8.26317\n",
      "iteration:  31000    average_loss:  8.262472\n",
      "iteration:  31100    average_loss:  8.262483\n",
      "iteration:  31200    average_loss:  8.262631\n",
      "iteration:  31300    average_loss:  8.262258\n",
      "iteration:  31400    average_loss:  8.2619095\n",
      "iteration:  31500    average_loss:  8.260973\n",
      "iteration:  31600    average_loss:  8.26122\n",
      "iteration:  31700    average_loss:  8.260325\n",
      "iteration:  31800    average_loss:  8.2594385\n",
      "iteration:  31900    average_loss:  8.258819\n",
      "iteration:  32000    average_loss:  8.258486\n",
      "iteration:  32100    average_loss:  8.25808\n",
      "iteration:  32200    average_loss:  8.257189\n",
      "iteration:  32300    average_loss:  8.257404\n",
      "iteration:  32400    average_loss:  8.257034\n",
      "iteration:  32500    average_loss:  8.257179\n",
      "iteration:  32600    average_loss:  8.256592\n",
      "iteration:  32700    average_loss:  8.25664\n",
      "iteration:  32800    average_loss:  8.256212\n",
      "iteration:  32900    average_loss:  8.255378\n",
      "iteration:  33000    average_loss:  8.255379\n",
      "iteration:  33100    average_loss:  8.254693\n",
      "iteration:  33200    average_loss:  8.254854\n",
      "iteration:  33300    average_loss:  8.254438\n",
      "iteration:  33400    average_loss:  8.253984\n",
      "iteration:  33500    average_loss:  8.253617\n",
      "iteration:  33600    average_loss:  8.253003\n",
      "iteration:  33700    average_loss:  8.252271\n",
      "iteration:  33800    average_loss:  8.252156\n",
      "iteration:  33900    average_loss:  8.252339\n",
      "iteration:  34000    average_loss:  8.252259\n",
      "iteration:  34100    average_loss:  8.252972\n",
      "iteration:  34200    average_loss:  8.252595\n",
      "iteration:  34300    average_loss:  8.251966\n",
      "iteration:  34400    average_loss:  8.251425\n",
      "iteration:  34500    average_loss:  8.250791\n",
      "iteration:  34600    average_loss:  8.250767\n",
      "iteration:  34700    average_loss:  8.250426\n",
      "iteration:  34800    average_loss:  8.250556\n",
      "iteration:  34900    average_loss:  8.250442\n",
      "iteration:  35000    average_loss:  8.250197\n",
      "iteration:  35100    average_loss:  8.249187\n",
      "iteration:  35200    average_loss:  8.248142\n",
      "iteration:  35300    average_loss:  8.248083\n",
      "iteration:  35400    average_loss:  8.247461\n",
      "iteration:  35500    average_loss:  8.247536\n",
      "iteration:  35600    average_loss:  8.248031\n",
      "iteration:  35700    average_loss:  8.24779\n",
      "iteration:  35800    average_loss:  8.247761\n",
      "iteration:  35900    average_loss:  8.248128\n",
      "iteration:  36000    average_loss:  8.247774\n",
      "iteration:  36100    average_loss:  8.247312\n",
      "iteration:  36200    average_loss:  8.247311\n",
      "iteration:  36300    average_loss:  8.246365\n",
      "iteration:  36400    average_loss:  8.246172\n",
      "iteration:  36500    average_loss:  8.245674\n",
      "iteration:  36600    average_loss:  8.245313\n",
      "iteration:  36700    average_loss:  8.244923\n",
      "iteration:  36800    average_loss:  8.244608\n",
      "iteration:  36900    average_loss:  8.243632\n",
      "iteration:  37000    average_loss:  8.243251\n",
      "iteration:  37100    average_loss:  8.242847\n",
      "iteration:  37200    average_loss:  8.242737\n",
      "iteration:  37300    average_loss:  8.242784\n",
      "iteration:  37400    average_loss:  8.241692\n",
      "iteration:  37500    average_loss:  8.240743\n",
      "iteration:  37600    average_loss:  8.240686\n",
      "iteration:  37700    average_loss:  8.240634\n",
      "iteration:  37800    average_loss:  8.240346\n",
      "iteration:  37900    average_loss:  8.239854\n",
      "iteration:  38000    average_loss:  8.239616\n",
      "iteration:  38100    average_loss:  8.239051\n",
      "iteration:  38200    average_loss:  8.239431\n",
      "iteration:  38300    average_loss:  8.2389965\n",
      "iteration:  38400    average_loss:  8.23917\n",
      "iteration:  38500    average_loss:  8.239343\n",
      "iteration:  38600    average_loss:  8.2398205\n",
      "iteration:  38700    average_loss:  8.239216\n",
      "iteration:  38800    average_loss:  8.239\n",
      "iteration:  38900    average_loss:  8.238712\n",
      "iteration:  39000    average_loss:  8.2388315\n",
      "iteration:  39100    average_loss:  8.238804\n",
      "iteration:  39200    average_loss:  8.238287\n",
      "iteration:  39300    average_loss:  8.237976\n",
      "iteration:  39400    average_loss:  8.237759\n",
      "iteration:  39500    average_loss:  8.237027\n",
      "iteration:  39600    average_loss:  8.23654\n",
      "iteration:  39700    average_loss:  8.236505\n",
      "iteration:  39800    average_loss:  8.236354\n",
      "iteration:  39900    average_loss:  8.236737\n",
      "iteration:  40000    average_loss:  8.236412\n",
      "iteration:  40100    average_loss:  8.2359085\n",
      "iteration:  40200    average_loss:  8.236656\n",
      "iteration:  40300    average_loss:  8.236693\n",
      "iteration:  40400    average_loss:  8.237296\n",
      "iteration:  40500    average_loss:  8.237504\n",
      "iteration:  40600    average_loss:  8.237631\n",
      "iteration:  40700    average_loss:  8.237398\n",
      "iteration:  40800    average_loss:  8.237451\n",
      "iteration:  40900    average_loss:  8.237741\n",
      "iteration:  41000    average_loss:  8.237315\n",
      "iteration:  41100    average_loss:  8.237113\n",
      "iteration:  41200    average_loss:  8.237352\n",
      "iteration:  41300    average_loss:  8.237007\n",
      "iteration:  41400    average_loss:  8.236504\n",
      "iteration:  41500    average_loss:  8.236017\n",
      "iteration:  41600    average_loss:  8.235206\n",
      "iteration:  41700    average_loss:  8.234371\n",
      "iteration:  41800    average_loss:  8.234112\n",
      "iteration:  41900    average_loss:  8.233806\n",
      "iteration:  42000    average_loss:  8.234136\n",
      "iteration:  42100    average_loss:  8.234398\n",
      "iteration:  42200    average_loss:  8.234672\n",
      "iteration:  42300    average_loss:  8.234711\n",
      "iteration:  42400    average_loss:  8.234208\n",
      "iteration:  42500    average_loss:  8.233611\n",
      "iteration:  42600    average_loss:  8.233037\n",
      "iteration:  42700    average_loss:  8.232384\n",
      "iteration:  42800    average_loss:  8.231946\n",
      "iteration:  42900    average_loss:  8.231706\n",
      "iteration:  43000    average_loss:  8.230718\n",
      "iteration:  43100    average_loss:  8.229727\n",
      "iteration:  43200    average_loss:  8.229214\n",
      "iteration:  43300    average_loss:  8.2286825\n",
      "iteration:  43400    average_loss:  8.228115\n",
      "iteration:  43500    average_loss:  8.227106\n",
      "iteration:  43600    average_loss:  8.226319\n",
      "iteration:  43700    average_loss:  8.225569\n",
      "iteration:  43800    average_loss:  8.225446\n",
      "iteration:  43900    average_loss:  8.224784\n",
      "iteration:  44000    average_loss:  8.224107\n",
      "iteration:  44100    average_loss:  8.223399\n",
      "iteration:  44200    average_loss:  8.223684\n",
      "iteration:  44300    average_loss:  8.22313\n",
      "iteration:  44400    average_loss:  8.222632\n",
      "iteration:  44500    average_loss:  8.222638\n",
      "iteration:  44600    average_loss:  8.22247\n",
      "iteration:  44700    average_loss:  8.222927\n",
      "iteration:  44800    average_loss:  8.2232485\n",
      "iteration:  44900    average_loss:  8.22348\n",
      "iteration:  45000    average_loss:  8.22417\n",
      "iteration:  45100    average_loss:  8.224549\n",
      "iteration:  45200    average_loss:  8.225047\n",
      "iteration:  45300    average_loss:  8.225492\n",
      "iteration:  45400    average_loss:  8.225721\n",
      "iteration:  45500    average_loss:  8.225676\n",
      "iteration:  45600    average_loss:  8.226113\n",
      "iteration:  45700    average_loss:  8.22593\n",
      "iteration:  45800    average_loss:  8.225986\n",
      "iteration:  45900    average_loss:  8.225858\n",
      "iteration:  46000    average_loss:  8.225525\n",
      "iteration:  46100    average_loss:  8.225741\n",
      "iteration:  46200    average_loss:  8.226007\n",
      "iteration:  46300    average_loss:  8.226093\n",
      "iteration:  46400    average_loss:  8.226401\n",
      "iteration:  46500    average_loss:  8.226633\n",
      "iteration:  46600    average_loss:  8.226649\n",
      "iteration:  46700    average_loss:  8.226363\n",
      "iteration:  46800    average_loss:  8.226051\n",
      "iteration:  46900    average_loss:  8.22575\n",
      "iteration:  47000    average_loss:  8.226101\n",
      "iteration:  47100    average_loss:  8.225528\n",
      "iteration:  47200    average_loss:  8.225216\n",
      "iteration:  47300    average_loss:  8.224597\n",
      "iteration:  47400    average_loss:  8.224219\n",
      "iteration:  47500    average_loss:  8.223616\n",
      "iteration:  47600    average_loss:  8.223473\n",
      "iteration:  47700    average_loss:  8.223307\n",
      "iteration:  47800    average_loss:  8.223083\n",
      "iteration:  47900    average_loss:  8.222813\n",
      "iteration:  48000    average_loss:  8.222343\n",
      "iteration:  48100    average_loss:  8.22185\n",
      "iteration:  48200    average_loss:  8.221082\n",
      "iteration:  48300    average_loss:  8.220602\n",
      "iteration:  48400    average_loss:  8.2201805\n",
      "iteration:  48500    average_loss:  8.219417\n",
      "iteration:  48600    average_loss:  8.218769\n",
      "iteration:  48700    average_loss:  8.218014\n",
      "iteration:  48800    average_loss:  8.217928\n",
      "iteration:  48900    average_loss:  8.217717\n",
      "iteration:  49000    average_loss:  8.217826\n",
      "iteration:  49100    average_loss:  8.217888\n",
      "iteration:  49200    average_loss:  8.217558\n",
      "iteration:  49300    average_loss:  8.2172785\n",
      "iteration:  49400    average_loss:  8.217968\n",
      "iteration:  49500    average_loss:  8.2179785\n",
      "iteration:  49600    average_loss:  8.217374\n",
      "iteration:  49700    average_loss:  8.217436\n",
      "iteration:  49800    average_loss:  8.217512\n",
      "iteration:  49900    average_loss:  8.217871\n",
      "iteration:  50000    average_loss:  8.217969\n",
      "iteration:  50100    average_loss:  8.218171\n",
      "iteration:  50200    average_loss:  8.218364\n",
      "iteration:  50300    average_loss:  8.218849\n",
      "iteration:  50400    average_loss:  8.218949\n",
      "iteration:  50500    average_loss:  8.219054\n",
      "iteration:  50600    average_loss:  8.219331\n",
      "iteration:  50700    average_loss:  8.219274\n",
      "iteration:  50800    average_loss:  8.218868\n",
      "iteration:  50900    average_loss:  8.21847\n",
      "iteration:  51000    average_loss:  8.218516\n",
      "iteration:  51100    average_loss:  8.218229\n",
      "iteration:  51200    average_loss:  8.218149\n",
      "iteration:  51300    average_loss:  8.217895\n",
      "iteration:  51400    average_loss:  8.217694\n",
      "iteration:  51500    average_loss:  8.217573\n",
      "iteration:  51600    average_loss:  8.217733\n",
      "iteration:  51700    average_loss:  8.217665\n",
      "iteration:  51800    average_loss:  8.218074\n",
      "iteration:  51900    average_loss:  8.217903\n",
      "iteration:  52000    average_loss:  8.217994\n",
      "iteration:  52100    average_loss:  8.218216\n",
      "iteration:  52200    average_loss:  8.217869\n",
      "iteration:  52300    average_loss:  8.217348\n",
      "iteration:  52400    average_loss:  8.217035\n",
      "iteration:  52500    average_loss:  8.217282\n",
      "iteration:  52600    average_loss:  8.217549\n",
      "iteration:  52700    average_loss:  8.217792\n",
      "iteration:  52800    average_loss:  8.217879\n",
      "iteration:  52900    average_loss:  8.218472\n",
      "iteration:  53000    average_loss:  8.218768\n",
      "iteration:  53100    average_loss:  8.218593\n",
      "iteration:  53200    average_loss:  8.218766\n",
      "iteration:  53300    average_loss:  8.2192\n",
      "iteration:  53400    average_loss:  8.219127\n",
      "iteration:  53500    average_loss:  8.218514\n",
      "iteration:  53600    average_loss:  8.218469\n",
      "iteration:  53700    average_loss:  8.217733\n",
      "iteration:  53800    average_loss:  8.217609\n",
      "iteration:  53900    average_loss:  8.217525\n",
      "iteration:  54000    average_loss:  8.217276\n",
      "iteration:  54100    average_loss:  8.217257\n",
      "iteration:  54200    average_loss:  8.217161\n",
      "iteration:  54300    average_loss:  8.217047\n",
      "iteration:  54400    average_loss:  8.216737\n",
      "iteration:  54500    average_loss:  8.216893\n",
      "iteration:  54600    average_loss:  8.21674\n",
      "iteration:  54700    average_loss:  8.216075\n",
      "iteration:  54800    average_loss:  8.21606\n",
      "iteration:  54900    average_loss:  8.21575\n",
      "iteration:  55000    average_loss:  8.215415\n",
      "iteration:  55100    average_loss:  8.215369\n",
      "iteration:  55200    average_loss:  8.215624\n",
      "iteration:  55300    average_loss:  8.215522\n",
      "iteration:  55400    average_loss:  8.215305\n",
      "iteration:  55500    average_loss:  8.214943\n",
      "iteration:  55600    average_loss:  8.215136\n",
      "iteration:  55700    average_loss:  8.214706\n",
      "iteration:  55800    average_loss:  8.214951\n",
      "iteration:  55900    average_loss:  8.214884\n",
      "iteration:  56000    average_loss:  8.214926\n",
      "iteration:  56100    average_loss:  8.215029\n",
      "iteration:  56200    average_loss:  8.215066\n",
      "iteration:  56300    average_loss:  8.215118\n",
      "iteration:  56400    average_loss:  8.215219\n",
      "iteration:  56500    average_loss:  8.2151\n",
      "iteration:  56600    average_loss:  8.214739\n",
      "iteration:  56700    average_loss:  8.21461\n",
      "iteration:  56800    average_loss:  8.21457\n",
      "iteration:  56900    average_loss:  8.214619\n",
      "iteration:  57000    average_loss:  8.214746\n",
      "iteration:  57100    average_loss:  8.215044\n",
      "iteration:  57200    average_loss:  8.215181\n",
      "iteration:  57300    average_loss:  8.215341\n",
      "iteration:  57400    average_loss:  8.21526\n",
      "iteration:  57500    average_loss:  8.215311\n",
      "iteration:  57600    average_loss:  8.215251\n",
      "iteration:  57700    average_loss:  8.214755\n",
      "iteration:  57800    average_loss:  8.214384\n",
      "iteration:  57900    average_loss:  8.214052\n",
      "iteration:  58000    average_loss:  8.213834\n",
      "iteration:  58100    average_loss:  8.213811\n",
      "iteration:  58200    average_loss:  8.213373\n",
      "iteration:  58300    average_loss:  8.213137\n",
      "iteration:  58400    average_loss:  8.212597\n",
      "iteration:  58500    average_loss:  8.212487\n",
      "iteration:  58600    average_loss:  8.212608\n",
      "iteration:  58700    average_loss:  8.212961\n",
      "iteration:  58800    average_loss:  8.212969\n",
      "iteration:  58900    average_loss:  8.213205\n",
      "iteration:  59000    average_loss:  8.212749\n",
      "iteration:  59100    average_loss:  8.212306\n",
      "iteration:  59200    average_loss:  8.211557\n",
      "iteration:  59300    average_loss:  8.210994\n",
      "iteration:  59400    average_loss:  8.210382\n",
      "iteration:  59500    average_loss:  8.209639\n",
      "iteration:  59600    average_loss:  8.209359\n",
      "iteration:  59700    average_loss:  8.209212\n",
      "iteration:  59800    average_loss:  8.208821\n",
      "iteration:  59900    average_loss:  8.208574\n",
      "iteration:  60000    average_loss:  8.208455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Any[(1, 7.040567f0), (2, 8.117951f0), (3, 7.313332f0), (4, 7.2831316f0), (5, 7.654583f0), (6, 6.8582253f0), (7, 6.2464676f0), (8, 7.8086653f0), (9, 7.216798f0), (10, 8.303032f0)  …  (59991, 7.5751877f0), (59992, 11.29594f0), (59993, 9.573978f0), (59994, 10.3238945f0), (59995, 9.421295f0), (59996, 8.521521f0), (59997, 7.7621174f0), (59998, 8.156101f0), (59999, 9.507229f0), (60000, 9.259523f0)], Any[7.040567f0, 7.579259f0, 7.4906163f0, 7.4387455f0, 7.481913f0, 7.3779655f0, 7.216323f0, 7.2903657f0, 7.2821918f0, 7.384276f0  …  8.208291f0, 8.208342f0, 8.2083645f0, 8.2084f0, 8.20842f0, 8.208425f0, 8.208417f0, 8.208416f0, 8.208438f0, 8.208455f0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r,avg = train(\"resnetAttn_v3.jld2\", resnet_maker, dataset, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = [i[1] for i in r]\n",
    "v = [i[2] for i in r];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAGQCAIAAAD9V4nPAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nO3daXgUVboH8PdUVa/ZOosJYNj3gMqquKDI0A4zozcIGkBIlIFBBAUdwBEJjo8OKIsyLmwKdwzLaOASZlDHB6MoguIoEwwqKIKARAIxG0l3Or1UnfuhQmTC1oZ0V9Ln//vA01ROqt7TBf3vc2pjnHMCAAAQlWR0AQAAAEZCEAIAgNAQhAAAIDQEIQAACA1BCAAAQkMQAgCA0BCEAAAgtHAEoaqqEyZM0F9XV1dnZ2ffeeed8+bNq66uDsPWAQAALiLkQZiXlzdjxoyioiL9r7m5uSkpKbm5ucnJyRs3bgz11gEAAC4u5EHYqVOn8ePH1/91165d6enpZrM5PT19586dod46AADAxSmh3kCfPn3O/mtZWVlKSgoRpaSklJeXX+i3NmzYcG5MPvfcc2az+dzGqqpKksQYa4p6W55AIKAoId+PzRN2vbC7XtM0IpIkQc9ywK4PftcrinLJj4hwv5Wcc70mzrnen/NavHjx8uXLGyy80I53uVx2u91kMjVhnS2FpmlVVVUJCQlGF2KMmpoai8Vy3q9HIqisrExKSjK6CmPU1NQoimK1Wo0uxBgi73qPxyNJks1ma8J1hjsIExMTS0pKUlNTS0tLL7IjFUW54YYbglyn6YwmqrEl0TRN2L6T2LueznTf6CqMYTKZFEURufsi912SpKbtfrgnFgYNGrRt2zbO+bZt24KPOgAAgBAJdxBmZmZ+//33Y8eOPXr06Lhx48K8dQAAgAbCNDWan5+vv4iOjp4/f354NgoAAHBJgp5zBQAAoIu0IORGFwAAAC1LpAXh+z9yr0pEdKgKmQgAAJcWaUH49F71QCU/7aOPTvKfao2uBgAAmr2IujfBKQ/tPMndAXrha+3rCn7ap93ZnnWIEfS2IwAAEIyICsITNZwTfXearzyglnio3Mu8qvTYNQhCAAC4oIiaGv3BxR1m2nKMF9eQyumTU3zJPvXgaRwsBACAC4qoICzx0K+ulP51XCOiaBPVBKjMSztPIggBAOCCIioIX/lGaxtFAY2I6I+9pRgTEdGfCzR3wNi6AACg+YqcIFQ57SnlPRystZ16OtidHaQhraXUKPajmxeUYlAIAADnFzlBeLSaE9GgZDa2szSvr9TKzqalSTemMCLaX8mJSEMaAgDAOSLnrNGfaqlTDOsWy6alSW2jmCLRrW1YTYDlfk+rv9F+1YYVlvHrkplPozZ2ZpWNLhcAAJqHyAnCUx5+V0dmU6jTmQsHzRJ1jWNEtKeUr/tOW3OQT+jG9pbyK6PY9F5S22gWK+jzvAAA4GeRE4Qfn+IJloaXDKY5WJqD7a/kS77UagK06gAvrSXGeGs7pdjYAz0jZ2YYAAAaJ3KSYEcxT7Y1XCgxmn21REQ1ASKin2qJE2mcVh7Qlu/XKn1hrxIAAJqZyBkRVvoovf15cj2jk/TQbtXl/6+Fpzx0ysP/fkibmhY5XwUAIDzeeeedl19+2cACAoGAokTOp3eQfve7302dOjUUa46ctzKgUaLlPMvtCl2dwD45VXfOqMR+Pn109bdatzg27Ercgw0AfoFjx4796le/GjdunNGFCGTv3r1bt24N0cojJAg1Tn7tgj/9Q3fp0xK1lY35NT64lSQx+uikVuKhvWV88T512JUR8iYAQNjExsampKQYXYVAEhISQrfyCMkAv0aWC18RcVdH6R/H+P09pDZRFKVQu2i2rYg99IlW5ee7S/g7x/nwtgyjQgAAMUVOEDrMF/xptIme6i+lOZhy5oDg7e0kzuml/dp7P/J7PgjsGaHYFGpjRxoCAAgnQk4VqVUp5qIXBV6d8HMKEhEj+p/2Us4t8v09pCo/jXxP3VdOJz2hLhMAAJqdSAnCi06NXkgbO/t9d2l0J+lAJV9YqOYdufBhRgAAiFARMjXqVckiN2Zis38SWzpIJqLXD2vfV+NqCgAA4UTI5/5xd2NGhEQkM0qx0b1dpTgz/ejmn/2EO3MDAIglQoKwwktXWBv/68OuZEuuk/snsUX7NE+g7omGAAAggggJwiMu6hDT+HM+ZUa/7yY9erVUWMbT8wPvFCEJAQBEESFB6PJT3OU9SkJi9Ksrpayu0sHTtGSfdhq3IQWAFqKkpOSJJ54YOXLk3Xff/Ze//KW0tPSXrsHpdAbfzOl0ZmVlnThxIsiVHzp0KCsrK8hNGCJCgtCnkXzZXXGYKaMT++v1Ukkt/W5b4LvTOF4IAC3AokWLevbsmZOTs379+uTk5KVLl4Z6i2vXrm3Tpo3++tFHH7144y5duqxduzbUJV2OCAlCj9o0Pekex4anSvm/kWVG/5Ov1qpNsVIAgFD67rvvhg8fHhMTY7FYMjMzY2Njw7n1vXv3hnNzoRAhQehVm2BEqLPKlBrF3vq10iue/ekzlYiq/Jf8JQAAw1xzzTVPPvnkrl27PB6PzWb705/+RERLly7dvHmz3mDp0qW5ublOp3PNmjUZGRnr1q1bu3btfffdl56e/ve//11vk5OTM2rUqOnTp//4449EVFVVtXDhwoyMjNGjRy9cuLCqqqrBRvWpzieeeIKI7r//fqfT+f77748aNcrpdH7wwQeTJ08eOXJkfQHNXIRcR1irUtuoprxBWoyJVg+Wh74dGPWe+tu27PfdJdx+DQAuhBNN+zhMM0gSo5dv+K/LxbKzs//5z3++8cYbCxYsuOmmm6ZMmZKQkDB48OANGzaMGjXK7/fv2rVrxYoVq1ev7tix48KFCydPnjxlypRXX31137598+bNu+eee4hI07SNGze+9tpry5YtW7BgwYoVKxRFWbduHREtW7Zs1apVs2fPPnujS5YsIaKnnnrK6XSuWrXK6XR+/fXXS5YsmTx5cklJyapVq7744ovs7OxRo0aF5225HJEQhLUqO1J9WZdPnJfDTFuc8q/fUd/8Qavy0yO9I2T0DABNjhHd1y1MHxHnfimXZfmuu+66++67S0tL16xZk52dvXz58j59+jzzzDMVFRUHDx7s0KFDcnIyEQ0ePFh/kOGIESNkWe7Xr5/fXzflNXz4cH09WVlZRPTvf/97zZo1FouFiCZMmDB58uQGG73mmmsaLMnKynI4HER0xx13MMb69u3r87WM0w4jIQhLamnnKTKH4B9h+2j25Sjl7vfV7D1qvDl8/9ABoMW59grDpo1GjBixfPnytm3bJiUl3X///ZmZmUSkKMr111//ySef7Nu3b9iwYXpLk6nu9HpZlomInfPcHcaY/qMGP1XVS4939RQkIrvdfln9CbtI+GT3akTUZMcIGzBJ9Opg+ZHe0sOfqusP4fpCAGh2rrvuupycnIqKirKysi1btvTt21dfPnjw4O3bt3/++eeDBw++5Eq2bdumqurmzZv79OlDRNdee+3f/vY3n8/n9Xr/9re/XXfddQ3aFxYW1r8OBAJN1xsDREIQ+jVGFJIRoe4KK/1lgPxPp/L459rKA9rnuA0bADQns2bNslqtU6ZMmThx4pEjR6ZPn64v79u37+HDh6+55pro6OhLrkRRlIyMjK+++mrKlClENHXqVK/XO27cuMzMTL/f/8ADD5y7Uf3Ftddeq8+mtlyRMDUa4EREphCfznJLa/bOcHnMdrVdNG29TWnULb4BAJqe1Wqtj6Wzmc3m9u3b18+L5ufnN3hR/1r/c/z48fXLY2NjH3vssQYrPPcXiWj+/PmXbNPMRcKIUL81qBL6rvSKZ1uc8mkf3fRmYG8ZxoUA0HwFAoFDhw6dOHFi4MCBRtfS3EVCEGqciMjWqKdP/FJdYtn23yl9E9nNbwVG5KtLv9JUBCIAND+7d+9+7LHHHnroIbPZ3OQrdzqdP/zwQ5CN9+/f35zvr0aRMTWqn8EStgv9zBItu1F+uLf0l73a03vV1w9rE7pJ93SWok2E+VIAaCYGDx4czDkyjfBLJzzT0tKa+RxpJAShyhlRWEOIEXWLY2tulktr5Uc/U5/4jzpvjxpvYYOS2ehOUoqNBhp3IjUAAPwikRCEfo2ujjdgNGaSqLWd1g2RSzzy43tUmVGZl8Z+EKgJ0AM9pTnXSFc26c1uAAAgFCIhCFVOw1PDNzV6rmQbrR5cd4jy6wqptJa2HNN6bw5MS5PS20t9E1kYTuQBAIDGiYQgDGhkajZJ0yueEdEtreXfd5NWHNCu3xpwXskyu0rXXcFSbCz68h6aCAAATS4SglDlzfHk16sT2Iob5bl9pE1H+NN7tW8quV2hqxJYlEI3prDbrpS6xrEUm9FVAgAILxKCUAvvmTK/SGoUe6Q3u7+H9KObH6jk+T/y7g52tJpP3KmeqOGDktmtraVrr2A9HfT8V5pFomuTWfc4FmOi1nYmM/IEyBYJuwgAoPmKhE9ZlRt5gDAYdoW6xrGucaxfEk+xMZNEC6+lci/lHdUKSvlHJ7VdJ3lrO/tVG/bqN1phOZ32cYnILFOVj/okslgztbKx3vEsyUopNtYxpm4CFgAMYbVa33zzzdLSUqMK4Jyfe7/syFZUVBS6lUdCEGrNPgjrpZ45j1RmdIWV7u9RN6db4iGvxvVHKnKigEZVfvIEuF1hJ2r44Soe0OiLcv5lMXkC/D+lvFbl3eNYgoUlWrhDMreL16JNFKVQDwdLjWIJlvM8qAUAmsp9990XzN07Q6e6ujomJsbAAsKvS5cunTt3DtHKIyEIA1o47q8WUsk2qn/KGCMySZRoIbIwIkqwsN7xjIhGdfy5vctPHxZzm0IlNdqxCvrBxWsCVBOgZfu1/RXcLFOylfWMZze3Ym3sdO0VUms7qbz5TiADtDh33XWXgVsvLS1NSkoysIAIEwlBqHJSBPuIjzbR7e0YEWkaq6z0JST815dTn0Yn3PzrSvp3ifb+CXriP4Ef3dyqsP5JrEM0dY9jskQ2mdpHM5tCyVZKsTEciQQAYUXC5x9u9tmAWaIOMaxDDP2ubd3VjZU+8mv0bSUv9vADlXTKwxnRuz/ynzy82k/HXGSSyK5QrJnizZRkZQkWUiTya8TrnuxB0SZqY2cJFroyinWKYUlWMkl1o0x+vkdmAwC0FJEQhIQP4ktxmImIrmjFLvRW+TQqqyWPyt1+OuWhKj8vrSWZkcNMikRVfqoNUKmXvj3NT9Tw76t4kZvLEvMEuE1hElGyjaIUUiSKt7AohRxmMkvUJoql2CjWxGJMFGem1nZmkihKIUtYbo8OABCkSAhCjAgvn1mi1nbSY/IqomC+WtSfo+RVqdxLtSp3+cmrUYmHAhqvVemUh74q5+Ve7tOozEslHl6rUqWPFEYqJ4tMcWa60s7izHUFSIyiTcSI7AqLNZNNpgQLJdtYnJnizRRtYtEmsiJEAaCpRUIQEpFgJxI3C/Vn6lrkn0P0jAvuj/p5VJ9GP7p5TYB+qiWNk8bJo/KARhqnMi8FNDrtoyI3P+7m1X6q8vHTPnIFyOUnu0LRJnKYySITqeYkG4uxqFEKJVnrMpVz0jjJEtkVYkQxJlI5MSJFIoUxi0wWmWJMZJOJEyVamVflZonVBLjGKcBJ42SVKcbEJKb/St1JRi39hCwAuJDICELkYItRv6fMEnWMYRf44SXUqnTKwwMaudy1Lm52c8kT4Kc8pA9JFUYSIyvRsWpyB7hfI42TWSYicvl5QCOPStV+qlU5I6rwkk8jiVGcmQIaRZvIp5LKya9RtZ8YI69KMqNatW61AU4BjRijgEZmiQKcLBIFODEii0yKRBaJZInsMtkVpkhkkijOTGaJZKZfEkNmmeLNZJKIMbJIZFeYVSaJkUUmm0IxJoo3M5tCcWaKVsgksQDnJonJTB8rk8RIZsQYDgcANJlICEKNE8O3dZFYZWofzYiomnGLhczmCx77DDWNE2PEeV00elWqVUnjVKvyWpVUTi4/uQPk14gRmWWySOTVyBMgn8YDGvk18mrk9pNXpZoAlXmp0stdAV7ppVqVKrxcI+KcFIlcfmKMfCppnDTSN0ESj9FYwK4Q58TPTC9bZNKTVa8q1sQ0TopENpksMjFG0Urd6Jbzuli1yExiZJbIp5FVplgzaZxkRjIj/bnT+sptMsln/UdTGEWZmFkii1z37kuMGFHgzAlWMiO7Qionq0wKI6vMFKmupVXGLZOgGYmEf4ycmuO9RkEE+vywPiNhkupOryWiIIK5CZK7tLTy7IvJ9GStUcmvkU/l+mjSpxIn8mt1eexVSR8iBzgpjPwaceK1al1Uc6KaQN1McoW3brVmmTinKk5+jXwa1zipnFRel/f6AJrOmvTWOPk1kiVSNdLXLDGqVblPI1Ujj1oXlm4/+XldeGucOCeVE2M/z0UTUa3683vr1yig1YWuxknTzLLEAjzAGJmlun3hVckqk1+jACeTRBonk0Q2mcwyyazuFGiVkyKR/uUgzsxUjRSJohQyy/oQXB+aM4nqxuhEJBEpEtmVujl8xsjlr3sP3f66UuvPVKh7E4i8KlcYq1W5XWH6CF7/MmSTfx7Q2xSyynVfO6wy0+u0K2Q6842BE5kkUhjVqnXT+36NiKiyUnYQD2h1bfQXFrnuzaw/bMHOFMbPvNbXr7+Z+k48m947/QuNRa6bBWFEKidOJLG6xxtIRFaF1QS4RSJFqnscrMrJLJFGpL+lPo0sEmln3hD9H4m+6xVW9/7X/x/QK1dY3T8bxsivkcLCdzwiQoIQc6MAikSKVD/SYmf9eXEt8j9Ptcsly4rdZr14M05U7a+7EEifx66PhIBG1X6u54orQJxTlZ9UTlU+7tf01CefWjfZ7vKSRMTPfO2OMpFEZJLoChvZZJIZBTjJjHwaEdUdZmZn7QK/VpcTVf66ZNK/cJR7iVFdtlX7NX3Q7wrUpVRAI6tMKievRra6uf26oXbAZ7ZYNZNUN6yv76we9j6NFEaMUa1KVrmu7/rXC79Wl6n6wXL91+uDWV+DfqTcE/j5bdSIiPSqOCPmUTkRWWWqVesmHujMpht8FPMzQaud2VCDn+pF6l+n9GCW2c898ml1SxSJfCoRkcRIItPO4erF9/svFQlBqOGeKQCCYRTUjRUZUeyFn32WbDvvKlrAp0lpqScpKcroKozhdrulpj4YFglzilqzfAwTAAC0CJGQIC3optsAANDcGDA1WlhYuHz58hMnTrRp02batGlXX331Za4QxwgBAKDRDBgRPvvss/fcc8+WLVvGjh377LPPXv4KOW8Jk/oAANAsGRCEdrvd7XZ7PB6Px2Oz2S5/hbjpMwAANBrjnF+6VZP69ttvH3zwQf31yy+/3L1793PbdO3aNS0trcHCVatWWSyWcxu/9LXqsCqZnUW856imaadPn46Pjze6EGO4XC6z2Ww2m40uxBhlZWWJiYlGV2EMt9utKMp5PxBEIPKur6mpkSTJar3ElTP1YmNjZfkSNyk24Bjh6tWrMzIyRo4cuXnz5jVr1ixatOjcNtHR0c8991yDhUlJSex8BwMtFk+U3RQbGwmXgvxSmqZpmhYbG2t0IcZgjFksFmGD0OfzCbvrJUlSFCX4T8MII/Kul2VZkqTgZxMl6dITnwaExzfffDNnzpyEhIQxY8aMGzfuvG1kWe7SpUuQK2SMSZJ0ycyPSIwxWZbF7DsRyWcYXYgx0HfBu290FcbQg7Bpu2/AMcKOHTu+8847Ho8nPz+/c+fOl79CTgyXTwAAQOMYEISzZs367LPPMjIyPvrooz/+8Y/hLwAAAKCeAVOj7dq1e+GFF5pwhRrOGgUAgMaKhDvLEC6oBwCAxoqEIAz7BSAAABA5IiIIMTUKAACNFQlBqOIxTAAA0FgIQgAAEFokBCHHY5gAAKCxIiIIjS4AAABarogIQo7LJwAAoJEiIQgBAAAaLRKCsNxndAUAANBiRUIQ+jlmRgEAoJEiIQgBAAAaLRKCEMNBAABotEgIQk64jhAAABopEoKQCNcSAgBAI0VIECoR0g8AAAg3BAgAAAgtQoIQxwgBAKBxIiEI8WBeAABotEgIQsYiohsAAGCESEgQPIYJAAAaLRKCkDE8mBcAABopEoKQc7IrRhcBAAAtUyQEIQAAQKMhCAEAQGgIQgAAEBqCEAAAhBYJQZhq51Emo4sAAICWKRLOtnykh99uRxICAEBjRMKIEAAAoNEQhAAAIDQEIQAACA1BCAAAQkMQAgCA0BCEAAAgNAQhAAAIDUEIAABCQxACAIDQEIQAACA0BCEAAAgNQQgAAEJDEAIAgNAQhAAAIDQEIQAACA1BCAAAQkMQAgCA0BCEAAAgNAQhAAAIDUEIAABCQxACAIDQEIQAACA0BCEAAAgNQQgAAEJDEAIAgNAQhAAAIDQEIQAACA1BCAAAQkMQAgCA0BCEAAAgNAQhAAAIzYAgVFX1xRdfHDVq1IwZM0pLS8NfAAAAQD0DgnDz5s1ut3vDhg29evXKyckJfwEAAAD1lPBvcvv27bNnz7ZarePHjy8qKgp/AQAAAPUMCMKSkpIPPvhg1qxZrVu3nj179nnbVFZW3nTTTQ0Wbtq0yWKxnNu4qqqqtrbWZDI1fa3NnqZpp0+fNroKw7hcLrPZbDabjS7EGBUVFZIk6GF+t9utKMp5PxBEIPKur6mpkSTJ4/EE2T4uLk6W5Yu3MSAI3W4353z16tVbt259/vnnX3rppXPbxMXF/etf/2qwMDo6+rwrZIzZ7XZhg5CIHA6H0YUYQ5Zli8UibBAGAgFhd72iKIqiWK1Wowsxhsi73mQySZJks9mCbB/MNwYDgtDhcIwcOTIxMTE9PT0vL++8bRhjsbGxQa5QOqPpamxJ0HfBu290FcbArhe8703bfQPeygEDBrz77rs+n+/tt9/u1q1b+AsAAACoZ0AQTpw4ce/evRkZGXv37v3jH/8Y/gIAAADqBTs1qmlaTU2NfpTO5/NVVVUlJiYyxhqxyYSEhEWLFjXiFwEAAJpcUCPC7777bty4catWrSKi/fv3Z2RkjB079g9/+MOJEydCXB4AAEBoBRWEK1eu7Nix49SpU4no1Vdf7dev36ZNmxISElauXBni8gAAAEIrqCA8ePDgzTffbLPZqqurv/rqq7vuusvhcAwdOnTfvn2hrg8AACCkggpCs9ns9/uJqLCw0Gq16qd6SpLUuGOEAAAAzUdQQXj11Vfv3LmzrKxsy5YtgwYNUhTF7Xbn5+f36NEj1PUBAACEVFBBOHny5NLS0jFjxvzwww+ZmZlENGXKlNLS0vvvvz/E5QEAAIRWUJdPtG7des2aNaWlpXFxcfrtrF566aW4uDhMjQIAQEsX7AX1nHObzaanoM/nCwQCoawKAAAgTHAdIQAACA3XEQIAgNBwHSEAAAgN1xECAIDQcB0hAAAIDdcRAgCA0HAdIQAACC3Y6wgZY1dccQURnTx50uv1OhwOpCAAAESAYB/Mu2PHjg0bNhw5ckT/a4cOHTIzM2+++eaQFQYAABAOQY0IP/jgg/nz5994442vvfbaW2+9lZOTc+ONNz799NM7duwIdX0AAAAhFdSIMDc3984777z33nv1v7Zp0+a+++5zu925ubm33HJLKMsDAAAIraBGhD/++GNaWlqDhb169Tp+/HgISgIAAAifoILwyiuv3L9/f4OFBw4cSE1NDUFJAAAA4RNUEI4ePTovL2/t2rXFxcU+n6+4uHjt2rV5eXmjR48OdX0AAAAhFdQxwltvvVWSpPXr169bt05f0r59+7lz5w4ZMiSEpQEAAIResJdP3HLLLbfccovX662oqIiPj7dYLCEtCwAAIDyCDUKdxWJp1apViEoBAAAIv4sF4Q8//HDJ32/Xrl3TFQMAABBuFwvCiRMnXvL38/PzicjpdOovAAAAWpaLBSGyDQAAIl6wN90GAACISAhCAAAQGoIQAACEhiAEAAChIQgBAEBoCEIAABAaghAAAISGIAQAAKE1TRDi0nsAAGihgrrpttPpPHehJEmxsbExMTFpaWm33357jx49mro2AACAkAsqCF944YX58+f37t176NChiYmJFRUV27dvP3DgwMyZM2traz///PNHHnlk/vz5/fr1C3W5AAAATSuoIMzLy+vXr9/MmTPrlwwcOPC5557btm3brFmzBg4cGBsbu3btWgQhAAC0OEEdIywoKOjfv3+Dhf379//000/rX3///fdNXBoAAEDoBRWESUlJBw8ebLDw22+/jYuL018fPXrU4XA0cWkAAAChF1QQjhkzZvPmzevWrSsuLvb5fMXFxevXr9+8efPo0aM9Hs8//vGPVatW3X777aGuFQAAoMkFdYzw1ltvtVqt69evX7t2rb6kffv2jz/++JAhQ4qKivLz83//+98jCAEAoCUKKgjvvfdep9M5b968xMTE8vLy+Ph4i8Wi/yg1NXXZsmWhrBAAACCEgpoa7dChw+uvv56VlTVnzpwvvvgiEAiEuiwAAIDwCGpE+NRTT7nd7t27d+/YseOFF1546aWXbrrpJqfT2bdvX1mWQ10iAABA6AQVhEQUFRU1bNiwYcOGud3uTz75ZMeOHXPnzo2Pj3/jjTdCWh8AAEBI/eJ7jVZUVJSXl58+fVrTNI/HE4qaAAAAwiaoESHn/NChQ7t27fr444+PHTtms9muv/76sWPHDhgwINT1AQAAhFRQQTh+/PiSkhKr1Tpo0KAJEyYMHDjQbDaHujIAAIAwCCoIe/bsOWXKlIEDB1qt1vqFqqr6fD6bzRay2gAAAEIuqGOE2dnZgwcPPjsFiWjPnj333HNPaKoCAAAIk6BGhMePH58/f/7hw4cbLD/3TtwAAAAtS1BBuGrVKkmSli1btnHjRpPJNGbMmO++++6VV17Jzs4OdX0AAAAhFdTU6P79+4cOHdqtW7ff/va35eXl7du3HzZs2M0337xu3bpQ1wcAABBSQQVhbW1tUlISEXXu3Pnw4cOccyIaOHDgnj17QlsdAABAiAUVhK1atTp06BARxcXFSZJ05MgRIlJV9dSpU6GtDgAAIMSCCsL+/ftv3br13XffJaK0tLQ333Edg1IAABGMSURBVHzT7XZv3769ffv2IS4PAAAgtIIKwnHjxl1xxRXbtm0jogceeKCgoGDEiBGFhYVTp04NcXkAAAChFdRZow6H45VXXnG5XESUkpLy2muvlZeXOxyOy3n0xJEjRx566KG33nqr0WsAAAC4fME+fUKW5bi4OP01YywxMfFytupyuRYtWuT1ei9nJQAAAJfvFz994vJpmrZo0SLclQYAAJqDYEeETej1119PTU0dPHjwRdpUVFT06dOnwcJ33nnHYrGc27iqqqq2ttZkMjVllS2EpmmnT582ugrDuFwus9ks7C3gKyoqJMmA77LNgdvtVhTlvB8IIhB519fU1EiSFPxDAOPi4i55FC/cQVhQULB3796FCxdevFl8fHzwFynKsmy324UNQkmSEhISjC7EGCaTyWKxCBuEmqYJu+vNZrOiKA1ugCwOkXe9xWKRJKlpn/cQ7u8UBQUFhYWFw4cPdzqdROR0Or/66qsw1wAAAFAv3CPCSZMmTZo0SX/tdDrz8/PDXAAAAMDZBJ1lBgAA0BkZhBgOAgCA4TAiBAAAoSEIAQBAaAhCAAAQGoIQAACEhiAEAAChIQgBAEBoCEIAABAaghAAAISGIAQAAKEhCAEAQGgIQgAAEBqCEAAAhIYgBAAAoSEIAQBAaAhCAAAQGoIQAACEhiAEAAChIQgBAEBoCEIAABAaghAAAISGIAQAAKEhCAEAQGgIQgAAEBqCEAAAhIYgBAAAoSEIAQBAaAhCAAAQGoIQAACEhiAEAAChIQgBAEBoCEIAABAaghAAAISGIAQAAKEhCAEAQGgIQgAAEBqCEAAAhIYgBAAAoSEIAQBAaAhCAAAQGoIQAACEhiAEAAChIQgBAEBoCEIAABAaghAAAISGIAQAAKEhCAEAQGgIQgAAEBqCEAAAhIYgBAAAoSEIAQBAaAhCAAAQGoIQAACEhiAEAAChIQgBAEBoCEIAABAaghAAAISGIAQAAKEhCAEAQGgIQgAAEBqCEAAAhKaEf5M7d+7MyckpLS3t2LHjzJkzU1NTw18DAACALtwjwuLi4sWLF8+aNSs3N/f6669fvHhxmAsAAAA4mwFBOHTo0B49elgslttuu+348eNhLgAAAOBsjHNuyIZVVX355ZcZY9OnTz/3p506dYqNjW2w8O2337ZYLOc2rqqqstlsJpMpJIU2b5qmnT59Oj4+3uhCjOFyucxms9lsNroQY5SXlyckJBhdhTHcbreiKOf9QBCByLu+pqZGkiSr1Rpke4fDoSiXOAhowDFCItqzZ8/q1asHDBgwYcKE8zZISEjYs2dPkGszmUx2u13YIFQURdj/EhaLxWKxCBuERJSUlGR0CcawWq2KogT/aRh5hN31brdbkiSbzdaE6wx3EHLOX3311QMHDmRnZ+M0GQAAMFy4g3Dfvn27d+9+6aWXZFn2eDxE1LTBDgAA8IuEOwgLCwuLioruvPPO+iX5+flhrgEAAKBeuIMwKysrKysrzBsFAAC4ENxZBgAAhIYgBAAAoSEIAQBAaAhCAAAQGoIQAACEhiAEAAChIQgBAEBoCEIAABAaghAAAISGIAQAAKEhCAEAQGgIQgAAEBqCEAAAhIYgBAAAoSEIAQBAaAhCAAAQGoIQAACEhiAEAAChIQgBAEBoCEIAABAaghAAAISGIAQAAKEhCAEAQGgIQgAAEBqCEAAAhIYgBAAAoSEIAQBAaAhCAAAQGoIQAACEhiAEAAChIQgBAEBoCEIAABAaghAAAISGIAQAAKEhCAEAQGgIQgAAEBqCEAAAhIYgBAAAoSEIAQBAaAhCAAAQGoIQAACEhiAEAAChIQgBAEBoCEIAABAaghAAAISGIAQAAKEhCAEAQGgIQgAAEBqCEAAAhIYgBAAAoSEIAQBAaAhCAAAQGoIQAACEhiAEAAChIQgBAEBoCEIAABAaghAAAISGIAQAAKEhCAEAQGiREIQrV648dOiQ0VUYo7q6+umnnza6CsPk5OQUFhYaXYUxNE2bM2eO0VUYZuPGjZ9++qnRVRjmT3/6k9ElGOatt956//33m3adBgRhdXV1dnb2nXfeOW/evOrq6stf4TfffFNVVXX562mJ/H7/F198YXQVhjl06FBFRYXRVRiDc15QUGB0FYY5evToTz/9ZHQVhvnPf/5jdAmGKSoqKi4ubtp1GhCEubm5KSkpubm5ycnJGzduDH8BAAAA9QwIwl27dqWnp5vN5vT09J07d4a/AAAAgHqMcx7mTd5xxx3/93//Z7FYvF7v3XffvXXr1nPbpKenv/feew0Wdu3aVZLOk9zV1dU2m01RlJCU27xxzquqquLi4owuxBgul8tsNpvNZqMLMUZFRUV8fLzRVRijpqZGlmWLxWJ0IcYQedd7PB7GmNVqDbL93//+9x49ely8jQHhwTlnjOkvNE07b5t//vOf4S0KAAAEZcDUaGJiYklJCRGVlpYmJSWFvwAAAIB6BgThoEGDtm3bxjnftm3bDTfcEP4CAAAA6hlwjNDlcj3zzDOHDx/u2rXrY489FhUVFeYCAAAA6hkwIoyOjp4/f/4bb7zx9NNPIwXD5vDhw7fffrvRVRhD5L6T2N0Xue8kdvd/Ud9b8JmW1dXVCxcu/Prrr3v37v3oo4/GxMQYXVHzdfr06fXr13u9XqMLMYDIfSexuy9y30ns7v/SvrfgW6zhwvwgqar64osvTp482ehCDCBy30ns7ovcdxK7+43oewsOQlyYH6Q1a9YMHz68devWRhdiAJH7TmJ3X+S+k9jdb0TfDThZpqkEc2E+EFFWVtbZt+bLz883sJgwE7nvJHb3Re47id39RvS9BQfh7bffnpeXZzaba2tr77rrrrfeesvoipo7p9Mp1P+Hs4ncdxK7+yL3ncTufvB9b8FToxF8Yb6qqhMmTAi+TZM/0MNAO3funDRp0ogRIx555JGioqILNYvU7n/22WcTJ04cMWLExIkT9+zZc6Fmkdp9Ijpy5MjFT/aL1L5Pnz7decZf//rXCzWLvO7rh/RGjRo1Y8aM0tLSizQLXcdbcBBG6oX5eXl5M2bMuEgGnNsmyPOGmv8Xw+Li4sWLF8+aNSs3N/f6669fvHjxeZs1ovvNv+9EpKrqggULpk2blpeXd++99y5ZsuS8zSK1+0TkcrkWLVp0kZP9IrXvnPOioqLc3NytW7du3bp16tSp520Wkd3fvHmz2+3esGFDr169cnJyztsm1J94LTgIMzMzv//++7Fjxx49enTcuHFGl9NkOnXqNH78+LOXqKpa/zoQCJzbJmLOGyouLh46dGiPHj0sFsttt912/PhxEqn7qqrOmTOnb9++tbW1JpNJv8pWnO5rmrZo0aJ77rmnfok4fS8rK1NVde7cuRkZGc8884zb7SZhur99+/aMjAyr1Tp+/Pg77riDjOh4C76OUL8w3+gqml6fPn0aLFm6dOmAAQOGDBmSm5vr9XqzsrIatCkrK0tJSSGilJSU8vLy8NXa1Pr169evXz8iUlU1JydnyJAhJFL3zWbzdddd5/F40tPTiUifHxOn+6+//npqaurgwYPrl4jT9/Ly8q5du06ZMiU5OXnFihXLly+fO3euIN0vKSn54IMPZs2a1bp169mzZ5MR+70FjwjFMWXKlLy8vIULF3755Zdjx449t0EwD/RoQfbs2TNt2rSoqKhp06aReN232Wxbt26dMGHC8uXLSZjuFxQU7N27d+LEiWcvFKTvRNStW7clS5Z06dIlNjZ20qRJ+uFhQbrvdrs556tXrx44cODzzz9PRnQcQdgCREdH33DDDR9++OGwYcNMJtO5DSLmvCHO+SuvvLJhw4bs7OxJkybJskwidb+4uPjVV18lIpvN9pvf/OaHH34gYbpfUFBQWFg4fPhwp9NJRE6n86uvvhKk70R08ODBr7/+Wn9tMpn0zgrSfYfDMXLkyMTExPT09KNHj5IRHUcQtgBvvvlmQUFBTk7OG2+8sXv37nMbRMx5Q/v27du9e/fTTz+dmJjo8Xg8Hg+J1P3ExMS33377yy+/5Jx/+OGHXbp0IWG6P2nSpPwziCg/P793796C9J2Iamtrn3zyyWPHjvn9/vXr1994440kzK4fMGDAu+++6/P53n777W7dupEhHefQLA0bNqz+9aeffurxeDjn5eXlBQUF57aprq5+/PHHR48enZ2d7XK5wlxqE8rJyRn237hI3eecf/HFF1OmTBkxYsRDDz107NgxLlj3dfVdE6fvmqZt2bIlMzNz5MiRzz77rN4XQbpfVlY2e/bs9PR0/YopbkTHW/AF9QAAAJcPU6MAACA0BCEAAAgNQQgQKk6nUz/zk4i2bdumXyXdJM5e29lbAYBGQBAChMro0aPrnxe9ZMmSsrKyplrz2Ws7eysA0Agt+M4yAM3cpEmTImYrABEMZ40ChIrT6VyzZk27du30i8R1+fn5RUVFK1as2L9/v6Ioffv2zcrKSk1NdTqdK1eu3LhxY0FBwaZNm4iosrJyw4YNBQUFJ0+etNlsPXv2nDx5ctu2bRusrX4r+pIPP/wwNze3qKiobdu2GRkZ+m3qnE7n6tWrP/zww48//ri4uLh79+7Tp0/Xf0W/in/fvn1er7dbt24PPPCAfv0igDgwNQoQcuvWrSOihQsXrlu3rrS09MEHH4yJiZk+ffqYMWMKCwsfeuihEydOENGKFSuqqqpGjRpFRKqqzpw5c9u2bdddd92sWbNGjx5dVlam31z37LU12NCOHTvmz59/1VVXzZw5s3fv3vPnz//oo4/0H7344osVFRWTJ0+eNWtWWVmZ/mQPzvncuXOrq6uzsrKmTp1qNpufeOKJlnuzLoDGwdQoQMi1atWKiJKSklq1arVs2bJbb711xowZ+o9uuummSZMmvfbaa0QUHR395z//Wb+JosvlcjgcGRkZv/71r/WWV1999YMPPthgbQ02tH79+vHjx997771ENGTIELvdvn79+ptvvpmIUlNTH374Yb1ZTEzMY489RkSlpaXHjx9fvHhxYmKi/ivLly+vqamJjo4O8VsC0IxgRAgQVnv37r311lvr/5qSktKnT599+/YR0fDhw/UUJKK4uLjnnntOT8Gqqqr6+dKL8Hq9R48eve222+qXOJ3OI0eO+Hw+ItLnSHVJSUn6MZG4uDiHw7FgwYJdu3ZVV1fbbLaZM2ciBUE0GBEChNXJkydnzpzZYKF+Z2H9yTL1SkpKli9ffuDAAVVVO3XqpA/aLqKiooKIzm6m349Yf05NQkLCub9iNpuXLFmybt26hQsX1tbWXnXVVZMmTUpLS2tMxwBaLAQhQFjZ7fZHH320Q4cODZZPnDhRf9qGjnM+e/bsvn37LliwoFOnToyxY8eOvffeexdZc3x8PBGVlZW1bt1aX6JfYqEvrx9rNtC+ffvs7OxAIHDw4MFNmzbNnTs3NzfXbDY3vocALQ2mRgHCqnPnzocPH253hv4g1u3btzdoVl5efuLEiT/84Q+dO3fWM+y77767+JotFkv79u31pzfo3n333Q4dOlgslgv9SmVlZWZmptvtVhQlLS1txowZLperqqrqMvoH0PJgRAgQDrIs79y5s0+fPpmZmQ8//HBJScnAgQNLS0u3b99+/Pjx++67b8OGDWe3dzgcDodj6dKlQ4cOlWX5888/18///Oyzz/r371+/tl69ep39W+PHj1+wYIHb7e7Zs+eBAwfy8vLmzZt3kari4uJkWX7yySedTifnfPv27e3atbvkHCxAhMGIECAcRo0a9cYbb2RnZ6elpS1ZsqS4uHjp0qWbNm1KTk5+/vnnu3fv3qC9nk8nTpyYP3/+ypUrZVn+3//93/79+y9YsMDn89WvrcFvDRkyZM6cOYWFhc8991xhYWF2drZ+yuiFMMaefPJJSZKWL1++YsUKk8n01FNPXWgSFSBS4YJ6AAAQGkaEAAAgNAQhAAAIDUEIAABCQxACAIDQEIQAACA0BCEAAAgNQQgAAEJDEAIAgNAQhAAAILT/B1IrQhpzH+9cAAAAAElFTkSuQmCC"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(iter, avg,ylim=(0.0,10),\n",
    "     labels=[:trn],xlabel=\"iterations\",ylabel=\"avg_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
